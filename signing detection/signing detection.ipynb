{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac7ad5b-80e4-4d69-b6e2-002c2c86eff0",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fe38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install tensorflow\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install mediapipe==0.9.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e090ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pyarrow.parquet as pq\n",
    "from tensorflow.keras import layers\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "# For extraction and drawing\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f37c8d-ba42-4cc6-ad4d-a924756beeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n",
      "TensorFlow v2.14.0\n",
      "Mediapipe v0.9.0.1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e404aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "cv2.setRNGSeed(seed)\n",
    "tf. random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557ce0e-fbbd-4cac-920a-66a6ea953764",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b96aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose coordinates for hand movement.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "def extract_from_result(res):\n",
    "    # Extract specific pose landmarks if available\n",
    "    px, py, pz = [[]]*3\n",
    "    if res.pose_landmarks:\n",
    "        for i in POSE:\n",
    "            lm = res.pose_landmarks.landmark[i]\n",
    "            px.append(lm.x)\n",
    "            py.append(lm.y)\n",
    "            pz.append(lm.z)\n",
    "    else:\n",
    "        px, py, pz = [[0.]*len(POSE)]*3\n",
    "\n",
    "    # Extract left hand landmarks if available\n",
    "    lx, ly, lz = [[]]*3\n",
    "    if res.left_hand_landmarks:\n",
    "        for lm in res.left_hand_landmarks.landmark:\n",
    "            lx.append(lm.x)\n",
    "            ly.append(lm.y)\n",
    "            lz.append(lm.z)\n",
    "    else:\n",
    "        lx, ly, lz = [[0.]*21]*3\n",
    "\n",
    "    # Extract right hand landmarks if available\n",
    "    rx, ry, rz = [[]]*3\n",
    "    if res.right_hand_landmarks:\n",
    "        for lm in res.right_hand_landmarks.landmark:\n",
    "            rx.append(lm.x)\n",
    "            ry.append(lm.y)\n",
    "            rz.append(lm.z)\n",
    "    else:\n",
    "        rx, ry, rz = [[0.]*21]*3\n",
    "\n",
    "    return list(chain(rx, lx, px, ry, ly, py, rz, lz, pz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a7c9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_video(path_to_video: str):\n",
    "    data = []\n",
    "    video = cv2.VideoCapture(path_to_video)\n",
    "    try:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "            while True:\n",
    "                _, frame = video.read()\n",
    "                if frame is None:\n",
    "                    break\n",
    "\n",
    "                frame.flags.writeable = False\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(frame)\n",
    "                data.append(extract_from_result(results))\n",
    "    finally:\n",
    "        video.release()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "786c5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def preprocess_data(data: list):\n",
    "    sliding_window = deque(maxlen=MAX_LEN)\n",
    "\n",
    "    sequences = []\n",
    "    for pose in data:\n",
    "        sliding_window.append(preprocess_lms(pose))\n",
    "        if len(sliding_window) == MAX_LEN:\n",
    "            seq = deepcopy(list(sliding_window))\n",
    "            sequences.append(seq)\n",
    "        \n",
    "    return  sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a899f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7700fa7b0f438f9b7b74f7478ea052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe7871f691d47d4a1bee98af0839ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in tqdm(range(1, 6)):\n",
    "    data = load_data_from_video(f\"signing samples/{i}.mp4\")\n",
    "    d = preprocess_data(data)\n",
    "    x.extend(d)\n",
    "    y.extend([1]*len(d))\n",
    "    \n",
    "for i in tqdm(range(1, 10)):\n",
    "    data = load_data_from_video(f\"not signing samples/{i}.mp4\")\n",
    "    d = preprocess_data(data)\n",
    "    x.extend(d)\n",
    "    y.extend([0]*len(d))\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "perm = np.random.permutation(len(x))\n",
    "\n",
    "x = x[perm]\n",
    "y = y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac8c00dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6c1769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 16)                11072     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11106 (43.38 KB)\n",
      "Trainable params: 11106 (43.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(MAX_LEN, 156)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c90ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "219/219 [==============================] - 3s 7ms/step - loss: 0.2329 - accuracy: 0.9231 - val_loss: 0.1271 - val_accuracy: 0.9580\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.1142 - accuracy: 0.9646 - val_loss: 0.0854 - val_accuracy: 0.9730\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0820 - accuracy: 0.9734 - val_loss: 0.0723 - val_accuracy: 0.9707\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0617 - accuracy: 0.9779 - val_loss: 0.0650 - val_accuracy: 0.9817\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0617 - accuracy: 0.9780 - val_loss: 0.0421 - val_accuracy: 0.9837\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0533 - accuracy: 0.9813 - val_loss: 0.0397 - val_accuracy: 0.9850\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0457 - accuracy: 0.9831 - val_loss: 0.0499 - val_accuracy: 0.9847\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0449 - accuracy: 0.9829 - val_loss: 0.1067 - val_accuracy: 0.9627\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0387 - accuracy: 0.9866 - val_loss: 0.1127 - val_accuracy: 0.9647\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0372 - accuracy: 0.9860 - val_loss: 0.0286 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x175008b7dc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x[:10000], y[:10000], validation_split=0.3, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de45bcf7-315a-40ae-b5b2-9a0ee171ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignDetectionModel(tf.Module):\n",
    "    def __init__(self, model: Sequential):\n",
    "        super(SignDetectionModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[tf.constant(MAX_LEN, dtype=tf.int32), tf.constant(156, dtype=tf.int32)], dtype=tf.float32),\n",
    "    ])\n",
    "    def predict(self, landmarks):\n",
    "        # Inference\n",
    "        landmarks = tf.expand_dims(landmarks, axis=0)\n",
    "        logits = self.model(landmarks)\n",
    "\n",
    "        probabilities = tf.nn.softmax(logits)[0]\n",
    "        pred = tf.argmax(probabilities)\n",
    "        return {\"result\" : pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "887cb95f-a4d6-4d30-9336-7af6704cf03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signing_detector = SignDetectionModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4666b6a-1d43-41de-89f9-a36c1e09f8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': <tf.Tensor: shape=(), dtype=int64, numpy=0>}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signing_detector.predict(np.zeros((MAX_LEN, 156)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b61fddc-acea-408a-bbdf-56a831bc0f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "save_model_name = \"saved_model\"\n",
    "if os.path.isdir(save_model_name):\n",
    "    print(f\"A model with the same name has already been saved!\")\n",
    "else:\n",
    "    tf.saved_model.save(signing_detector, export_dir=save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884533a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
