{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b12b75-d31d-49b8-98f8-b1103bf9d783",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c1109a63-a002-465d-9554-b305902c7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install openai\n",
    "!pip install tensorflow\n",
    "!pip install scikit-learn\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install mediapipe==0.9.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f17a1c85-90dc-478c-a4ca-be6757abaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import openai\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from sklearn.utils import check_random_state as sklear_set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0e64c7-e4fa-45b5-bcbd-343292eab2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n",
      "TensorFlow v2.14.0\n",
      "Mediapipe v0.9.0.1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea0845a1-7495-48aa-8807-911823b74cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "cv2.setRNGSeed(seed)\n",
    "sklear_set_seed(seed)\n",
    "tf. random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1fae5-e860-47ab-96e9-96e7292eb6a7",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "986fe9cc-86dc-4646-b4aa-84d40895c305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79054\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.282440</td>\n",
       "      <td>0.600049</td>\n",
       "      <td>-1.870000e-07</td>\n",
       "      <td>0.315119</td>\n",
       "      <td>0.568314</td>\n",
       "      <td>-0.020261</td>\n",
       "      <td>0.341418</td>\n",
       "      <td>0.497320</td>\n",
       "      <td>-0.028145</td>\n",
       "      <td>0.345182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245077</td>\n",
       "      <td>0.454705</td>\n",
       "      <td>-0.029709</td>\n",
       "      <td>0.256525</td>\n",
       "      <td>0.503494</td>\n",
       "      <td>-0.027200</td>\n",
       "      <td>0.265023</td>\n",
       "      <td>0.532601</td>\n",
       "      <td>-0.019086</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.306540</td>\n",
       "      <td>0.464684</td>\n",
       "      <td>-2.530000e-07</td>\n",
       "      <td>0.345548</td>\n",
       "      <td>0.433487</td>\n",
       "      <td>-0.015010</td>\n",
       "      <td>0.376949</td>\n",
       "      <td>0.352773</td>\n",
       "      <td>-0.019309</td>\n",
       "      <td>0.383595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273192</td>\n",
       "      <td>0.294298</td>\n",
       "      <td>-0.030596</td>\n",
       "      <td>0.283822</td>\n",
       "      <td>0.349954</td>\n",
       "      <td>-0.026084</td>\n",
       "      <td>0.290784</td>\n",
       "      <td>0.386303</td>\n",
       "      <td>-0.016583</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.312548</td>\n",
       "      <td>0.466279</td>\n",
       "      <td>-2.500000e-07</td>\n",
       "      <td>0.351508</td>\n",
       "      <td>0.435728</td>\n",
       "      <td>-0.017326</td>\n",
       "      <td>0.383225</td>\n",
       "      <td>0.355989</td>\n",
       "      <td>-0.022386</td>\n",
       "      <td>0.390655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279703</td>\n",
       "      <td>0.295002</td>\n",
       "      <td>-0.031225</td>\n",
       "      <td>0.289348</td>\n",
       "      <td>0.351177</td>\n",
       "      <td>-0.027367</td>\n",
       "      <td>0.296098</td>\n",
       "      <td>0.386388</td>\n",
       "      <td>-0.017859</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1             2         3         4         5         6   \\\n",
       "0  0.282440  0.600049 -1.870000e-07  0.315119  0.568314 -0.020261  0.341418   \n",
       "1  0.306540  0.464684 -2.530000e-07  0.345548  0.433487 -0.015010  0.376949   \n",
       "2  0.312548  0.466279 -2.500000e-07  0.351508  0.435728 -0.017326  0.383225   \n",
       "\n",
       "         7         8         9   ...        54        55        56        57  \\\n",
       "0  0.497320 -0.028145  0.345182  ...  0.245077  0.454705 -0.029709  0.256525   \n",
       "1  0.352773 -0.019309  0.383595  ...  0.273192  0.294298 -0.030596  0.283822   \n",
       "2  0.355989 -0.022386  0.390655  ...  0.279703  0.295002 -0.031225  0.289348   \n",
       "\n",
       "         58        59        60        61        62  63  \n",
       "0  0.503494 -0.027200  0.265023  0.532601 -0.019086   a  \n",
       "1  0.349954 -0.026084  0.290784  0.386303 -0.016583   a  \n",
       "2  0.351177 -0.027367  0.296098  0.386388 -0.017859   a  \n",
       "\n",
       "[3 rows x 64 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\"fehervali_lamp_night_webcam.csv\", dtype={63: str}, header=None)\n",
    "dataset_df = dataset_df[dataset_df[63].notna()]\n",
    "print(len(dataset_df))\n",
    "dataset_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9c0f1-fbce-4f36-895c-e766f1d0ec91",
   "metadata": {},
   "source": [
    "As the data cosists of sequential frames many of them are similar, so only every n frames are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fa8cfe9-b168-47f9-991a-a02b25e645da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "dataset_df = dataset_df[dataset_df.index % n == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e20c50c6-1bfe-4254-afbd-fe8deed8e3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category_counts = dataset_df[63].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "category_counts.plot(kind='bar')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Distribution of Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4918847-c341-4f20-968b-f6f1fa95ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_DS = \"shuffled_dataset_train.csv\"\n",
    "PATH_VALID_DS = \"shuffled_dataset_train.csv\"\n",
    "shuffled_df = dataset_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df, valid_df = train_test_split(shuffled_df, train_size=0.8, shuffle=True)\n",
    "\n",
    "train_df.to_csv(PATH_TRAIN_DS, index=False)\n",
    "valid_df.to_csv(PATH_VALID_DS, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a7282-d67d-40ae-b30f-765be12900ec",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3652c1a1-9e73-479d-b1d3-96d754a74a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cats = sorted(set(dataset_df[63].apply(lambda c: str(c))))\n",
    "cat_to_num = {sign : i for i, sign in enumerate(sorted_cats)}\n",
    "num_to_cat = {i : sign for sign, i in cat_to_num.items()}\n",
    "\n",
    "tf_cat_to_num = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(cat_to_num.keys()),\n",
    "        values=list(cat_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"signs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ccad94-dbe8-4d21-ad91-d6d5325a8ce1",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "Data looks like this: x1,y1,z1, x2,y2,z2 ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "88b65877-9b23-48ff-8f4f-483cbb612bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_lm(flat_xyz,\n",
    "               flip_prob=0.3,\n",
    "               zoom_prob=0.3,\n",
    "               affine_prob=0.3,\n",
    "               scale=(0.8, 1.2),\n",
    "               shear=(-0.15, 0.15),\n",
    "               shift=(-0.1, 0.1),\n",
    "               degree=(-30, 30),\n",
    "               zoom=(0.8, 1.2)):\n",
    "    \n",
    "    # Reshape flat data to [num_samples, 3]\n",
    "    xyz = tf.reshape(flat_xyz, [21, 3]) # 21 coords for the hands\n",
    "    \n",
    "    # 1. Flip x coordinates with given probability\n",
    "    if tf.random.uniform(()) < flip_prob:\n",
    "        xyz = tf.stack([1-xyz[:, 0], xyz[:, 1], xyz[:, 2]], axis=1)\n",
    "    \n",
    "    # 2. Apply zoom with given probability\n",
    "    if tf.random.uniform(()) < zoom_prob:\n",
    "        zx = zy = tf.random.uniform((), *zoom)\n",
    "    else:\n",
    "        zx, zy = 1.0, 1.0\n",
    "    \n",
    "    # 3. Affine transformations\n",
    "    if tf.random.uniform(()) < affine_prob:\n",
    "        # Random scale factors for x and y\n",
    "        sx = tf.random.uniform((), *scale)\n",
    "        sy = tf.random.uniform((), *scale)\n",
    "        sx *= zx\n",
    "        sy *= zy\n",
    "\n",
    "        # Random shear factors for x and y\n",
    "        shx = tf.random.uniform((), *shear)\n",
    "        shy = tf.random.uniform((), *shear)\n",
    "\n",
    "        # Random shift for x and y\n",
    "        tx = tf.random.uniform((), *shift)\n",
    "        ty = tf.random.uniform((), *shift)\n",
    "\n",
    "        # Random rotation in radians\n",
    "        pi_on_180 = 0.017453292519943295\n",
    "        theta = tf.random.uniform((), degree[0]*pi_on_180, degree[1]*pi_on_180)\n",
    "        c, s = tf.cos(theta), tf.sin(theta)\n",
    "\n",
    "        # Constructing the affine transformation matrix\n",
    "        matrix = tf.convert_to_tensor([\n",
    "            [sx*c + shx*s, -sy*s + shx*c, tx],\n",
    "            [sx*s + shy*c, sy*c + shy*s, ty]\n",
    "        ])\n",
    "\n",
    "        # Apply transformation to x and y\n",
    "        xy = tf.matmul(xyz[:, :2], tf.transpose(matrix[:, :2])) + matrix[:, 2]\n",
    "    else:\n",
    "        xy = xyz[:, :2]\n",
    "    \n",
    "    # Combining transformed x and y with original z\n",
    "    transformed_xyz = tf.concat([xy, tf.expand_dims(xyz[:, 2], axis=-1)], axis=1)\n",
    "    \n",
    "    # Flatten back to original shape\n",
    "    flat_transformed = tf.reshape(transformed_xyz, [-1])\n",
    "    \n",
    "    return flat_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3a8b84c0-25d0-4607-8365-c608828576d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.08281422 0.325535   0.4        0.43096402 0.8735771  0.7       ], shape=(6,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Sample test\n",
    "flat_xyz_data = tf.constant([0.1, 0.3, 0.4, 0.5, 0.6, 0.7])\n",
    "transformed = spatial_random_affine(flat_xyz_data)\n",
    "print(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74c530c-02fb-4b27-b673-aabdd981ff2f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4b8dd930-3409-4476-a97c-f0521af1bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "\n",
    "def preprocess_lms(lm):\n",
    "    mean = tf_nan_mean(lm)\n",
    "    std = tf_nan_std(lm)\n",
    "    lm = (lm - mean)/std\n",
    "    lm = tf.where(tf.math.is_nan(lm),tf.constant(0.,lm.dtype),lm)\n",
    "    return lm\n",
    "\n",
    "def preprocess_cat(cat):\n",
    "    return tf_cat_to_num.lookup(cat)\n",
    "\n",
    "def preprocess_csv(line, augment=False):\n",
    "    columns = tf.strings.split(line, ',')\n",
    "    features = tf.strings.to_number(columns[:-1], out_type=tf.float32)\n",
    "    \n",
    "    if augment:\n",
    "        features = augment_lm(features)\n",
    "        \n",
    "    return preprocess_lms(features), preprocess_cat(columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bd981b71-1121-40e3-8bd6-4bacc3a02b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(csv_path, *, batch_size=1, drop_remainder=False, shuffle_size=None, repeat=False, cache=False, augment=False):\n",
    "    ds = tf.data.TextLineDataset(csv_path).skip(1) # skip header\n",
    "    ds = ds.map(lambda x: preprocess_csv(x, augment=augment), tf.data.AUTOTUNE)\n",
    "    \n",
    "    if repeat: \n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle_size:\n",
    "        ds = ds.shuffle(shuffle_size)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_deterministic = (False)\n",
    "        ds = ds.with_options(options)\n",
    "        \n",
    "    if batch_size > 1:\n",
    "        ds = ds.batch(batch_size)\n",
    "    \n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    \n",
    "    return ds\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_ds = get_dataset(PATH_TRAIN_DS, batch_size=batch_size, shuffle_size=4*batch_size, augment=True)\n",
    "valid_ds = get_dataset(PATH_VALID_DS, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2f83d8d5-de32-4290-8b33-478fc6eefa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(256, 63), dtype=float32, numpy=\n",
      "array([[ 0.91803676,  0.04994599, -1.1481365 , ...,  1.0608692 ,\n",
      "         0.2573768 , -1.2382442 ],\n",
      "       [ 0.43908828,  1.9541461 , -0.99697226, ...,  0.38656712,\n",
      "        -0.45505682, -1.3197205 ],\n",
      "       [-0.25387034,  1.6339612 , -0.9422369 , ..., -0.32827696,\n",
      "         1.5115246 , -1.0375955 ],\n",
      "       ...,\n",
      "       [-0.15190245,  1.5807458 , -0.6785015 , ..., -0.5493588 ,\n",
      "         1.6863092 , -0.8825209 ],\n",
      "       [-0.05439661,  2.4683104 , -0.81832457, ..., -0.54877067,\n",
      "         0.7105839 , -1.0772018 ],\n",
      "       [ 0.7113025 ,  1.34161   , -1.281158  , ...,  0.7795377 ,\n",
      "         1.0745475 , -1.4045508 ]], dtype=float32)>, <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
      "array([24,  4, 19,  2, 26, 10,  1,  6, 25, 12, 19, 32, 29, 17,  8, 30,  7,\n",
      "       22, 24, 32, 26, 18,  1,  9, 13, 11, 27,  1, 11,  5, 11,  9, 19, 20,\n",
      "       23, 20, 17, 18, 30, 30, 11, 16, 24, 14, 26,  1, 32, 30, 21, 14, 20,\n",
      "       18, 24, 15, 19, 17,  0, 27, 16, 24,  3,  7, 24,  5, 17, 22, 29, 32,\n",
      "       11, 32, 19,  0,  4, 23, 27, 18, 32, 19,  9, 23, 12, 28,  6, 25, 11,\n",
      "       16, 14, 14,  0,  9, 31, 26, 13, 29, 24, 11, 23, 32, 13,  4, 14,  2,\n",
      "       17, 12, 29, 32,  4,  9, 30, 17, 12, 20,  6, 24, 25, 14, 21,  5, 21,\n",
      "       15,  7,  4, 16, 22, 18, 18, 18,  4, 14,  9, 32,  6, 10,  4,  3, 10,\n",
      "       18,  0,  9,  6,  7, 24, 22, 18,  4, 32,  4, 13, 22,  9, 11, 31, 27,\n",
      "       11, 19, 12,  5, 20, 24, 16, 16, 24, 15,  4, 20, 16, 28, 22, 25, 15,\n",
      "       27, 22, 23, 26,  8, 13, 26, 26, 15,  4,  2, 13, 12, 28, 16, 25, 11,\n",
      "        4, 17, 21,  7, 19,  7,  8,  0, 31,  1, 17, 27,  8,  8, 26, 17, 10,\n",
      "       30, 21, 11, 23, 13, 18, 14, 12,  5, 14, 28, 19,  1,  8,  4,  6, 18,\n",
      "       13,  9, 29,  3,  7, 13,  5, 32, 16, 31, 27,  5, 30, 26,  3,  2, 24,\n",
      "       17,  9,  9, 32, 32,  2, 30,  5, 31, 17, 27,  0, 19, 11,  7, 23,  7,\n",
      "        2])>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1fb23-52de-4741-9f03-91648ec18570",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2fdbb7-ed17-4ff4-9e1a-a727ea7377cd",
   "metadata": {},
   "source": [
    "## Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7de3a2ce-726c-4d4a-a014-1407eddd151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 128)               8192      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 33)                2145      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18593 (72.63 KB)\n",
      "Trainable params: 18593 (72.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(63,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(len(sorted_cats), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f6ae0-8616-4969-aecc-42fce5a20b26",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e435e1db-89d0-41ca-ae05-25a07e24536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, 63, 1)             0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 61, 4)             16        \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 244)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               31360     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 33)                2145      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41777 (163.19 KB)\n",
      "Trainable params: 41777 (163.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(63,)),\n",
    "    layers.Lambda(lambda x: keras.backend.expand_dims(x, axis=-1)),\n",
    "    layers.Conv1D(4, 3, activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(len(sorted_cats), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bb7f4258-6e19-42f3-8b48-884c66cab4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "7/7 - 0s - loss: 0.7167 - accuracy: 0.7592 - val_loss: 0.4637 - val_accuracy: 0.8774 - 311ms/epoch - 44ms/step\n",
      "Epoch 2/30\n",
      "7/7 - 0s - loss: 0.6698 - accuracy: 0.7661 - val_loss: 0.4582 - val_accuracy: 0.8900 - 296ms/epoch - 42ms/step\n",
      "Epoch 3/30\n",
      "7/7 - 0s - loss: 0.6903 - accuracy: 0.7459 - val_loss: 0.4560 - val_accuracy: 0.8957 - 294ms/epoch - 42ms/step\n",
      "Epoch 4/30\n",
      "7/7 - 0s - loss: 0.6841 - accuracy: 0.7668 - val_loss: 0.4745 - val_accuracy: 0.8666 - 318ms/epoch - 45ms/step\n",
      "Epoch 5/30\n",
      "7/7 - 0s - loss: 0.6732 - accuracy: 0.7649 - val_loss: 0.4449 - val_accuracy: 0.8938 - 297ms/epoch - 42ms/step\n",
      "Epoch 6/30\n",
      "7/7 - 0s - loss: 0.6611 - accuracy: 0.7693 - val_loss: 0.4405 - val_accuracy: 0.8919 - 271ms/epoch - 39ms/step\n",
      "Epoch 7/30\n",
      "7/7 - 0s - loss: 0.6823 - accuracy: 0.7566 - val_loss: 0.4480 - val_accuracy: 0.8729 - 290ms/epoch - 41ms/step\n",
      "Epoch 8/30\n",
      "7/7 - 0s - loss: 0.6376 - accuracy: 0.7794 - val_loss: 0.4373 - val_accuracy: 0.8818 - 274ms/epoch - 39ms/step\n",
      "Epoch 9/30\n",
      "7/7 - 0s - loss: 0.6756 - accuracy: 0.7573 - val_loss: 0.4301 - val_accuracy: 0.8925 - 293ms/epoch - 42ms/step\n",
      "Epoch 10/30\n",
      "7/7 - 0s - loss: 0.6541 - accuracy: 0.7762 - val_loss: 0.4374 - val_accuracy: 0.8951 - 290ms/epoch - 41ms/step\n",
      "Epoch 11/30\n",
      "7/7 - 0s - loss: 0.6580 - accuracy: 0.7724 - val_loss: 0.4458 - val_accuracy: 0.8824 - 285ms/epoch - 41ms/step\n",
      "Epoch 12/30\n",
      "7/7 - 0s - loss: 0.6870 - accuracy: 0.7623 - val_loss: 0.4304 - val_accuracy: 0.8894 - 289ms/epoch - 41ms/step\n",
      "Epoch 13/30\n",
      "7/7 - 0s - loss: 0.6436 - accuracy: 0.7762 - val_loss: 0.4197 - val_accuracy: 0.9027 - 274ms/epoch - 39ms/step\n",
      "Epoch 14/30\n",
      "7/7 - 0s - loss: 0.6156 - accuracy: 0.7908 - val_loss: 0.4230 - val_accuracy: 0.8850 - 276ms/epoch - 39ms/step\n",
      "Epoch 15/30\n",
      "7/7 - 0s - loss: 0.6651 - accuracy: 0.7680 - val_loss: 0.4362 - val_accuracy: 0.8729 - 283ms/epoch - 40ms/step\n",
      "Epoch 16/30\n",
      "7/7 - 0s - loss: 0.6243 - accuracy: 0.7813 - val_loss: 0.4365 - val_accuracy: 0.8748 - 285ms/epoch - 41ms/step\n",
      "Epoch 17/30\n",
      "7/7 - 0s - loss: 0.6528 - accuracy: 0.7718 - val_loss: 0.4236 - val_accuracy: 0.8919 - 286ms/epoch - 41ms/step\n",
      "Epoch 18/30\n",
      "7/7 - 0s - loss: 0.6705 - accuracy: 0.7592 - val_loss: 0.4340 - val_accuracy: 0.8913 - 345ms/epoch - 49ms/step\n",
      "Epoch 19/30\n",
      "7/7 - 0s - loss: 0.6387 - accuracy: 0.7718 - val_loss: 0.4195 - val_accuracy: 0.8957 - 285ms/epoch - 41ms/step\n",
      "Epoch 20/30\n",
      "7/7 - 0s - loss: 0.6590 - accuracy: 0.7655 - val_loss: 0.4363 - val_accuracy: 0.8729 - 285ms/epoch - 41ms/step\n",
      "Epoch 21/30\n",
      "7/7 - 0s - loss: 0.6251 - accuracy: 0.7914 - val_loss: 0.4152 - val_accuracy: 0.8957 - 273ms/epoch - 39ms/step\n",
      "Epoch 22/30\n",
      "7/7 - 0s - loss: 0.6521 - accuracy: 0.7674 - val_loss: 0.4046 - val_accuracy: 0.8951 - 276ms/epoch - 39ms/step\n",
      "Epoch 23/30\n",
      "7/7 - 0s - loss: 0.6234 - accuracy: 0.7826 - val_loss: 0.4198 - val_accuracy: 0.8812 - 280ms/epoch - 40ms/step\n",
      "Epoch 24/30\n",
      "7/7 - 0s - loss: 0.6018 - accuracy: 0.7863 - val_loss: 0.4267 - val_accuracy: 0.8780 - 281ms/epoch - 40ms/step\n",
      "Epoch 25/30\n",
      "7/7 - 0s - loss: 0.6277 - accuracy: 0.7750 - val_loss: 0.3965 - val_accuracy: 0.9008 - 288ms/epoch - 41ms/step\n",
      "Epoch 26/30\n",
      "7/7 - 0s - loss: 0.5878 - accuracy: 0.7914 - val_loss: 0.3953 - val_accuracy: 0.8881 - 281ms/epoch - 40ms/step\n",
      "Epoch 27/30\n",
      "7/7 - 0s - loss: 0.6014 - accuracy: 0.7870 - val_loss: 0.3975 - val_accuracy: 0.8995 - 275ms/epoch - 39ms/step\n",
      "Epoch 28/30\n",
      "7/7 - 0s - loss: 0.5912 - accuracy: 0.7927 - val_loss: 0.3818 - val_accuracy: 0.9109 - 278ms/epoch - 40ms/step\n",
      "Epoch 29/30\n",
      "7/7 - 0s - loss: 0.5839 - accuracy: 0.7971 - val_loss: 0.3917 - val_accuracy: 0.8881 - 288ms/epoch - 41ms/step\n",
      "Epoch 30/30\n",
      "7/7 - 0s - loss: 0.6209 - accuracy: 0.7863 - val_loss: 0.3896 - val_accuracy: 0.8932 - 326ms/epoch - 47ms/step\n",
      "CPU times: total: 34 s\n",
      "Wall time: 9.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d1cd78f370>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_ds,\n",
    "          validation_data=valid_ds,\n",
    "          epochs=30,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44c544-9ae7-4cae-af45-2b697ded6e3e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5c8f7-96a1-4944-9d40-7f6193031ff8",
   "metadata": {},
   "source": [
    "## Util to work with video feeds using mediapipe hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ca5d2630-081d-452c-9926-3b980a3ddaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBCAM = 0\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def video_loop_mp_hands(source, process_data_func):\n",
    "    video = cv2.VideoCapture(source)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    try:\n",
    "        with mp_hands.Hands(model_complexity=1,max_num_hands=1,min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\n",
    "            while True:\n",
    "                _, image = video.read()\n",
    "    \n",
    "                if image is None:\n",
    "                    break\n",
    "\n",
    "                image. flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = hands.process(image)\n",
    "\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                image = cv2.flip(image, 1)\n",
    "                if results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in results.multi_hand_landmarks: # 1 hand only\n",
    "                        # Draw landmarks\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            image,\n",
    "                            hand_landmarks,\n",
    "                            mp_hands.HAND_CONNECTIONS,\n",
    "                            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                            mp_drawing_styles.get_default_hand_connections_style())\n",
    "        \n",
    "                        # Get bounding box for the hand\n",
    "                        lms = hand_landmarks.landmark\n",
    "                        x_coords = [lm.x for lm in lms]\n",
    "                        y_coords = [lm.y for lm in lms]\n",
    "                        min_x, max_x = int(min(x_coords) * image.shape[1]), int(max(x_coords) * image.shape[1])\n",
    "                        min_y, max_y = int(min(y_coords) * image.shape[0]), int(max(y_coords) * image.shape[0])\n",
    "\n",
    "                        image = cv2.flip(image, 1)\n",
    "                        # Draw the bounding box around the hand\n",
    "                        cv2.rectangle(image, (min_x, min_y), (max_x, max_y), (0, 255, 0), 2)  # RGB green\n",
    "                \n",
    "                        data = []\n",
    "                        for lm in lms:\n",
    "                            data.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "                        # Get prediction and probability from function\n",
    "                        pred, prob = process_data_func(data)\n",
    "\n",
    "                        image = cv2.flip(image, 1)\n",
    "                        # Display prediction and probability near the bounding box\n",
    "                        label = f\"{pred}: {prob:.2f}%\"\n",
    "                        cv2.putText(image, label, (min_x, min_y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "                \n",
    "                _, image = cv2.imencode('.jpeg', image)\n",
    "                display_handle.update(Image(data=image.tobytes()))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        video.release()\n",
    "        display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f1e4d-5070-4c87-840d-699e26fc472c",
   "metadata": {},
   "source": [
    "## Predict every frame model for isolated signs\n",
    "\n",
    "While this model performs well, but in certain positions and during changing poses, random predictions might happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "77428668-5946-4f7e-a727-b19a70df9e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_frame(frame):\n",
    "    x = preprocess_lms(frame)\n",
    "    probs = model(np.array([x]))\n",
    "    prob = np.max(probs, axis=1)[0]\n",
    "    pred = np.argmax(probs)\n",
    "    pred = num_to_cat[pred]\n",
    "    return pred, prob\n",
    "\n",
    "video_loop_mp_hands(WEBCAM, process_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff946c-2301-4227-8078-25375fdb1b4e",
   "metadata": {},
   "source": [
    "## Buffered model for isolated signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f35040de-5194-43d5-86e4-4b139d79267b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class BufferedModel:\n",
    "    def __init__(self, model, preprocess_fn, *, confidence, buffer_size):\n",
    "        self.model = model\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.confidence_threshold = int(buffer_size*confidence)\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        x = self.preprocess_fn(frame)\n",
    "        probs = self.model(np.array([x]))\n",
    "        prob = np.max(probs, axis=1)[0]\n",
    "        pred = np.argmax(probs)\n",
    "        \n",
    "        self.buffer.append(pred)\n",
    "        buffered_pred, count = Counter(self.buffer).most_common(1)[0]\n",
    "        if count >= self.confidence_threshold:\n",
    "            return num_to_cat[buffered_pred], count/self.buffer_size\n",
    "\n",
    "        return \"unknown\", 0.\n",
    "            \n",
    "buffered_model = BufferedModel(model, preprocess_lms, confidence=0.7, buffer_size=10)\n",
    "video_loop_mp_hands(WEBCAM, lambda data: buffered_model.process_frame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad106ef9-75bd-4ebd-9719-0d64fc1aeb39",
   "metadata": {},
   "source": [
    "## Continuous model with signing detection\n",
    "\n",
    "For this model the mediapipe holistic model is needed, gather more pose information than simply the 2 hands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4828a1-ea04-4885-8310-63eeb3e04cc1",
   "metadata": {},
   "source": [
    "### Handle mediapipe holistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b75a5056-2ead-4dd8-84e1-97f25abbcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_landmarks_on_image(image, results):\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    return image\n",
    "\n",
    "def video_loop_mp_holistics(source, process_data_func):\n",
    "    video = cv2.VideoCapture(source)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    try:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "            while True:\n",
    "                _, frame = video.read()\n",
    "    \n",
    "                if frame is None:\n",
    "                    break\n",
    "    \n",
    "                #image = cv2.resize(frame, (360, 240))\n",
    "                image=frame\n",
    "    \n",
    "                # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "                image.flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "\n",
    "                # Run the model\n",
    "                process_data_func(results)\n",
    "    \n",
    "                # Draw landmark annotation on the image.\n",
    "                image = draw_landmarks_on_image(image, results)\n",
    "    \n",
    "                image = cv2.flip(image, 1)\n",
    "                _, image = cv2.imencode('.jpeg', image)\n",
    "                display_handle.update(Image(data=image.tobytes()))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        video.release()\n",
    "        display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30f2fc-0094-4072-bc11-04c416c314b3",
   "metadata": {},
   "source": [
    "### Signing detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "18a82240-bb7a-4bb7-9cc4-ad7bc3eccb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose coordinates for hand movement.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "def extract_for_signing_detection(res):\n",
    "    # Extract specific pose landmarks if available\n",
    "    px = []\n",
    "    py = []\n",
    "    pz = []\n",
    "    if res.pose_landmarks:\n",
    "        for i in POSE:\n",
    "            lm = res.pose_landmarks.landmark[i]\n",
    "            px.append(lm.x)\n",
    "            py.append(lm.y)\n",
    "            pz.append(lm.z)\n",
    "    else:\n",
    "        px = [0.0]*len(POSE)\n",
    "        py = [0.0]*len(POSE)\n",
    "        pz = [0.0]*len(POSE)\n",
    "\n",
    "    # Extract left hand landmarks if available\n",
    "    lx = []\n",
    "    ly = []\n",
    "    lz = []\n",
    "    if res.left_hand_landmarks:\n",
    "        for lm in res.left_hand_landmarks.landmark:\n",
    "            lx.append(lm.x)\n",
    "            ly.append(lm.y)\n",
    "            lz.append(lm.z)\n",
    "    else:\n",
    "        lx = [0.0]*21\n",
    "        ly = [0.0]*21\n",
    "        lz = [0.0]*21\n",
    "\n",
    "    # Extract right hand landmarks if available\n",
    "    rx = []\n",
    "    ry = []\n",
    "    rz = []\n",
    "    if res.right_hand_landmarks:\n",
    "        for lm in res.right_hand_landmarks.landmark:\n",
    "            rx.append(lm.x)\n",
    "            ry.append(lm.y)\n",
    "            rz.append(lm.z)\n",
    "    else:\n",
    "        rx = [0.0]*21\n",
    "        ry = [0.0]*21\n",
    "        rz = [0.0]*21\n",
    "\n",
    "    return list(chain(rx, lx, px, ry, ly, py, rz, lz, pz))\n",
    "\n",
    "# Only load once\n",
    "signing_detection_model = tf.saved_model.load(\"signing_detection_model\")\n",
    "\n",
    "class SigningDetectionModel:\n",
    "    def __init__(self):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "\n",
    "    def is_signing(self, inp_lm):\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        return signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy() == 1\n",
    "\n",
    "class BufferedSigningDetectionModel:\n",
    "    def __init__(self, buffer_len=5, confidence_number=3):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "        self.signing_detector_buffer = deque(maxlen=buffer_len)\n",
    "        self.confidence_number = confidence_number \n",
    "\n",
    "    def is_signing(self, inp_lm):\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        pred = signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy()\n",
    "        self.signing_detector_buffer.append(pred)\n",
    "        buffered_pred, count = Counter(self.signing_detector_buffer).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            return buffered_pred == 1\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6e3b3560-61fa-4467-b847-ac264bbfd41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------*******a****************************b*****************************c****************************************l***********************************************r*************************************g****************************************-------------------------------------------------------------------------------------------------------------------------------------------------------------*******h***********************************5****************b*****************************t**********-------*t********y********************************t****************************y*********************t*******************y**************------------------------------"
     ]
    }
   ],
   "source": [
    "def extract_for_static_fingerspelling(res):\n",
    "    # Extract right hand landmarks if available\n",
    "    data = []\n",
    "    if res.right_hand_landmarks:\n",
    "        for lm in res.right_hand_landmarks.landmark:\n",
    "            data.append(lm.x)\n",
    "            data.append(lm.y)\n",
    "            data.append(lm.z)\n",
    "    elif res.left_hand_landmarks:\n",
    "        for lm in res.left_hand_landmarks.landmark:\n",
    "            data.append(lm.x)\n",
    "            data.append(lm.y)\n",
    "            data.append(lm.z)\n",
    "    else:\n",
    "        data = [0.0]*63\n",
    "        \n",
    "    return data\n",
    "\n",
    "class ContinuousModelWithSigningDetection:\n",
    "    def __init__(self):\n",
    "        self.signing_detection_model = BufferedSigningDetectionModel()\n",
    "        self.buffered_model = BufferedModel(model, preprocess_lms, confidence=0.7, buffer_size=10)\n",
    "        self.last_pred = None\n",
    "\n",
    "    def process_result(self, mp_result):\n",
    "        sd_inp = extract_for_signing_detection(mp_result)\n",
    "        if self.signing_detection_model.is_signing(sd_inp):\n",
    "            print(\"*\", end=\"\")\n",
    "            pred, prob = self.buffered_model.process_frame(extract_for_static_fingerspelling(mp_result))\n",
    "            if self.last_pred != pred and pred != \"unknown\":\n",
    "                self.last_pred = pred\n",
    "                print(pred, end=\"\")\n",
    "                return\n",
    "        else:\n",
    "            print(\"-\", end=\"\")\n",
    "            self.last_pred = None\n",
    "\n",
    "m = ContinuousModelWithSigningDetection()\n",
    "\n",
    "video_loop_mp_holistics(WEBCAM, lambda mp_res: m.process_result(mp_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f23b0a9-6740-4cf9-8b9b-b0c110054402",
   "metadata": {},
   "source": [
    "### LLm improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c5950366-bc33-4f10-be08-a7a45c32cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.environ.get('OPEN_AI_API_KEY')\n",
    "if key is not None:\n",
    "    openai.api_key = key\n",
    "else:\n",
    "    print(\"Error: Please set a valid api key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e3c5857a-28d1-4ca1-8c26-2edd21631e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_output(pred):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\", \n",
    "       messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a machine that tries to correct the output of a fingerspelling recognition model. Some letters might be missing, but it's also possible that the given text has extra characters. The model will always give i instead of j and z also doesn't work. Only reply the corrected text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"lebra\"},\n",
    "        {\"role\": \"system\", \"content\": \"zebra\"},\n",
    "        {\"role\": \"user\", \"content\": \"beark\"},\n",
    "        {\"role\": \"system\", \"content\": \"bear\"},\n",
    "        {\"role\": \"user\", \"content\": \"iagudar\"},\n",
    "        {\"role\": \"system\", \"content\": \"jaguar\"},\n",
    "        {\"role\": \"user\", \"content\": pred},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "641c1938-9efc-41c3-8b71-dbedf0c39978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jackal'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_output(\"iackdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "aff9f1c4-3bec-45d8-87ac-6e5f53f49410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zoom'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_output(\"loom\") # what happens if both are acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06832236-1f73-4787-96b8-8087a66ce314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
