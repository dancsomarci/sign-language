{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466495a9-b430-455b-b3f1-a4a3de52d80d",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "This notebook is meant for testing the trained encoder-decoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e1bf9-dabd-49ed-92f9-aa19a4a4fe2f",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8656ecc3-9850-40fe-9602-8ce195c6a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install tensorflow\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install mediapipe==0.9.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d9bab87b-6c62-42e8-9737-2151e3016478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import difflib\n",
    "import matplotlib\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pyarrow.parquet as pq\n",
    "from tensorflow.keras import layers\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a535ef19-6946-46c1-81c3-2dd5d347b070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n",
      "TensorFlow v2.14.0\n",
      "Mediapipe v0.9.0.1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4763f5-6df6-4065-acab-dc42d30c95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "cv2.setRNGSeed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3572f83-e4fc-4efc-a9e4-e1b70a15de8d",
   "metadata": {},
   "source": [
    "# Fetch from TfRecords\n",
    "\n",
    "To acquire the tfrecords you might want to run the data_handling notebook first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b267c6-41b6-453e-ba6c-d6be27179620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 53 TFRecord files.\n"
     ]
    }
   ],
   "source": [
    "PATH_KAGGLE_DS = \"kaggle_dataset\"\n",
    "dataset_df = pd.read_csv(os.path.join(PATH_KAGGLE_DS, \"supplemental_metadata.csv\"))\n",
    "PATH_TFRECORD_DS = os.path.join(PATH_KAGGLE_DS, \"test_tfrecords\")\n",
    "tf_records = dataset_df.file_id.map(lambda x: os.path.join(PATH_TFRECORD_DS, f\"{x}.tfrecord\")).unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efae36c2-583a-43e1-a3fc-7fe981f00709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_right_hand_0',\n",
       " 'x_right_hand_1',\n",
       " 'x_right_hand_2',\n",
       " 'x_right_hand_3',\n",
       " 'x_right_hand_4',\n",
       " 'x_right_hand_5',\n",
       " 'x_right_hand_6',\n",
       " 'x_right_hand_7',\n",
       " 'x_right_hand_8',\n",
       " 'x_right_hand_9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(PATH_TFRECORD_DS, \"feature_columns.json\"), 'r') as f:\n",
    "    json_str = f.read()\n",
    "FEATURE_COLUMNS = json.loads(json_str)\n",
    "FEATURE_COLUMNS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac39545-93d2-4944-809f-e196151a8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These points represent the hands, elbows, and shoulders.\n",
    "# LPOSE = [13, 15, 17, 19, 21]\n",
    "# RPOSE = [14, 16, 18, 20, 22]\n",
    "\n",
    "# # Facial information isn't necessary, but the nose will serve as a midpoint for normalizing the data, as it is usually located in the middle of the frame.\n",
    "# FPOSE = [0] # Nose as midpoint\n",
    "\n",
    "# # Collecting the indices of certain important/distinct sets of features.\n",
    "# # This can be beneficial during the preprocessing step.\n",
    "# RHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\n",
    "# LHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\n",
    "# RPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) in RPOSE]\n",
    "# LPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) in LPOSE]\n",
    "# MID_POINT_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) == 0] # Nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4c03c8-6571-45a2-ae76-53f0ada783b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COLUMNS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n",
    "    # Transpose to maintain the original shape of landmarks data.\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "    \n",
    "    return landmarks, phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "064d74e5-a641-4f6f-b7ce-44727079f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default mapping that came with the dataset was changed:\n",
    "# padding is represented with the number 0\n",
    "# start_token is 60\n",
    "# end_token is 61\n",
    "with open (os.path.join(PATH_KAGGLE_DS, \"character_to_prediction_index.json\"), \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "    \n",
    "char_to_num = {c:char_to_num[c]+1 for c in char_to_num}\n",
    "\n",
    "# Add pad_token, start pointer and end pointer to the dict\n",
    "pad_token = 'P'\n",
    "pad_token_idx = 0\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "\n",
    "start_token = '<'\n",
    "start_token_idx = 60\n",
    "char_to_num[start_token] = start_token_idx\n",
    "\n",
    "end_token = '>'\n",
    "end_token_idx = 61\n",
    "char_to_num[end_token] = end_token_idx\n",
    "\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1365b9-9f43-40aa-b4cf-83950d2f01dc",
   "metadata": {},
   "source": [
    "## Preprocess phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4177f0c1-bbe5-4fe7-a460-2404b6b8deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(char_to_num.keys()),\n",
    "        values=list(char_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"tf_char_to_num\"\n",
    ")\n",
    "\n",
    "# Function to decode the characters and pad the phrases\n",
    "MAX_PHRASE_LEN = 31 + 2 # The start and end token take space as well\n",
    "def preprocess_phrase(phrase):\n",
    "    phrase = start_token + phrase + end_token\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "    \n",
    "    max_len_plus = MAX_PHRASE_LEN + 1\n",
    "    amount_to_pad = max_len_plus - tf.shape(phrase)[0]\n",
    "    \n",
    "    if amount_to_pad > 0:\n",
    "        phrase = tf.pad(phrase, paddings=[[0, amount_to_pad]], mode = 'CONSTANT', constant_values = pad_token_idx)\n",
    "    else:\n",
    "        phrase = phrase[:max_len_plus]\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41469e7-8498-4afb-baa7-12fa3973a0df",
   "metadata": {},
   "source": [
    "Notice that landmarks don't need to be preprocessed, the saved model should contain it's own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbedc7dc-6e5f-405a-8b5c-d54abcc78e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(landmark, phrase):\n",
    "    phrase = preprocess_phrase(phrase)\n",
    "    return (landmark, phrase[:-1]), phrase[1:] # Shifted phrase for encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6fe826-2a06-4f37-9af9-0708da71e49a",
   "metadata": {},
   "source": [
    "## Create TFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07a5676-02d9-4664-b887-4b7486c066e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tfrecords, batch_size=1, repeat=False, shuffle=False, drop_remainder=False, cache=False):\n",
    "    ds = tf.data.TFRecordDataset(tf_records)\n",
    "    ds = ds.map(decode_fn, tf.data.AUTOTUNE)\n",
    "    # Note: preprocessing can happen before and after the batching (if you can preprocess the whole batch at once to save computation time)\n",
    "    ds = ds.map(preprocess, tf.data.AUTOTUNE)\n",
    "    \n",
    "    if repeat: \n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_deterministic = (False)\n",
    "        ds = ds.with_options(options)\n",
    "\n",
    "    if batch_size >= 1:\n",
    "        # There's also a padded_batch version of this function\n",
    "        ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "        \n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # If the system doesn't have enough RAM caching might slow down the process\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    \n",
    "    return ds\n",
    "\n",
    "test_ds = get_dataset(tf_records, batch_size=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd0914d-0fc4-4444-9b7a-0663df18d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "\n",
      "Saved shapes:\n",
      "lm_shape: 159\n",
      "phrase_shape: 1\n",
      "----------------------------------------\n",
      "\n",
      "Encoder input - first in batch (Landmarks:)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 178, 159)\n",
      "tf.Tensor(\n",
      "[[ 0.12627919  0.22162548  0.3116899  ... -2.7103877  -2.565694\n",
      "  -2.1914308 ]\n",
      " [        nan         nan         nan ... -2.791268   -2.6042914\n",
      "  -2.3839378 ]\n",
      " [        nan         nan         nan ... -2.7606401  -2.5794535\n",
      "  -2.3470478 ]\n",
      " ...\n",
      " [        nan         nan         nan ... -2.8067975  -2.6485422\n",
      "  -1.3479928 ]\n",
      " [ 0.13762568  0.20138527  0.25962168 ... -2.979717   -2.6853716\n",
      "  -1.9384248 ]\n",
      " [ 0.10258354  0.1709002   0.24561381 ... -3.1564484  -2.9503818\n",
      "  -2.201033  ]], shape=(178, 159), dtype=float32)\n",
      "----------------------------------------\n",
      "\n",
      "Decoder input (Context):\n",
      "(1, 33)\n",
      "tf.Tensor(\n",
      "[60 41 46 52 37 50 37 51 52 41 46 39  1 47 34 51 37 50 54 33 52 41 47 46\n",
      "  1 55 33 51  1 45 33 36 37], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n",
      "\n",
      "Model target output (Phrase):\n",
      "(1, 33)\n",
      "tf.Tensor(\n",
      "[41 46 52 37 50 37 51 52 41 46 39  1 47 34 51 37 50 54 33 52 41 47 46  1\n",
      " 55 33 51  1 45 33 36 37 61], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lm_shape = None\n",
    "phrase_shape = None\n",
    "\n",
    "# Create an iterator for the train and valid datasets\n",
    "test_iterator = iter(test_ds)\n",
    "\n",
    "# Print data points from the training dataset\n",
    "print(\"Training Data:\\n\")\n",
    "(landmarks, context), phrase = next(test_iterator)\n",
    "\n",
    "# Save shapes\n",
    "lm_shape = landmarks.shape[2]\n",
    "phrase_shape = phrase.shape[0]\n",
    "print(\"Saved shapes:\")\n",
    "print(f\"lm_shape: {lm_shape}\")\n",
    "print(f\"phrase_shape: {phrase_shape}\")\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Encoder input - first in batch (Landmarks:)\")\n",
    "print(type(landmarks))\n",
    "print(landmarks.shape)\n",
    "print(landmarks[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Decoder input (Context):\")\n",
    "print(context.shape)\n",
    "print(context[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Model target output (Phrase):\")\n",
    "print(phrase.shape)\n",
    "print(phrase[0])\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e35d-d23c-4d31-bf5a-77aa94e48802",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9e8b5-0a63-4dd2-9ca2-803ffea6fba4",
   "metadata": {},
   "source": [
    "Load the model from saved model format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1862c9ed-944a-48ce-b925-846a109d46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(\"GRU_local_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b335e-e782-4451-bbf3-aaf71ddc9ab3",
   "metadata": {},
   "source": [
    "- The models preprocess the input inside with the method that was used during training.\n",
    "- The return types are tensorflow specific, so one needs to call .numpy() on them to get the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14b7caa1-ed72-4dab-94fb-ac47e636ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 30.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.55950594>,\n",
       " 'result': <tf.Tensor: shape=(), dtype=string, numpy=b'h'>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "loaded_model.predict(np.zeros((10, len(FEATURE_COLUMNS))), \"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0476617-b356-49e8-97cb-7764bb632017",
   "metadata": {},
   "source": [
    "Each model stores the landmarks that are needed for it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12ee5add-bdc8-4b26-99b9-520440089aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(159,), dtype=string, numpy=\n",
       "array([b'x_right_hand_0', b'x_right_hand_1', b'x_right_hand_2',\n",
       "       b'x_right_hand_3', b'x_right_hand_4', b'x_right_hand_5',\n",
       "       b'x_right_hand_6', b'x_right_hand_7', b'x_right_hand_8',\n",
       "       b'x_right_hand_9', b'x_right_hand_10', b'x_right_hand_11',\n",
       "       b'x_right_hand_12', b'x_right_hand_13', b'x_right_hand_14',\n",
       "       b'x_right_hand_15', b'x_right_hand_16', b'x_right_hand_17',\n",
       "       b'x_right_hand_18', b'x_right_hand_19', b'x_right_hand_20',\n",
       "       b'x_left_hand_0', b'x_left_hand_1', b'x_left_hand_2',\n",
       "       b'x_left_hand_3', b'x_left_hand_4', b'x_left_hand_5',\n",
       "       b'x_left_hand_6', b'x_left_hand_7', b'x_left_hand_8',\n",
       "       b'x_left_hand_9', b'x_left_hand_10', b'x_left_hand_11',\n",
       "       b'x_left_hand_12', b'x_left_hand_13', b'x_left_hand_14',\n",
       "       b'x_left_hand_15', b'x_left_hand_16', b'x_left_hand_17',\n",
       "       b'x_left_hand_18', b'x_left_hand_19', b'x_left_hand_20',\n",
       "       b'x_pose_13', b'x_pose_15', b'x_pose_17', b'x_pose_19',\n",
       "       b'x_pose_21', b'x_pose_14', b'x_pose_16', b'x_pose_18',\n",
       "       b'x_pose_20', b'x_pose_22', b'x_pose_0', b'y_right_hand_0',\n",
       "       b'y_right_hand_1', b'y_right_hand_2', b'y_right_hand_3',\n",
       "       b'y_right_hand_4', b'y_right_hand_5', b'y_right_hand_6',\n",
       "       b'y_right_hand_7', b'y_right_hand_8', b'y_right_hand_9',\n",
       "       b'y_right_hand_10', b'y_right_hand_11', b'y_right_hand_12',\n",
       "       b'y_right_hand_13', b'y_right_hand_14', b'y_right_hand_15',\n",
       "       b'y_right_hand_16', b'y_right_hand_17', b'y_right_hand_18',\n",
       "       b'y_right_hand_19', b'y_right_hand_20', b'y_left_hand_0',\n",
       "       b'y_left_hand_1', b'y_left_hand_2', b'y_left_hand_3',\n",
       "       b'y_left_hand_4', b'y_left_hand_5', b'y_left_hand_6',\n",
       "       b'y_left_hand_7', b'y_left_hand_8', b'y_left_hand_9',\n",
       "       b'y_left_hand_10', b'y_left_hand_11', b'y_left_hand_12',\n",
       "       b'y_left_hand_13', b'y_left_hand_14', b'y_left_hand_15',\n",
       "       b'y_left_hand_16', b'y_left_hand_17', b'y_left_hand_18',\n",
       "       b'y_left_hand_19', b'y_left_hand_20', b'y_pose_13', b'y_pose_15',\n",
       "       b'y_pose_17', b'y_pose_19', b'y_pose_21', b'y_pose_14',\n",
       "       b'y_pose_16', b'y_pose_18', b'y_pose_20', b'y_pose_22',\n",
       "       b'y_pose_0', b'z_right_hand_0', b'z_right_hand_1',\n",
       "       b'z_right_hand_2', b'z_right_hand_3', b'z_right_hand_4',\n",
       "       b'z_right_hand_5', b'z_right_hand_6', b'z_right_hand_7',\n",
       "       b'z_right_hand_8', b'z_right_hand_9', b'z_right_hand_10',\n",
       "       b'z_right_hand_11', b'z_right_hand_12', b'z_right_hand_13',\n",
       "       b'z_right_hand_14', b'z_right_hand_15', b'z_right_hand_16',\n",
       "       b'z_right_hand_17', b'z_right_hand_18', b'z_right_hand_19',\n",
       "       b'z_right_hand_20', b'z_left_hand_0', b'z_left_hand_1',\n",
       "       b'z_left_hand_2', b'z_left_hand_3', b'z_left_hand_4',\n",
       "       b'z_left_hand_5', b'z_left_hand_6', b'z_left_hand_7',\n",
       "       b'z_left_hand_8', b'z_left_hand_9', b'z_left_hand_10',\n",
       "       b'z_left_hand_11', b'z_left_hand_12', b'z_left_hand_13',\n",
       "       b'z_left_hand_14', b'z_left_hand_15', b'z_left_hand_16',\n",
       "       b'z_left_hand_17', b'z_left_hand_18', b'z_left_hand_19',\n",
       "       b'z_left_hand_20', b'z_pose_13', b'z_pose_15', b'z_pose_17',\n",
       "       b'z_pose_19', b'z_pose_21', b'z_pose_14', b'z_pose_16',\n",
       "       b'z_pose_18', b'z_pose_20', b'z_pose_22', b'z_pose_0'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953171f-ca12-4f7e-8406-02b512a6e4d3",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81060d38-2986-4f3a-a599-65128c526d36",
   "metadata": {},
   "source": [
    "## Metrics on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ced00ed1-7cd5-48e0-83e8-e8aaadaf528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(fs_model, inp, max_len):\n",
    "    ctx = str(num_to_char[start_token_idx])\n",
    "    for i in range(max_len):\n",
    "        res = fs_model.predict(inp, ctx)\n",
    "        res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        ctx += res_char\n",
    "\n",
    "        if res_char == num_to_char[end_token_idx]:\n",
    "            break\n",
    "    return ctx\n",
    "\n",
    "def generate_teacher_forcing(fs_model, inp, expected):\n",
    "    pred = str(num_to_char[start_token_idx])\n",
    "    ctx = str(num_to_char[start_token_idx])\n",
    "    for e in expected:\n",
    "        if e == 'P':\n",
    "            break\n",
    "        res = fs_model.predict(inp, ctx)\n",
    "        res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        pred += res_char\n",
    "        ctx += e\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0da00211-3634-4ca5-ad2c-37c90ebbecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(a, b):\n",
    "    return sum(1 for _ in difflib.ndiff(a, b) if '+' in _ or '-' in _)\n",
    "\n",
    "def word_error_rate(ref, hyp):\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "    distance = levenshtein_distance(ref_words, hyp_words)\n",
    "    return distance / max(len(ref_words), 1)\n",
    "\n",
    "def character_error_rate(ref, hyp):\n",
    "    distance = levenshtein_distance(ref, hyp)\n",
    "    return distance / max(len(ref), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "362bf853-eaf4-45c0-af7b-7a5f6d192f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edit_dist_gen_on_own</th>\n",
       "      <th>edit_dist_tf</th>\n",
       "      <th>wer_gen_on_own</th>\n",
       "      <th>wer_tf</th>\n",
       "      <th>cer_gen_on_own</th>\n",
       "      <th>cer_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>51</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.545455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   edit_dist_gen_on_own  edit_dist_tf  wer_gen_on_own  wer_tf  cer_gen_on_own  \\\n",
       "0                    30            51            1.25    1.25        0.909091   \n",
       "\n",
       "     cer_tf  \n",
       "0  1.545455  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for (inp_batch, _ctx), expected_batch in test_ds:\n",
    "    for seq, expected in zip(inp_batch, expected_batch):\n",
    "        expected = \"\".join([num_to_char[num.numpy()] for num in expected])\n",
    "\n",
    "        gen_on_own = generate(loaded_model, seq, MAX_PHRASE_LEN)\n",
    "        gen_teacher_forcing = generate_teacher_forcing(loaded_model, seq, expected)\n",
    "\n",
    "        data.append([levenshtein_distance(expected, gen_on_own),\n",
    "                     levenshtein_distance(expected, gen_teacher_forcing),\n",
    "                     word_error_rate(expected, gen_on_own),\n",
    "                     word_error_rate(expected, gen_teacher_forcing),\n",
    "                     character_error_rate(expected, gen_on_own),\n",
    "                     character_error_rate(expected, gen_teacher_forcing)])\n",
    "    \n",
    "        # print(\"Expected: \" + expected)\n",
    "        # print(\"Gen on own: \" + generate(loaded_model, seq, MAX_PHRASE_LEN))\n",
    "        # print(\"Gen teacher forcing: \" + generate_teacher_forcing(loaded_model, seq, expected))\n",
    "        # print('\\n~~~\\n')\n",
    "\n",
    "    break\n",
    "columns = [\"edit_dist_gen_on_own\",\n",
    "           \"edit_dist_tf\",\n",
    "           \"wer_gen_on_own\",\n",
    "           \"wer_tf\",\n",
    "           \"cer_gen_on_own\",\n",
    "           \"cer_tf\"]\n",
    "stat_df = pd.DataFrame(data, columns=columns)\n",
    "stat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a70eb2-41c4-442c-86cc-8dcb4d950287",
   "metadata": {},
   "source": [
    "## Real world testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb05ebcf-01f8-4f3b-b809-60aeae424ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_landmarks_on_image(image, results):\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d1e5e58-c415-4b6c-9c17-4e2a0a38cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBCAM = 0\n",
    "\n",
    "def video_loop(source, process_result_func):\n",
    "    video = cv2.VideoCapture(source)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    try:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "            while True:\n",
    "                _, frame = video.read()\n",
    "    \n",
    "                if frame is None:\n",
    "                    break\n",
    "    \n",
    "                #image = cv2.resize(frame, (360, 240))\n",
    "                image=frame\n",
    "    \n",
    "                # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "                image.flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                process_result_func(results)\n",
    "    \n",
    "                # Draw landmark annotation on the image.\n",
    "                image = draw_landmarks_on_image(image, results)\n",
    "    \n",
    "                image = cv2.flip(image, 1)\n",
    "                _, image = cv2.imencode('.jpeg', image)\n",
    "                display_handle.update(Image(data=image.tobytes()))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        video.release()\n",
    "        display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb89a3d-5617-4473-8839-5f36ee989460",
   "metadata": {},
   "source": [
    "### Signing detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6b7b9ef-afe7-4e27-a48b-55fdccdbd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose coordinates for hand movement.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "def extract_for_signing_detection(res):\n",
    "    # Extract specific pose landmarks if available\n",
    "    px = []\n",
    "    py = []\n",
    "    pz = []\n",
    "    if res.pose_landmarks:\n",
    "        for i in POSE:\n",
    "            lm = res.pose_landmarks.landmark[i]\n",
    "            px.append(lm.x)\n",
    "            py.append(lm.y)\n",
    "            pz.append(lm.z)\n",
    "    else:\n",
    "        px = [0.0]*len(POSE)\n",
    "        py = [0.0]*len(POSE)\n",
    "        pz = [0.0]*len(POSE)\n",
    "\n",
    "    # Extract left hand landmarks if available\n",
    "    lx = []\n",
    "    ly = []\n",
    "    lz = []\n",
    "    if res.left_hand_landmarks:\n",
    "        for lm in res.left_hand_landmarks.landmark:\n",
    "            lx.append(lm.x)\n",
    "            ly.append(lm.y)\n",
    "            lz.append(lm.z)\n",
    "    else:\n",
    "        lx = [0.0]*21\n",
    "        ly = [0.0]*21\n",
    "        lz = [0.0]*21\n",
    "\n",
    "    # Extract right hand landmarks if available\n",
    "    rx = []\n",
    "    ry = []\n",
    "    rz = []\n",
    "    if res.right_hand_landmarks:\n",
    "        for lm in res.right_hand_landmarks.landmark:\n",
    "            rx.append(lm.x)\n",
    "            ry.append(lm.y)\n",
    "            rz.append(lm.z)\n",
    "    else:\n",
    "        rx = [0.0]*21\n",
    "        ry = [0.0]*21\n",
    "        rz = [0.0]*21\n",
    "\n",
    "    return list(chain(rx, lx, px, ry, ly, py, rz, lz, pz))\n",
    "\n",
    "# Only load once\n",
    "signing_detection_model = tf.saved_model.load(\"signing_detection_model\")\n",
    "\n",
    "class SigningDetectionModel:\n",
    "    def __init__(self):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "\n",
    "    def is_signing(self, mp_holistic_result):\n",
    "        inp_lm = extract_for_signing_detection(mp_holistic_result)\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        return signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy() == 1\n",
    "\n",
    "class BufferedSigningDetectionModel:\n",
    "    def __init__(self, buffer_len=5, confidence_number=3):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "        self.signing_detector_buffer = deque(maxlen=buffer_len)\n",
    "        self.confidence_number = confidence_number \n",
    "\n",
    "    def is_signing(self, mp_holistic_result):\n",
    "        inp_lm = extract_for_signing_detection(mp_holistic_result)\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        pred = signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy()\n",
    "        self.signing_detector_buffer.append(pred)\n",
    "        buffered_pred, count = Counter(self.signing_detector_buffer).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            return buffered_pred == 1\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5db61d-850f-4549-8b85-1e32f87fe04d",
   "metadata": {},
   "source": [
    "### Fingerspelling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "364d8881-9eaf-4b16-9b2b-f5320aa80a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that currently the formatter only supports pose and hand landmarks not face landmarks\n",
    "class ModelInputFormatter:\n",
    "    def __init__(self, model):\n",
    "        self.required_landmarks = [bytes.decode(\"utf-8\") for bytes in model.info().numpy()]\n",
    "\n",
    "    def get_model_input(self, mp_holistic_result):\n",
    "        (rx, ry, rz), (lx, ly, lz), (px, py, pz) = self._extract_from_result(mp_holistic_result)\n",
    "\n",
    "        mapped_list = []\n",
    "        for item in self.required_landmarks:\n",
    "            parts = item.split('_')\n",
    "            idx = int(parts[-1]) # Extract the index\n",
    "        \n",
    "            if parts[0] == 'x':\n",
    "                if 'right_hand' in item:\n",
    "                    mapped_list.append(rx[idx])\n",
    "                elif 'left_hand' in item:\n",
    "                    mapped_list.append(lx[idx])\n",
    "                elif 'pose' in item:\n",
    "                    mapped_list.append(px[idx])\n",
    "        \n",
    "            elif parts[0] == 'y':\n",
    "                if 'right_hand' in item:\n",
    "                    mapped_list.append(ry[idx])\n",
    "                elif 'left_hand' in item:\n",
    "                    mapped_list.append(ly[idx])\n",
    "                elif 'pose' in item:\n",
    "                    mapped_list.append(py[idx])\n",
    "        \n",
    "            elif parts[0] == 'z':\n",
    "                if 'right_hand' in item:\n",
    "                    mapped_list.append(rz[idx])\n",
    "                elif 'left_hand' in item:\n",
    "                    mapped_list.append(lz[idx])\n",
    "                elif 'pose' in item:\n",
    "                    mapped_list.append(pz[idx])\n",
    "\n",
    "        return mapped_list\n",
    "\n",
    "    def _extract_from_result(self, res):\n",
    "        # Extract specific pose landmarks if available\n",
    "        px = []\n",
    "        py = []\n",
    "        pz = []\n",
    "        if res.pose_landmarks:\n",
    "            for lm in res.pose_landmarks.landmark:\n",
    "                px.append(lm.x)\n",
    "                py.append(lm.y)\n",
    "                pz.append(lm.z)\n",
    "        else:\n",
    "            px = [0.0]*len(POSE)\n",
    "            py = [0.0]*len(POSE)\n",
    "            pz = [0.0]*len(POSE)\n",
    "    \n",
    "        # Extract left hand landmarks if available\n",
    "        lx = []\n",
    "        ly = []\n",
    "        lz = []\n",
    "        if res.left_hand_landmarks:\n",
    "            for lm in res.left_hand_landmarks.landmark:\n",
    "                lx.append(lm.x)\n",
    "                ly.append(lm.y)\n",
    "                lz.append(lm.z)\n",
    "        else:\n",
    "            lx = [0.0]*21\n",
    "            ly = [0.0]*21\n",
    "            lz = [0.0]*21\n",
    "    \n",
    "        # Extract right hand landmarks if available\n",
    "        rx = []\n",
    "        ry = []\n",
    "        rz = []\n",
    "        if res.right_hand_landmarks:\n",
    "            for lm in res.right_hand_landmarks.landmark:\n",
    "                rx.append(lm.x)\n",
    "                ry.append(lm.y)\n",
    "                rz.append(lm.z)\n",
    "        else:\n",
    "            rx = [0.0]*21\n",
    "            ry = [0.0]*21\n",
    "            rz = [0.0]*21\n",
    "    \n",
    "        return (rx, ry, rz), (lx, ly, lz), (px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41e001-5e4c-4f8f-8ffb-0ade52357348",
   "metadata": {},
   "source": [
    "#### Continuous model\n",
    "\n",
    "Works well for isolated sequences. Can't handle sudden pauses, and stops.\n",
    "Extremely sensitive to window size. Also, the training data was from professional signers. For beginners who sign slower the same window size isn't suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d9ed11c3-636d-4783-9831-04bd4e32280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousRecognitionModel:\n",
    "    def __init__(self, model, confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7):\n",
    "        self.model = model\n",
    "        self.formatter = ModelInputFormatter(self.model)\n",
    "\n",
    "        # Collect a maximum of inp_buf_len frames for inference\n",
    "        self.input = deque(maxlen=inp_buf_len)\n",
    "        self.inp_len = inp_buf_len\n",
    "\n",
    "        # The output is also buffered\n",
    "        self.inner_fifo = deque(maxlen=out_buf_len)\n",
    "        # Only predictions with a higher confidence make it inside the buffer\n",
    "        self.trust_confidence = confidence_threshold\n",
    "        # Need a confidence_number majority in the buffer to be returned as output\n",
    "        self.confidence_number = out_majority_threshold\n",
    "        # Previous predictions for the model\n",
    "        self.context = str(num_to_char[start_token_idx])\n",
    "\n",
    "    def process_frame(self, mp_holistic_result):\n",
    "        selected_landmarks_for_model = formatter.get_model_input(mp_holistic_result)\n",
    "        self.input.append(selected_landmarks_for_model)\n",
    "    \n",
    "        res = self.model.predict(self.input, self.context)\n",
    "        pred = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        prob = res[\"confidence\"].numpy()\n",
    "\n",
    "        if prob < self.trust_confidence:\n",
    "            return None\n",
    "\n",
    "        self.inner_fifo.append(pred)\n",
    "        pred_char, count = Counter(self.inner_fifo).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            if self.context[-1] != pred_char:\n",
    "                self.context += pred_char\n",
    "                print(pred_char, end=\"\")\n",
    "                \n",
    "                # Predicted the end\n",
    "                # if pred_char == '>':\n",
    "                #     # restart the detection\n",
    "                #     self.context = str(num_to_char[start_token_idx])\n",
    "                #     self.inner_fifo.clear()\n",
    "                #     self.input.clear()\n",
    "\n",
    "                return pred_char\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2aaecdc5-2948-4514-8b41-4f3ca8a6817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear> "
     ]
    }
   ],
   "source": [
    "fs_model = ContinuousRecognitionModel(loaded_model, confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7)\n",
    "video_loop(os.path.join(\"test_videos\", \"bear.mp4\"), lambda data: fs_model.process_frame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a0ac9-bbc8-4f4c-93f2-d8e34c687cd8",
   "metadata": {},
   "source": [
    "#### Translate in long chunks\n",
    "\n",
    "This model performs well on single words that fit into the buffer. But for longer text it fails to translate well, presumable because the signs are cut off at the wrong positions. For longer text pause detection/signing detection is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a278ff2-c389-4e32-83f2-e07b6d25dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonContinuousRecognitionModel:\n",
    "    def __init__(self, model, max_out_length=31, confidence_threshold=0.2):\n",
    "        self.model = model\n",
    "        self.formatter = ModelInputFormatter(self.model)\n",
    "\n",
    "        self.max_out_length = max_out_length\n",
    "        # Only predictions with a higher confidence count as a predicted character\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.input = []\n",
    "\n",
    "    def reset_buffer(self):\n",
    "        self.input.clear()\n",
    "\n",
    "    def translate_buffer(self, reset_buffer=False):\n",
    "        res = None\n",
    "        if len(self.input) > 0:\n",
    "            res = self._generate_with_confidence()\n",
    "            \n",
    "        if reset_buffer:\n",
    "            self.reset_buffer()\n",
    "            \n",
    "        return res\n",
    "\n",
    "    def process_frame(self, mp_holistic_result):\n",
    "        selected_landmarks_for_model = formatter.get_model_input(mp_holistic_result)\n",
    "        self.input.append(selected_landmarks_for_model)\n",
    "\n",
    "    def _generate_with_confidence(self):\n",
    "        ctx = str(num_to_char[start_token_idx])\n",
    "        for i in range(self.max_out_length):\n",
    "            res = self.model.predict(self.input, ctx)\n",
    "            res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "            prob = res[\"confidence\"].numpy()\n",
    "            if prob > self.confidence_threshold:\n",
    "                ctx += res_char\n",
    "                if res_char == num_to_char[end_token_idx]:\n",
    "                    break\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d686a5eb-f859-4263-98ee-8aa97a2d1269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<aliga trove>'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(loaded_model, max_out_length=31, confidence_threshold=0.0)\n",
    "video_loop(os.path.join(\"test_videos\", \"alligator.mp4\"), lambda data: fs_model.process_frame(data))\n",
    "fs_model.translate_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cad2ab0e-7887-4198-b546-4023a052152c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<mookeye>'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(loaded_model, max_out_length=MAX_PHRASE_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "#sign_detector = BufferedSigningDetectionModel()\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "\n",
    "video_loop(os.path.join(\"test_videos\", \"monkey.mp4\"), process_data)\n",
    "fs_model.translate_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3bf55500-bb63-40f8-b325-a9dc9221f318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<era lu>\n",
      "<tegue pere>\n",
      "<terba riber>\n",
      "<hat\n",
      "<jean andrad>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(loaded_model, max_out_length=MAX_PHRASE_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "sign_detector = BufferedSigningDetectionModel(buffer_len=10, confidence_number=7)\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "    else:\n",
    "        res = fs_model.translate_buffer(reset_buffer=True)\n",
    "        if res and res != \"<\":\n",
    "            print(res)\n",
    "\n",
    "video_loop(os.path.join(\"test_videos\", \"fingerspelling_animals.mp4\"), process_data)\n",
    "fs_model.translate_buffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bde39c-08ef-480f-bc05-2ace53c091ed",
   "metadata": {},
   "source": [
    "### Correct with llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6c8f2fb-d561-4e38-95bb-fe45beae8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.environ.get('OPEN_AI_API_KEY')\n",
    "if key is not None:\n",
    "    openai.api_key = key\n",
    "else:\n",
    "    print(\"Error: Please set a valid api key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4bdbba8-547d-4e39-b04a-8ffda0018eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_output(pred):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\", \n",
    "       messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a machine that tries to correct the output of a fingerspelling recognition model. Some letters might be missing, but it's also possible that the given text has extra characters. Only reply the corrected text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"angaro angaro\"},\n",
    "        {\"role\": \"system\", \"content\": \"kangaroo\"},\n",
    "        {\"role\": \"user\", \"content\": \"beark\"},\n",
    "        {\"role\": \"system\", \"content\": \"bear\"},\n",
    "        {\"role\": \"user\", \"content\": \"6 halee hale\"},\n",
    "        {\"role\": \"system\", \"content\": \"whale\"},\n",
    "        {\"role\": \"user\", \"content\": pred},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e94ca388-1937-4af7-9532-ad3a865a1814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiger, tiger, kangaroo, alligator, horse'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_output(\"earkh/tiger/tiger angar key ligator alligator h horse gro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c7d3ddbb-ba75-4de9-ae78-43f60fb8e09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era lu\n",
      "tug of war\n",
      "riverbank\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<hat'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(loaded_model, max_out_length=MAX_PHRASE_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "sign_detector = BufferedSigningDetectionModel(buffer_len=10, confidence_number=7)\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "    else:\n",
    "        res = fs_model.translate_buffer(reset_buffer=True)\n",
    "        if res and res != \"<\":\n",
    "            print(correct_output(res[1:]))\n",
    "\n",
    "video_loop(os.path.join(\"test_videos\", \"fingerspelling_animals.mp4\"), process_data)\n",
    "fs_model.translate_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885df677-3186-4225-9691-b7299b2a096f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
