{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466495a9-b430-455b-b3f1-a4a3de52d80d",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "This notebook is meant for testing the trained encoder-decoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e1bf9-dabd-49ed-92f9-aa19a4a4fe2f",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8656ecc3-9850-40fe-9602-8ce195c6a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install tensorflow\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install mediapipe==0.9.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bab87b-6c62-42e8-9737-2151e3016478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pyarrow.parquet as pq\n",
    "from tensorflow.keras import layers\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a535ef19-6946-46c1-81c3-2dd5d347b070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n",
      "TensorFlow v2.14.0\n",
      "Mediapipe v0.9.0.1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4763f5-6df6-4065-acab-dc42d30c95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "cv2.setRNGSeed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3572f83-e4fc-4efc-a9e4-e1b70a15de8d",
   "metadata": {},
   "source": [
    "# Fetch from TfRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0b267c6-41b6-453e-ba6c-d6be27179620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 53 TFRecord files.\n"
     ]
    }
   ],
   "source": [
    "PATH_KAGGLE_DS = \"kaggle_dataset\"\n",
    "dataset_df = pd.read_csv(os.path.join(PATH_KAGGLE_DS, \"supplemental_metadata.csv\"))\n",
    "PATH_TFRECORD_DS = os.path.join(PATH_KAGGLE_DS, \"test_tfrecords\")\n",
    "tf_records = dataset_df.file_id.map(lambda x: os.path.join(PATH_TFRECORD_DS, f\"{x}.tfrecord\")).unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efae36c2-583a-43e1-a3fc-7fe981f00709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_right_hand_0',\n",
       " 'x_right_hand_1',\n",
       " 'x_right_hand_2',\n",
       " 'x_right_hand_3',\n",
       " 'x_right_hand_4',\n",
       " 'x_right_hand_5',\n",
       " 'x_right_hand_6',\n",
       " 'x_right_hand_7',\n",
       " 'x_right_hand_8',\n",
       " 'x_right_hand_9']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(PATH_TFRECORD_DS, \"feature_columns.json\"), 'r') as f:\n",
    "    json_str = f.read()\n",
    "FEATURE_COLUMNS = json.loads(json_str)\n",
    "FEATURE_COLUMNS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ac39545-93d2-4944-809f-e196151a8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These points represent the hands, elbows, and shoulders.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "\n",
    "# Facial information isn't necessary, but the nose will serve as a midpoint for normalizing the data, as it is usually located in the middle of the frame.\n",
    "FPOSE = [0] # Nose as midpoint\n",
    "\n",
    "# Collecting the indices of certain important/distinct sets of features.\n",
    "# This can be beneficial during the preprocessing step.\n",
    "RHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\n",
    "LHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\n",
    "RPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) in RPOSE]\n",
    "LPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) in LPOSE]\n",
    "MID_POINT_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) == 0] # Nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e4c03c8-6571-45a2-ae76-53f0ada783b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COLUMNS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n",
    "    # Transpose to maintain the original shape of landmarks data.\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "    \n",
    "    return landmarks, phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "064d74e5-a641-4f6f-b7ce-44727079f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default mapping that came with the dataset was changed:\n",
    "# padding is represented with the number 0\n",
    "# start_token is 60\n",
    "# end_token is 61\n",
    "with open (os.path.join(PATH_KAGGLE_DS, \"character_to_prediction_index.json\"), \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "    \n",
    "char_to_num = {c:char_to_num[c]+1 for c in char_to_num}\n",
    "\n",
    "# Add pad_token, start pointer and end pointer to the dict\n",
    "pad_token = 'P'\n",
    "pad_token_idx = 0\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "\n",
    "start_token = '<'\n",
    "start_token_idx = 60\n",
    "char_to_num[start_token] = start_token_idx\n",
    "\n",
    "end_token = '>'\n",
    "end_token_idx = 61\n",
    "char_to_num[end_token] = end_token_idx\n",
    "\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1365b9-9f43-40aa-b4cf-83950d2f01dc",
   "metadata": {},
   "source": [
    "## Preprocess phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4177f0c1-bbe5-4fe7-a460-2404b6b8deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(char_to_num.keys()),\n",
    "        values=list(char_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"tf_char_to_num\"\n",
    ")\n",
    "\n",
    "# Function to decode the characters and pad the phrases\n",
    "MAX_PHRASE_LEN = 31 + 2 # The start and end token take space as well\n",
    "def preprocess_phrase(phrase):\n",
    "    phrase = start_token + phrase + end_token\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "    \n",
    "    max_len_plus = MAX_PHRASE_LEN + 1\n",
    "    amount_to_pad = max_len_plus - tf.shape(phrase)[0]\n",
    "    \n",
    "    if amount_to_pad > 0:\n",
    "        phrase = tf.pad(phrase, paddings=[[0, amount_to_pad]], mode = 'CONSTANT', constant_values = pad_token_idx)\n",
    "    else:\n",
    "        phrase = phrase[:max_len_plus]\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41469e7-8498-4afb-baa7-12fa3973a0df",
   "metadata": {},
   "source": [
    "Notice that landmarks don't need to be preprocessed, the saved model should contain it's own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbedc7dc-6e5f-405a-8b5c-d54abcc78e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(landmark, phrase):\n",
    "    phrase = preprocess_phrase(phrase)\n",
    "    return (landmark, phrase[:-1]), phrase[1:] # Shifted phrase for encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6fe826-2a06-4f37-9af9-0708da71e49a",
   "metadata": {},
   "source": [
    "## Create TFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a07a5676-02d9-4664-b887-4b7486c066e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tfrecords, batch_size=1, repeat=False, shuffle=False, drop_remainder=False, cache=False):\n",
    "    ds = tf.data.TFRecordDataset(tf_records)\n",
    "    ds = ds.map(decode_fn, tf.data.AUTOTUNE)\n",
    "    # Note: preprocessing can happen before and after the batching (if you can preprocess the whole batch at once to save computation time)\n",
    "    ds = ds.map(preprocess, tf.data.AUTOTUNE)\n",
    "    \n",
    "    if repeat: \n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_deterministic = (False)\n",
    "        ds = ds.with_options(options)\n",
    "\n",
    "    if batch_size >= 1:\n",
    "        # There's also a padded_batch version of this function\n",
    "        ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "        \n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # If the system doesn't have enough RAM caching might slow down the process\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    \n",
    "    return ds\n",
    "\n",
    "test_ds = get_dataset(tf_records, batch_size=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bd0914d-0fc4-4444-9b7a-0663df18d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "\n",
      "Saved shapes:\n",
      "lm_shape: 159\n",
      "phrase_shape: 1\n",
      "----------------------------------------\n",
      "\n",
      "Encoder input - first in batch (Landmarks:)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 178, 159)\n",
      "tf.Tensor(\n",
      "[[ 0.12627919  0.22162548  0.3116899  ... -2.7103877  -2.565694\n",
      "  -2.1914308 ]\n",
      " [        nan         nan         nan ... -2.791268   -2.6042914\n",
      "  -2.3839378 ]\n",
      " [        nan         nan         nan ... -2.7606401  -2.5794535\n",
      "  -2.3470478 ]\n",
      " ...\n",
      " [        nan         nan         nan ... -2.8067975  -2.6485422\n",
      "  -1.3479928 ]\n",
      " [ 0.13762568  0.20138527  0.25962168 ... -2.979717   -2.6853716\n",
      "  -1.9384248 ]\n",
      " [ 0.10258354  0.1709002   0.24561381 ... -3.1564484  -2.9503818\n",
      "  -2.201033  ]], shape=(178, 159), dtype=float32)\n",
      "----------------------------------------\n",
      "\n",
      "Decoder input (Context):\n",
      "(1, 33)\n",
      "tf.Tensor(\n",
      "[60 41 46 52 37 50 37 51 52 41 46 39  1 47 34 51 37 50 54 33 52 41 47 46\n",
      "  1 55 33 51  1 45 33 36 37], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n",
      "\n",
      "Model target output (Phrase):\n",
      "(1, 33)\n",
      "tf.Tensor(\n",
      "[41 46 52 37 50 37 51 52 41 46 39  1 47 34 51 37 50 54 33 52 41 47 46  1\n",
      " 55 33 51  1 45 33 36 37 61], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lm_shape = None\n",
    "phrase_shape = None\n",
    "\n",
    "# Create an iterator for the train and valid datasets\n",
    "test_iterator = iter(test_ds)\n",
    "\n",
    "# Print data points from the training dataset\n",
    "print(\"Training Data:\\n\")\n",
    "(landmarks, context), phrase = next(test_iterator)\n",
    "\n",
    "# Save shapes\n",
    "lm_shape = landmarks.shape[2]\n",
    "phrase_shape = phrase.shape[0]\n",
    "print(\"Saved shapes:\")\n",
    "print(f\"lm_shape: {lm_shape}\")\n",
    "print(f\"phrase_shape: {phrase_shape}\")\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Encoder input - first in batch (Landmarks:)\")\n",
    "print(type(landmarks))\n",
    "print(landmarks.shape)\n",
    "print(landmarks[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Decoder input (Context):\")\n",
    "print(context.shape)\n",
    "print(context[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Model target output (Phrase):\")\n",
    "print(phrase.shape)\n",
    "print(phrase[0])\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e35d-d23c-4d31-bf5a-77aa94e48802",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1862c9ed-944a-48ce-b925-846a109d46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(\"GRU_local_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14b7caa1-ed72-4dab-94fb-ac47e636ecce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': <tf.Tensor: shape=(), dtype=string, numpy=b'h'>,\n",
       " 'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.55950594>}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(np.zeros((FRAME_LEN, len(FEATURE_COLUMNS))), \"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12ee5add-bdc8-4b26-99b9-520440089aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(159,), dtype=string, numpy=\n",
       "array([b'x_right_hand_0', b'x_right_hand_1', b'x_right_hand_2',\n",
       "       b'x_right_hand_3', b'x_right_hand_4', b'x_right_hand_5',\n",
       "       b'x_right_hand_6', b'x_right_hand_7', b'x_right_hand_8',\n",
       "       b'x_right_hand_9', b'x_right_hand_10', b'x_right_hand_11',\n",
       "       b'x_right_hand_12', b'x_right_hand_13', b'x_right_hand_14',\n",
       "       b'x_right_hand_15', b'x_right_hand_16', b'x_right_hand_17',\n",
       "       b'x_right_hand_18', b'x_right_hand_19', b'x_right_hand_20',\n",
       "       b'x_left_hand_0', b'x_left_hand_1', b'x_left_hand_2',\n",
       "       b'x_left_hand_3', b'x_left_hand_4', b'x_left_hand_5',\n",
       "       b'x_left_hand_6', b'x_left_hand_7', b'x_left_hand_8',\n",
       "       b'x_left_hand_9', b'x_left_hand_10', b'x_left_hand_11',\n",
       "       b'x_left_hand_12', b'x_left_hand_13', b'x_left_hand_14',\n",
       "       b'x_left_hand_15', b'x_left_hand_16', b'x_left_hand_17',\n",
       "       b'x_left_hand_18', b'x_left_hand_19', b'x_left_hand_20',\n",
       "       b'x_pose_13', b'x_pose_15', b'x_pose_17', b'x_pose_19',\n",
       "       b'x_pose_21', b'x_pose_14', b'x_pose_16', b'x_pose_18',\n",
       "       b'x_pose_20', b'x_pose_22', b'x_pose_0', b'y_right_hand_0',\n",
       "       b'y_right_hand_1', b'y_right_hand_2', b'y_right_hand_3',\n",
       "       b'y_right_hand_4', b'y_right_hand_5', b'y_right_hand_6',\n",
       "       b'y_right_hand_7', b'y_right_hand_8', b'y_right_hand_9',\n",
       "       b'y_right_hand_10', b'y_right_hand_11', b'y_right_hand_12',\n",
       "       b'y_right_hand_13', b'y_right_hand_14', b'y_right_hand_15',\n",
       "       b'y_right_hand_16', b'y_right_hand_17', b'y_right_hand_18',\n",
       "       b'y_right_hand_19', b'y_right_hand_20', b'y_left_hand_0',\n",
       "       b'y_left_hand_1', b'y_left_hand_2', b'y_left_hand_3',\n",
       "       b'y_left_hand_4', b'y_left_hand_5', b'y_left_hand_6',\n",
       "       b'y_left_hand_7', b'y_left_hand_8', b'y_left_hand_9',\n",
       "       b'y_left_hand_10', b'y_left_hand_11', b'y_left_hand_12',\n",
       "       b'y_left_hand_13', b'y_left_hand_14', b'y_left_hand_15',\n",
       "       b'y_left_hand_16', b'y_left_hand_17', b'y_left_hand_18',\n",
       "       b'y_left_hand_19', b'y_left_hand_20', b'y_pose_13', b'y_pose_15',\n",
       "       b'y_pose_17', b'y_pose_19', b'y_pose_21', b'y_pose_14',\n",
       "       b'y_pose_16', b'y_pose_18', b'y_pose_20', b'y_pose_22',\n",
       "       b'y_pose_0', b'z_right_hand_0', b'z_right_hand_1',\n",
       "       b'z_right_hand_2', b'z_right_hand_3', b'z_right_hand_4',\n",
       "       b'z_right_hand_5', b'z_right_hand_6', b'z_right_hand_7',\n",
       "       b'z_right_hand_8', b'z_right_hand_9', b'z_right_hand_10',\n",
       "       b'z_right_hand_11', b'z_right_hand_12', b'z_right_hand_13',\n",
       "       b'z_right_hand_14', b'z_right_hand_15', b'z_right_hand_16',\n",
       "       b'z_right_hand_17', b'z_right_hand_18', b'z_right_hand_19',\n",
       "       b'z_right_hand_20', b'z_left_hand_0', b'z_left_hand_1',\n",
       "       b'z_left_hand_2', b'z_left_hand_3', b'z_left_hand_4',\n",
       "       b'z_left_hand_5', b'z_left_hand_6', b'z_left_hand_7',\n",
       "       b'z_left_hand_8', b'z_left_hand_9', b'z_left_hand_10',\n",
       "       b'z_left_hand_11', b'z_left_hand_12', b'z_left_hand_13',\n",
       "       b'z_left_hand_14', b'z_left_hand_15', b'z_left_hand_16',\n",
       "       b'z_left_hand_17', b'z_left_hand_18', b'z_left_hand_19',\n",
       "       b'z_left_hand_20', b'z_pose_13', b'z_pose_15', b'z_pose_17',\n",
       "       b'z_pose_19', b'z_pose_21', b'z_pose_14', b'z_pose_16',\n",
       "       b'z_pose_18', b'z_pose_20', b'z_pose_22', b'z_pose_0'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ba291-79af-4d03-9dac-b4a199a586dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
