{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466495a9-b430-455b-b3f1-a4a3de52d80d",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "This notebook is meant for testing the trained encoder-decoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e1bf9-dabd-49ed-92f9-aa19a4a4fe2f",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8656ecc3-9850-40fe-9602-8ce195c6a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install tensorflow\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install mediapipe==0.9.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bab87b-6c62-42e8-9737-2151e3016478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import difflib\n",
    "import matplotlib\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pyarrow.parquet as pq\n",
    "from tensorflow.keras import layers\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a535ef19-6946-46c1-81c3-2dd5d347b070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n",
      "TensorFlow v2.14.0\n",
      "Mediapipe v0.9.0.1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4763f5-6df6-4065-acab-dc42d30c95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "cv2.setRNGSeed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3572f83-e4fc-4efc-a9e4-e1b70a15de8d",
   "metadata": {},
   "source": [
    "# Fetch from TfRecords\n",
    "\n",
    "To acquire the tfrecords you might want to run the data_handling notebook first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b267c6-41b6-453e-ba6c-d6be27179620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 68 TFRecord files.\n"
     ]
    }
   ],
   "source": [
    "PATH_KAGGLE_DS = \"kaggle_dataset\"\n",
    "dataset_df = pd.read_csv(os.path.join(PATH_KAGGLE_DS, \"train.csv\"))\n",
    "PATH_TFRECORD_DS = os.path.join(PATH_KAGGLE_DS, \"train_tfrecords\")\n",
    "tf_records = dataset_df.file_id.map(lambda x: os.path.join(PATH_TFRECORD_DS, f\"{x}.tfrecord\")).unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efae36c2-583a-43e1-a3fc-7fe981f00709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_right_hand_0',\n",
       " 'x_right_hand_1',\n",
       " 'x_right_hand_2',\n",
       " 'x_right_hand_3',\n",
       " 'x_right_hand_4',\n",
       " 'x_right_hand_5',\n",
       " 'x_right_hand_6',\n",
       " 'x_right_hand_7',\n",
       " 'x_right_hand_8',\n",
       " 'x_right_hand_9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(PATH_TFRECORD_DS, \"feature_columns.json\"), 'r') as f:\n",
    "    json_str = f.read()\n",
    "FEATURE_COLUMNS = json.loads(json_str)\n",
    "FEATURE_COLUMNS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4c03c8-6571-45a2-ae76-53f0ada783b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COLUMNS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n",
    "    # Transpose to maintain the original shape of landmarks data.\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "    \n",
    "    return landmarks, phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "064d74e5-a641-4f6f-b7ce-44727079f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default mapping that came with the dataset was changed:\n",
    "# padding is represented with the number 0\n",
    "# start_token is 60\n",
    "# end_token is 61\n",
    "with open (os.path.join(PATH_KAGGLE_DS, \"character_to_prediction_index.json\"), \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "    \n",
    "char_to_num = {c:char_to_num[c]+1 for c in char_to_num}\n",
    "\n",
    "# Add pad_token, start pointer and end pointer to the dict\n",
    "pad_token = 'P'\n",
    "pad_token_idx = 0\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "\n",
    "start_token = '<'\n",
    "start_token_idx = 60\n",
    "char_to_num[start_token] = start_token_idx\n",
    "\n",
    "end_token = '>'\n",
    "end_token_idx = 61\n",
    "char_to_num[end_token] = end_token_idx\n",
    "\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1365b9-9f43-40aa-b4cf-83950d2f01dc",
   "metadata": {},
   "source": [
    "## Preprocess phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4177f0c1-bbe5-4fe7-a460-2404b6b8deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(char_to_num.keys()),\n",
    "        values=list(char_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"tf_char_to_num\"\n",
    ")\n",
    "\n",
    "# Function to decode the characters and pad the phrases\n",
    "MAX_PHRASE_LEN = 31 + 2 # The start and end token take space as well\n",
    "def preprocess_phrase(phrase):\n",
    "    phrase = start_token + phrase + end_token\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "    \n",
    "    max_len_plus = MAX_PHRASE_LEN + 1\n",
    "    amount_to_pad = max_len_plus - tf.shape(phrase)[0]\n",
    "    \n",
    "    if amount_to_pad > 0:\n",
    "        phrase = tf.pad(phrase, paddings=[[0, amount_to_pad]], mode = 'CONSTANT', constant_values = pad_token_idx)\n",
    "    else:\n",
    "        phrase = phrase[:max_len_plus]\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41469e7-8498-4afb-baa7-12fa3973a0df",
   "metadata": {},
   "source": [
    "Notice that landmarks don't need to be preprocessed, the saved model should contain it's own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbedc7dc-6e5f-405a-8b5c-d54abcc78e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(landmark, phrase):\n",
    "    phrase = preprocess_phrase(phrase)\n",
    "    return (landmark, phrase[:-1]), phrase[1:] # Shifted phrase for encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6fe826-2a06-4f37-9af9-0708da71e49a",
   "metadata": {},
   "source": [
    "## Create TFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07a5676-02d9-4664-b887-4b7486c066e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tfrecords, batch_size=1, repeat=False, shuffle=False, drop_remainder=False, cache=False):\n",
    "    ds = tf.data.TFRecordDataset(tf_records)\n",
    "    ds = ds.map(decode_fn, tf.data.AUTOTUNE)\n",
    "    # Note: preprocessing can happen before and after the batching (if you can preprocess the whole batch at once to save computation time)\n",
    "    ds = ds.map(preprocess, tf.data.AUTOTUNE)\n",
    "    \n",
    "    if repeat: \n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_deterministic = (False)\n",
    "        ds = ds.with_options(options)\n",
    "\n",
    "    if batch_size >= 1:\n",
    "        # There's also a padded_batch version of this function\n",
    "        ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "        \n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # If the system doesn't have enough RAM caching might slow down the process\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    \n",
    "    return ds\n",
    "\n",
    "test_ds = get_dataset(tf_records[-1000:], batch_size=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bd0914d-0fc4-4444-9b7a-0663df18d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "\n",
      "Saved shapes:\n",
      "lm_shape: 159\n",
      "phrase_shape: 1\n",
      "----------------------------------------\n",
      "\n",
      "Encoder input - first in batch (Landmarks:)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(1, 123, 159)\n",
      "tf.Tensor(\n",
      "[[ 0.40883234  0.5199118   0.61215943 ... -4.017103   -3.7243094\n",
      "  -2.2438369 ]\n",
      " [        nan         nan         nan ... -3.4922464  -3.1558092\n",
      "  -2.0979135 ]\n",
      " [        nan         nan         nan ... -3.8470871  -3.5316234\n",
      "  -2.2468953 ]\n",
      " ...\n",
      " [        nan         nan         nan ... -2.8840117  -2.5492065\n",
      "  -1.7740132 ]\n",
      " [        nan         nan         nan ... -3.840464   -3.5428739\n",
      "  -1.8344551 ]\n",
      " [ 0.32008234  0.4331264   0.49733612 ... -3.629844   -3.3544126\n",
      "  -1.720821  ]], shape=(123, 159), dtype=float32)\n",
      "----------------------------------------\n",
      "\n",
      "Decoder input (Context):\n",
      "(1, 33)\n",
      "tf.Tensor(\n",
      "[60 19  1 35 50 37 37 43 40 47 53 51 37 61  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n",
      "\n",
      "Model target output (Phrase):\n",
      "(1, 33)\n",
      "tf.Tensor(\n",
      "[19  1 35 50 37 37 43 40 47 53 51 37 61  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lm_shape = None\n",
    "phrase_shape = None\n",
    "\n",
    "# Create an iterator for the train and valid datasets\n",
    "test_iterator = iter(test_ds)\n",
    "\n",
    "# Print data points from the training dataset\n",
    "print(\"Training Data:\\n\")\n",
    "(landmarks, context), phrase = next(test_iterator)\n",
    "\n",
    "# Save shapes\n",
    "lm_shape = landmarks.shape[2]\n",
    "phrase_shape = phrase.shape[0]\n",
    "print(\"Saved shapes:\")\n",
    "print(f\"lm_shape: {lm_shape}\")\n",
    "print(f\"phrase_shape: {phrase_shape}\")\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Encoder input - first in batch (Landmarks:)\")\n",
    "print(type(landmarks))\n",
    "print(landmarks.shape)\n",
    "print(landmarks[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Decoder input (Context):\")\n",
    "print(context.shape)\n",
    "print(context[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Model target output (Phrase):\")\n",
    "print(phrase.shape)\n",
    "print(phrase[0])\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e35d-d23c-4d31-bf5a-77aa94e48802",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9e8b5-0a63-4dd2-9ca2-803ffea6fba4",
   "metadata": {},
   "source": [
    "Load the model from saved model format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1862c9ed-944a-48ce-b925-846a109d46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(\"transformer_seq2seq_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b335e-e782-4451-bbf3-aaf71ddc9ab3",
   "metadata": {},
   "source": [
    "- The models preprocess the input inside with the method that was used during training.\n",
    "- The return types are tensorflow specific, so one needs to call .numpy() on them to get the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14b7caa1-ed72-4dab-94fb-ac47e636ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 781 ms\n",
      "Wall time: 663 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.54006404>,\n",
       " 'result': <tf.Tensor: shape=(), dtype=string, numpy=b'-'>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "loaded_model.predict(np.zeros((10, len(FEATURE_COLUMNS))), \"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0476617-b356-49e8-97cb-7764bb632017",
   "metadata": {},
   "source": [
    "Each model stores the landmarks that are needed for it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ee5add-bdc8-4b26-99b9-520440089aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(159,), dtype=string, numpy=\n",
       "array([b'x_right_hand_0', b'x_right_hand_1', b'x_right_hand_2',\n",
       "       b'x_right_hand_3', b'x_right_hand_4', b'x_right_hand_5',\n",
       "       b'x_right_hand_6', b'x_right_hand_7', b'x_right_hand_8',\n",
       "       b'x_right_hand_9', b'x_right_hand_10', b'x_right_hand_11',\n",
       "       b'x_right_hand_12', b'x_right_hand_13', b'x_right_hand_14',\n",
       "       b'x_right_hand_15', b'x_right_hand_16', b'x_right_hand_17',\n",
       "       b'x_right_hand_18', b'x_right_hand_19', b'x_right_hand_20',\n",
       "       b'x_left_hand_0', b'x_left_hand_1', b'x_left_hand_2',\n",
       "       b'x_left_hand_3', b'x_left_hand_4', b'x_left_hand_5',\n",
       "       b'x_left_hand_6', b'x_left_hand_7', b'x_left_hand_8',\n",
       "       b'x_left_hand_9', b'x_left_hand_10', b'x_left_hand_11',\n",
       "       b'x_left_hand_12', b'x_left_hand_13', b'x_left_hand_14',\n",
       "       b'x_left_hand_15', b'x_left_hand_16', b'x_left_hand_17',\n",
       "       b'x_left_hand_18', b'x_left_hand_19', b'x_left_hand_20',\n",
       "       b'x_pose_13', b'x_pose_15', b'x_pose_17', b'x_pose_19',\n",
       "       b'x_pose_21', b'x_pose_14', b'x_pose_16', b'x_pose_18',\n",
       "       b'x_pose_20', b'x_pose_22', b'x_pose_0', b'y_right_hand_0',\n",
       "       b'y_right_hand_1', b'y_right_hand_2', b'y_right_hand_3',\n",
       "       b'y_right_hand_4', b'y_right_hand_5', b'y_right_hand_6',\n",
       "       b'y_right_hand_7', b'y_right_hand_8', b'y_right_hand_9',\n",
       "       b'y_right_hand_10', b'y_right_hand_11', b'y_right_hand_12',\n",
       "       b'y_right_hand_13', b'y_right_hand_14', b'y_right_hand_15',\n",
       "       b'y_right_hand_16', b'y_right_hand_17', b'y_right_hand_18',\n",
       "       b'y_right_hand_19', b'y_right_hand_20', b'y_left_hand_0',\n",
       "       b'y_left_hand_1', b'y_left_hand_2', b'y_left_hand_3',\n",
       "       b'y_left_hand_4', b'y_left_hand_5', b'y_left_hand_6',\n",
       "       b'y_left_hand_7', b'y_left_hand_8', b'y_left_hand_9',\n",
       "       b'y_left_hand_10', b'y_left_hand_11', b'y_left_hand_12',\n",
       "       b'y_left_hand_13', b'y_left_hand_14', b'y_left_hand_15',\n",
       "       b'y_left_hand_16', b'y_left_hand_17', b'y_left_hand_18',\n",
       "       b'y_left_hand_19', b'y_left_hand_20', b'y_pose_13', b'y_pose_15',\n",
       "       b'y_pose_17', b'y_pose_19', b'y_pose_21', b'y_pose_14',\n",
       "       b'y_pose_16', b'y_pose_18', b'y_pose_20', b'y_pose_22',\n",
       "       b'y_pose_0', b'z_right_hand_0', b'z_right_hand_1',\n",
       "       b'z_right_hand_2', b'z_right_hand_3', b'z_right_hand_4',\n",
       "       b'z_right_hand_5', b'z_right_hand_6', b'z_right_hand_7',\n",
       "       b'z_right_hand_8', b'z_right_hand_9', b'z_right_hand_10',\n",
       "       b'z_right_hand_11', b'z_right_hand_12', b'z_right_hand_13',\n",
       "       b'z_right_hand_14', b'z_right_hand_15', b'z_right_hand_16',\n",
       "       b'z_right_hand_17', b'z_right_hand_18', b'z_right_hand_19',\n",
       "       b'z_right_hand_20', b'z_left_hand_0', b'z_left_hand_1',\n",
       "       b'z_left_hand_2', b'z_left_hand_3', b'z_left_hand_4',\n",
       "       b'z_left_hand_5', b'z_left_hand_6', b'z_left_hand_7',\n",
       "       b'z_left_hand_8', b'z_left_hand_9', b'z_left_hand_10',\n",
       "       b'z_left_hand_11', b'z_left_hand_12', b'z_left_hand_13',\n",
       "       b'z_left_hand_14', b'z_left_hand_15', b'z_left_hand_16',\n",
       "       b'z_left_hand_17', b'z_left_hand_18', b'z_left_hand_19',\n",
       "       b'z_left_hand_20', b'z_pose_13', b'z_pose_15', b'z_pose_17',\n",
       "       b'z_pose_19', b'z_pose_21', b'z_pose_14', b'z_pose_16',\n",
       "       b'z_pose_18', b'z_pose_20', b'z_pose_22', b'z_pose_0'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01925eb1-88bb-4114-845a-c45568568d1d",
   "metadata": {},
   "source": [
    "Load multiple models for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcd5b73-7e0f-4de8-a414-faeb3d0a1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "def load_model(saved_model):\n",
    "    models[saved_model] = tf.saved_model.load(saved_model)\n",
    "\n",
    "load_model(\"transformer_seq2seq_model\")\n",
    "load_model(\"transformer_wposembedding_seq2seq\")\n",
    "load_model(\"simpleRNN_seq2seq\")\n",
    "load_model(\"GRU_seq2seq\")\n",
    "load_model(\"LSTM_seq2seq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953171f-ca12-4f7e-8406-02b512a6e4d3",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81060d38-2986-4f3a-a599-65128c526d36",
   "metadata": {},
   "source": [
    "## Metrics on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ced00ed1-7cd5-48e0-83e8-e8aaadaf528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(fs_model, inp, max_len):\n",
    "    ctx = str(num_to_char[start_token_idx])\n",
    "    for i in range(max_len):\n",
    "        res = fs_model.predict(inp, ctx)\n",
    "        res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        ctx += res_char\n",
    "\n",
    "        if res_char == num_to_char[end_token_idx]:\n",
    "            break\n",
    "    return ctx\n",
    "\n",
    "def generate_teacher_forcing(fs_model, inp, expected):\n",
    "    pred = str(num_to_char[start_token_idx])\n",
    "    ctx = str(num_to_char[start_token_idx])\n",
    "    for e in expected:\n",
    "        if e == 'P':\n",
    "            break\n",
    "        res = fs_model.predict(inp, ctx)\n",
    "        res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        pred += res_char\n",
    "        ctx += e\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "739e4068-2c7c-4a80-a798-acec446782ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Obtaining dependency information for python-Levenshtein from https://files.pythonhosted.org/packages/27/89/c45dbdbd479453cfc8c4c1271c9f67fd607deaf84880e31c67b682980456/python_Levenshtein-0.23.0-py3-none-any.whl.metadata\n",
      "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting Levenshtein==0.23.0 (from python-Levenshtein)\n",
      "  Obtaining dependency information for Levenshtein==0.23.0 from https://files.pythonhosted.org/packages/7f/5c/50e7a3fb298301db6335bc4d08508c9278cc387eed6048cb750bb32411c7/Levenshtein-0.23.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading Levenshtein-0.23.0-cp39-cp39-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein)\n",
      "  Obtaining dependency information for rapidfuzz<4.0.0,>=3.1.0 from https://files.pythonhosted.org/packages/2a/ef/dbc5a182e2259134d84e61ef31dd3f4e7e11686eaabfa9c9e5c5ae1675d2/rapidfuzz-3.4.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading rapidfuzz-3.4.0-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading Levenshtein-0.23.0-cp39-cp39-win_amd64.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 101.1/101.1 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.4.0-cp39-cp39-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.4/1.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.7/1.8 MB 7.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.8 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 6.9 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.23.0 python-Levenshtein-0.23.0 rapidfuzz-3.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "424d4364-b331-4816-a717-7d475505a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as lev\n",
    "\n",
    "def levenshtein_distance(str1, str2):\n",
    "    return lev.distance(str1, str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0da00211-3634-4ca5-ad2c-37c90ebbecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_error_rate(ref, hyp):\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "    distance = levenshtein_distance(ref_words, hyp_words)\n",
    "    return distance / max(len(ref_words), 1)\n",
    "\n",
    "def character_error_rate(ref, hyp):\n",
    "    distance = levenshtein_distance(ref, hyp)\n",
    "    return distance / max(len(ref), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "362bf853-eaf4-45c0-af7b-7a5f6d192f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b648e2ade0f413691f0a64aa6ebb7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e618320042c4e359bf17be4547f87a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174ab2641ee443a2bb977dc9993ff220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409086921bcd4bc19c131de56ff4f610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6d85b231614bcc9ed0af1ad00b881f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ce469f045148dfbc2d2f3cf945fb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>edit_dist_gen_on_own</th>\n",
       "      <th>edit_dist_tf</th>\n",
       "      <th>wer_gen_on_own</th>\n",
       "      <th>wer_tf</th>\n",
       "      <th>cer_gen_on_own</th>\n",
       "      <th>cer_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_name  edit_dist_gen_on_own  edit_dist_tf  \\\n",
       "0    transformer_seq2seq_model                    15             8   \n",
       "1    transformer_seq2seq_model                    17            13   \n",
       "2    transformer_seq2seq_model                    22            13   \n",
       "3    transformer_seq2seq_model                     3             2   \n",
       "4    transformer_seq2seq_model                     5             6   \n",
       "..                         ...                   ...           ...   \n",
       "995               LSTM_seq2seq                    24            23   \n",
       "996               LSTM_seq2seq                    16            18   \n",
       "997               LSTM_seq2seq                    24            26   \n",
       "998               LSTM_seq2seq                    19            17   \n",
       "999               LSTM_seq2seq                    25            26   \n",
       "\n",
       "     wer_gen_on_own  wer_tf  cer_gen_on_own    cer_tf  \n",
       "0               1.0    1.00        0.468750  0.250000  \n",
       "1               1.5    1.00        0.607143  0.464286  \n",
       "2               1.0    1.00        0.666667  0.393939  \n",
       "3               1.0    0.75        0.130435  0.086957  \n",
       "4               1.0    1.00        0.250000  0.300000  \n",
       "..              ...     ...             ...       ...  \n",
       "995             1.0    1.00        0.774194  0.741935  \n",
       "996             1.0    1.00        0.695652  0.782609  \n",
       "997             1.0    1.00        0.727273  0.787879  \n",
       "998             1.0    1.00        0.760000  0.680000  \n",
       "999             1.0    1.00        0.833333  0.866667  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "def remove_chars(s, chars_to_remove=[num_to_char[start_token_idx], num_to_char[end_token_idx], num_to_char[pad_token_idx]]):\n",
    "    for char in chars_to_remove:\n",
    "        s = s.replace(char, \"\")\n",
    "    return s\n",
    "for model_name in tqdm(models.keys()):\n",
    "    m = models[model_name]\n",
    "    for (inp_batch, _ctx), expected_batch in tqdm(test_ds.take(200)):\n",
    "        for seq, expected in zip(inp_batch, expected_batch):\n",
    "            expected = \"\".join([num_to_char[num.numpy()] for num in expected])\n",
    "            \n",
    "            gen_on_own = generate(m, seq, MAX_PHRASE_LEN)\n",
    "            gen_teacher_forcing = generate_teacher_forcing(m, seq, expected)\n",
    "    \n",
    "            # Clean up padding and excess tokens\n",
    "            expected = remove_chars(expected)\n",
    "            gen_on_own = remove_chars(gen_on_own)\n",
    "            gen_teacher_forcing = remove_chars(gen_teacher_forcing)\n",
    "    \n",
    "            data.append([model_name,\n",
    "                         levenshtein_distance(expected, gen_on_own),\n",
    "                         levenshtein_distance(expected, gen_teacher_forcing),\n",
    "                         word_error_rate(expected, gen_on_own),\n",
    "                         word_error_rate(expected, gen_teacher_forcing),\n",
    "                         character_error_rate(expected, gen_on_own),\n",
    "                         character_error_rate(expected, gen_teacher_forcing)])\n",
    "        \n",
    "            # print(\"Expected: \" + expected)\n",
    "            # print(\"Gen on own: \" + gen_on_own)\n",
    "            # print(\"Gen teacher forcing: \" + gen_teacher_forcing)\n",
    "            # print('\\n~~~\\n')\n",
    "        \n",
    "columns = [\"model_name\",\n",
    "           \"edit_dist_gen_on_own\",\n",
    "           \"edit_dist_tf\",\n",
    "           \"wer_gen_on_own\",\n",
    "           \"wer_tf\",\n",
    "           \"cer_gen_on_own\",\n",
    "           \"cer_tf\"]\n",
    "stat_df = pd.DataFrame(data, columns=columns)\n",
    "stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0ef3d922-bbcc-4763-aab9-34e99e407d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edit_dist_gen_on_own</th>\n",
       "      <th>edit_dist_tf</th>\n",
       "      <th>wer_gen_on_own</th>\n",
       "      <th>wer_tf</th>\n",
       "      <th>cer_gen_on_own</th>\n",
       "      <th>cer_tf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRU_seq2seq</th>\n",
       "      <td>19.665</td>\n",
       "      <td>18.570</td>\n",
       "      <td>1.000536</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704354</td>\n",
       "      <td>0.663711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM_seq2seq</th>\n",
       "      <td>20.500</td>\n",
       "      <td>19.430</td>\n",
       "      <td>0.999167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733378</td>\n",
       "      <td>0.692710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simpleRNN_seq2seq</th>\n",
       "      <td>27.900</td>\n",
       "      <td>22.495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformer_seq2seq_model</th>\n",
       "      <td>13.245</td>\n",
       "      <td>10.075</td>\n",
       "      <td>0.993702</td>\n",
       "      <td>0.974274</td>\n",
       "      <td>0.471627</td>\n",
       "      <td>0.358716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformer_wposembedding_seq2seq</th>\n",
       "      <td>11.780</td>\n",
       "      <td>9.725</td>\n",
       "      <td>0.989905</td>\n",
       "      <td>0.967143</td>\n",
       "      <td>0.417860</td>\n",
       "      <td>0.345485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   edit_dist_gen_on_own  edit_dist_tf  \\\n",
       "model_name                                                              \n",
       "GRU_seq2seq                                      19.665        18.570   \n",
       "LSTM_seq2seq                                     20.500        19.430   \n",
       "simpleRNN_seq2seq                                27.900        22.495   \n",
       "transformer_seq2seq_model                        13.245        10.075   \n",
       "transformer_wposembedding_seq2seq                11.780         9.725   \n",
       "\n",
       "                                   wer_gen_on_own    wer_tf  cer_gen_on_own  \\\n",
       "model_name                                                                    \n",
       "GRU_seq2seq                              1.000536  1.000000        0.704354   \n",
       "LSTM_seq2seq                             0.999167  1.000000        0.733378   \n",
       "simpleRNN_seq2seq                        1.000000  1.000000        1.000000   \n",
       "transformer_seq2seq_model                0.993702  0.974274        0.471627   \n",
       "transformer_wposembedding_seq2seq        0.989905  0.967143        0.417860   \n",
       "\n",
       "                                     cer_tf  \n",
       "model_name                                   \n",
       "GRU_seq2seq                        0.663711  \n",
       "LSTM_seq2seq                       0.692710  \n",
       "simpleRNN_seq2seq                  0.807478  \n",
       "transformer_seq2seq_model          0.358716  \n",
       "transformer_wposembedding_seq2seq  0.345485  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_df.groupby(\"model_name\").mean()\n",
    "\tedit_dist_gen_on_own\tedit_dist_tf\twer_gen_on_own\twer_tf\tcer_gen_on_own\tcer_tf\n",
    "model_name\t\t\t\t\t\t\n",
    "GRU_seq2seq\t19.665\t18.570\t1.000536\t1.000000\t0.704354\t0.663711\n",
    "LSTM_seq2seq\t20.500\t19.430\t0.999167\t1.000000\t0.733378\t0.692710\n",
    "simpleRNN_seq2seq\t27.900\t22.495\t1.000000\t1.000000\t1.000000\t0.807478\n",
    "transformer_seq2seq_model\t13.245\t10.075\t0.993702\t0.974274\t0.471627\t0.358716\n",
    "transformer_wposembedding_seq2seq\t11.780\t9.725\t0.989905\t0.967143\t0.417860\t0.345485"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4672c4e9-240a-4ccc-bd01-8d8bdc123ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057b99ee89ef4425ace579802180b91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c58dfc68734c438b3e30654656593c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41de8f24b5c64ac8a19407b818db9ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cde29429be4cfa95e0b222b1b38df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c221a70590a4310af6596fb579455fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba5eecd044c481387859ef95bb432ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>edit_dist_gen_on_own</th>\n",
       "      <th>edit_dist_tf</th>\n",
       "      <th>wer_gen_on_own</th>\n",
       "      <th>wer_tf</th>\n",
       "      <th>cer_gen_on_own</th>\n",
       "      <th>cer_tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformer_seq2seq_model</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.241379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>LSTM_seq2seq</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_name  edit_dist_gen_on_own  edit_dist_tf  \\\n",
       "0    transformer_seq2seq_model                     0             0   \n",
       "1    transformer_seq2seq_model                     6             1   \n",
       "2    transformer_seq2seq_model                     0             0   \n",
       "3    transformer_seq2seq_model                     0             0   \n",
       "4    transformer_seq2seq_model                     0             0   \n",
       "..                         ...                   ...           ...   \n",
       "995               LSTM_seq2seq                    12             7   \n",
       "996               LSTM_seq2seq                     5             2   \n",
       "997               LSTM_seq2seq                    11             5   \n",
       "998               LSTM_seq2seq                    16            10   \n",
       "999               LSTM_seq2seq                    11             3   \n",
       "\n",
       "     wer_gen_on_own  wer_tf  cer_gen_on_own    cer_tf  \n",
       "0               0.0     0.0        0.000000  0.000000  \n",
       "1               2.0     2.0        0.400000  0.066667  \n",
       "2               0.0     0.0        0.000000  0.000000  \n",
       "3               0.0     0.0        0.000000  0.000000  \n",
       "4               0.0     0.0        0.000000  0.000000  \n",
       "..              ...     ...             ...       ...  \n",
       "995             1.0     1.0        0.413793  0.241379  \n",
       "996             1.0     1.0        0.384615  0.153846  \n",
       "997             1.0     1.0        0.647059  0.294118  \n",
       "998             1.0     1.0        0.761905  0.476190  \n",
       "999             1.0     1.0        0.500000  0.136364  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "def remove_chars(s, chars_to_remove=[num_to_char[start_token_idx], num_to_char[end_token_idx], num_to_char[pad_token_idx]]):\n",
    "    for char in chars_to_remove:\n",
    "        s = s.replace(char, \"\")\n",
    "    return s\n",
    "for model_name in tqdm(models.keys()):\n",
    "    m = models[model_name]\n",
    "    for (inp_batch, _ctx), expected_batch in tqdm(test_ds.take(200)):\n",
    "        for seq, expected in zip(inp_batch, expected_batch):\n",
    "            expected = \"\".join([num_to_char[num.numpy()] for num in expected])\n",
    "            \n",
    "            gen_on_own = generate(m, seq, MAX_PHRASE_LEN)\n",
    "            gen_teacher_forcing = generate_teacher_forcing(m, seq, expected)\n",
    "    \n",
    "            # Clean up padding and excess tokens\n",
    "            expected = remove_chars(expected)\n",
    "            gen_on_own = remove_chars(gen_on_own)\n",
    "            gen_teacher_forcing = remove_chars(gen_teacher_forcing)\n",
    "    \n",
    "            data.append([model_name,\n",
    "                         levenshtein_distance(expected, gen_on_own),\n",
    "                         levenshtein_distance(expected, gen_teacher_forcing),\n",
    "                         word_error_rate(expected, gen_on_own),\n",
    "                         word_error_rate(expected, gen_teacher_forcing),\n",
    "                         character_error_rate(expected, gen_on_own),\n",
    "                         character_error_rate(expected, gen_teacher_forcing)])\n",
    "        \n",
    "            # print(\"Expected: \" + expected)\n",
    "            # print(\"Gen on own: \" + gen_on_own)\n",
    "            # print(\"Gen teacher forcing: \" + gen_teacher_forcing)\n",
    "            # print('\\n~~~\\n')\n",
    "        \n",
    "columns = [\"model_name\",\n",
    "           \"edit_dist_gen_on_own\",\n",
    "           \"edit_dist_tf\",\n",
    "           \"wer_gen_on_own\",\n",
    "           \"wer_tf\",\n",
    "           \"cer_gen_on_own\",\n",
    "           \"cer_tf\"]\n",
    "stat_2_df = pd.DataFrame(data, columns=columns)\n",
    "stat_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c273b971-1f89-4be3-9e06-8904167d6eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edit_dist_gen_on_own</th>\n",
       "      <th>edit_dist_tf</th>\n",
       "      <th>wer_gen_on_own</th>\n",
       "      <th>wer_tf</th>\n",
       "      <th>cer_gen_on_own</th>\n",
       "      <th>cer_tf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRU_seq2seq</th>\n",
       "      <td>7.370</td>\n",
       "      <td>3.605</td>\n",
       "      <td>0.808417</td>\n",
       "      <td>0.737833</td>\n",
       "      <td>0.375828</td>\n",
       "      <td>0.184683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM_seq2seq</th>\n",
       "      <td>8.655</td>\n",
       "      <td>4.280</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.795667</td>\n",
       "      <td>0.456891</td>\n",
       "      <td>0.220285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simpleRNN_seq2seq</th>\n",
       "      <td>18.690</td>\n",
       "      <td>12.030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.022000</td>\n",
       "      <td>1.077332</td>\n",
       "      <td>0.681299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformer_seq2seq_model</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.244750</td>\n",
       "      <td>0.221417</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>0.024797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformer_wposembedding_seq2seq</th>\n",
       "      <td>3.150</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.663250</td>\n",
       "      <td>0.615333</td>\n",
       "      <td>0.174967</td>\n",
       "      <td>0.111829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   edit_dist_gen_on_own  edit_dist_tf  \\\n",
       "model_name                                                              \n",
       "GRU_seq2seq                                       7.370         3.605   \n",
       "LSTM_seq2seq                                      8.655         4.280   \n",
       "simpleRNN_seq2seq                                18.690        12.030   \n",
       "transformer_seq2seq_model                         0.995         0.445   \n",
       "transformer_wposembedding_seq2seq                 3.150         1.940   \n",
       "\n",
       "                                   wer_gen_on_own    wer_tf  cer_gen_on_own  \\\n",
       "model_name                                                                    \n",
       "GRU_seq2seq                              0.808417  0.737833        0.375828   \n",
       "LSTM_seq2seq                             0.871000  0.795667        0.456891   \n",
       "simpleRNN_seq2seq                        1.000000  1.022000        1.077332   \n",
       "transformer_seq2seq_model                0.244750  0.221417        0.054238   \n",
       "transformer_wposembedding_seq2seq        0.663250  0.615333        0.174967   \n",
       "\n",
       "                                     cer_tf  \n",
       "model_name                                   \n",
       "GRU_seq2seq                        0.184683  \n",
       "LSTM_seq2seq                       0.220285  \n",
       "simpleRNN_seq2seq                  0.681299  \n",
       "transformer_seq2seq_model          0.024797  \n",
       "transformer_wposembedding_seq2seq  0.111829  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_2_df.groupby(\"model_name\").mean()\n",
    "\tedit_dist_gen_on_own\tedit_dist_tf\twer_gen_on_own\twer_tf\tcer_gen_on_own\tcer_tf\n",
    "model_name\t\t\t\t\t\t\n",
    "GRU_seq2seq\t7.370\t3.605\t0.808417\t0.737833\t0.375828\t0.184683\n",
    "LSTM_seq2seq\t8.655\t4.280\t0.871000\t0.795667\t0.456891\t0.220285\n",
    "simpleRNN_seq2seq\t18.690\t12.030\t1.000000\t1.022000\t1.077332\t0.681299\n",
    "transformer_seq2seq_model\t0.995\t0.445\t0.244750\t0.221417\t0.054238\t0.024797\n",
    "transformer_wposembedding_seq2seq\t3.150\t1.940\t0.663250\t0.615333\t0.174967\t0.111829"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a70eb2-41c4-442c-86cc-8dcb4d950287",
   "metadata": {},
   "source": [
    "## Real world testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb05ebcf-01f8-4f3b-b809-60aeae424ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_landmarks_on_image(image, results):\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d1e5e58-c415-4b6c-9c17-4e2a0a38cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBCAM = 0\n",
    "\n",
    "def video_loop(source, process_result_func):\n",
    "    video = cv2.VideoCapture(source)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    try:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "            while True:\n",
    "                _, frame = video.read()\n",
    "    \n",
    "                if frame is None:\n",
    "                    break\n",
    "    \n",
    "                #image = cv2.resize(frame, (360, 240))\n",
    "                image=frame\n",
    "    \n",
    "                # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "                image.flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                pred = process_result_func(results)\n",
    "    \n",
    "                # Draw landmark annotation on the image.\n",
    "                image = draw_landmarks_on_image(image, results)\n",
    "    \n",
    "                image = cv2.flip(image, 1)\n",
    "                _, image = cv2.imencode('.jpeg', image)\n",
    "                display_handle.update(Image(data=image.tobytes()))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        video.release()\n",
    "        display_handle.update(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e7e8bab-c873-432a-a6ed-a661238f7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Assume draw_landmarks_on_image is defined elsewhere\n",
    "\n",
    "WEBCAM = 0\n",
    "\n",
    "def video_loop(source, process_result_func):\n",
    "    video = cv2.VideoCapture(source)\n",
    "    display_handle = display(None, display_id=True)\n",
    "    buffer = []  # Initialize the buffer\n",
    "\n",
    "    try:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "            while True:\n",
    "                _, frame = video.read()\n",
    "    \n",
    "                if frame is None:\n",
    "                    break\n",
    "    \n",
    "                image = frame\n",
    "                image.flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "\n",
    "                # Other image processing...\n",
    "                image = draw_landmarks_on_image(image, results)\n",
    "                image = cv2.flip(image, 1)\n",
    "\n",
    "                pred = process_result_func(results)\n",
    "\n",
    "                if pred is not None:\n",
    "                    buffer.append(pred)  # Update buffer\n",
    "\n",
    "                # Convert buffer to string and display on the image\n",
    "                buffer_text = ''.join(buffer)\n",
    "                image = put_text_on_image(image, buffer_text)\n",
    "                \n",
    "                _, image = cv2.imencode('.jpeg', image)\n",
    "                display_handle.update(Image(data=image.tobytes()))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        video.release()\n",
    "        display_handle.update(None)\n",
    "\n",
    "def put_text_on_image(image, label):\n",
    "   image = cv2.flip(image, 1)\n",
    "   return cv2.putText(image, label.upper(), (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# Add other function definitions and call video_loop as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb89a3d-5617-4473-8839-5f36ee989460",
   "metadata": {},
   "source": [
    "### Signing detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6b7b9ef-afe7-4e27-a48b-55fdccdbd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose coordinates for hand movement.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "def extract_for_signing_detection(res):\n",
    "    # Extract specific pose landmarks if available\n",
    "    px = []\n",
    "    py = []\n",
    "    pz = []\n",
    "    if res.pose_landmarks:\n",
    "        for i in POSE:\n",
    "            lm = res.pose_landmarks.landmark[i]\n",
    "            px.append(lm.x)\n",
    "            py.append(lm.y)\n",
    "            pz.append(lm.z)\n",
    "    else:\n",
    "        px = [0.0]*len(POSE)\n",
    "        py = [0.0]*len(POSE)\n",
    "        pz = [0.0]*len(POSE)\n",
    "\n",
    "    # Extract left hand landmarks if available\n",
    "    lx = []\n",
    "    ly = []\n",
    "    lz = []\n",
    "    if res.left_hand_landmarks:\n",
    "        for lm in res.left_hand_landmarks.landmark:\n",
    "            lx.append(lm.x)\n",
    "            ly.append(lm.y)\n",
    "            lz.append(lm.z)\n",
    "    else:\n",
    "        lx = [0.0]*21\n",
    "        ly = [0.0]*21\n",
    "        lz = [0.0]*21\n",
    "\n",
    "    # Extract right hand landmarks if available\n",
    "    rx = []\n",
    "    ry = []\n",
    "    rz = []\n",
    "    if res.right_hand_landmarks:\n",
    "        for lm in res.right_hand_landmarks.landmark:\n",
    "            rx.append(lm.x)\n",
    "            ry.append(lm.y)\n",
    "            rz.append(lm.z)\n",
    "    else:\n",
    "        rx = [0.0]*21\n",
    "        ry = [0.0]*21\n",
    "        rz = [0.0]*21\n",
    "\n",
    "    return list(chain(rx, lx, px, ry, ly, py, rz, lz, pz))\n",
    "\n",
    "# Only load once\n",
    "signing_detection_model = tf.saved_model.load(\"signing_detection_model\")\n",
    "\n",
    "class SigningDetectionModel:\n",
    "    def __init__(self):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "\n",
    "    def is_signing(self, mp_holistic_result):\n",
    "        inp_lm = extract_for_signing_detection(mp_holistic_result)\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        return signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy() == 1\n",
    "\n",
    "class BufferedSigningDetectionModel:\n",
    "    def __init__(self, buffer_len=5, confidence_number=3):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "        self.signing_detector_buffer = deque(maxlen=buffer_len)\n",
    "        self.confidence_number = confidence_number \n",
    "\n",
    "    def is_signing(self, mp_holistic_result):\n",
    "        inp_lm = extract_for_signing_detection(mp_holistic_result)\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        pred = signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy()\n",
    "        self.signing_detector_buffer.append(pred)\n",
    "        buffered_pred, count = Counter(self.signing_detector_buffer).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            return buffered_pred == 1\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5db61d-850f-4549-8b85-1e32f87fe04d",
   "metadata": {},
   "source": [
    "### Fingerspelling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "364d8881-9eaf-4b16-9b2b-f5320aa80a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that currently the formatter only supports pose and hand landmarks not face landmarks\n",
    "class ModelInputFormatter:\n",
    "    def __init__(self, model):\n",
    "        self.required_landmarks = [bytes.decode(\"utf-8\") for bytes in model.info().numpy()]\n",
    "\n",
    "    def get_model_input(self, mp_holistic_result):\n",
    "        (rx, ry, rz), (lx, ly, lz), (px, py, pz) = self._extract_from_result(mp_holistic_result)\n",
    "\n",
    "        mapped_list = []\n",
    "        for item in self.required_landmarks:\n",
    "            parts = item.split('_')\n",
    "            idx = int(parts[-1]) # Extract the index\n",
    "        \n",
    "            if parts[0] == 'x':\n",
    "                if 'right_hand' in item:\n",
    "                    mapped_list.append(rx[idx])\n",
    "                elif 'left_hand' in item:\n",
    "                    mapped_list.append(lx[idx])\n",
    "                elif 'pose' in item:\n",
    "                    mapped_list.append(px[idx])\n",
    "        \n",
    "            elif parts[0] == 'y':\n",
    "                if 'right_hand' in item:\n",
    "                    mapped_list.append(ry[idx])\n",
    "                elif 'left_hand' in item:\n",
    "                    mapped_list.append(ly[idx])\n",
    "                elif 'pose' in item:\n",
    "                    mapped_list.append(py[idx])\n",
    "        \n",
    "            elif parts[0] == 'z':\n",
    "                if 'right_hand' in item:\n",
    "                    mapped_list.append(rz[idx])\n",
    "                elif 'left_hand' in item:\n",
    "                    mapped_list.append(lz[idx])\n",
    "                elif 'pose' in item:\n",
    "                    mapped_list.append(pz[idx])\n",
    "\n",
    "        return mapped_list\n",
    "\n",
    "    def _extract_from_result(self, res):\n",
    "        # Extract specific pose landmarks if available\n",
    "        px = []\n",
    "        py = []\n",
    "        pz = []\n",
    "        if res.pose_landmarks:\n",
    "            for lm in res.pose_landmarks.landmark:\n",
    "                px.append(lm.x)\n",
    "                py.append(lm.y)\n",
    "                pz.append(lm.z)\n",
    "        else:\n",
    "            px = [0.0]*len(POSE)\n",
    "            py = [0.0]*len(POSE)\n",
    "            pz = [0.0]*len(POSE)\n",
    "    \n",
    "        # Extract left hand landmarks if available\n",
    "        lx = []\n",
    "        ly = []\n",
    "        lz = []\n",
    "        if res.left_hand_landmarks:\n",
    "            for lm in res.left_hand_landmarks.landmark:\n",
    "                lx.append(lm.x)\n",
    "                ly.append(lm.y)\n",
    "                lz.append(lm.z)\n",
    "        else:\n",
    "            lx = [0.0]*21\n",
    "            ly = [0.0]*21\n",
    "            lz = [0.0]*21\n",
    "    \n",
    "        # Extract right hand landmarks if available\n",
    "        rx = []\n",
    "        ry = []\n",
    "        rz = []\n",
    "        if res.right_hand_landmarks:\n",
    "            for lm in res.right_hand_landmarks.landmark:\n",
    "                rx.append(lm.x)\n",
    "                ry.append(lm.y)\n",
    "                rz.append(lm.z)\n",
    "        else:\n",
    "            rx = [0.0]*21\n",
    "            ry = [0.0]*21\n",
    "            rz = [0.0]*21\n",
    "    \n",
    "        return (rx, ry, rz), (lx, ly, lz), (px, py, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41e001-5e4c-4f8f-8ffb-0ade52357348",
   "metadata": {},
   "source": [
    "#### Continuous model\n",
    "\n",
    "Works well for isolated sequences. Can't handle sudden pauses, and stops.\n",
    "Extremely sensitive to window size. Also, the training data was from professional signers. For beginners who sign slower the same window size isn't suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9ed11c3-636d-4783-9831-04bd4e32280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousRecognitionModel:\n",
    "    def __init__(self, model, confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7):\n",
    "        self.model = model\n",
    "        self.formatter = ModelInputFormatter(self.model)\n",
    "\n",
    "        # Collect a maximum of inp_buf_len frames for inference\n",
    "        self.input = deque(maxlen=inp_buf_len)\n",
    "        self.inp_len = inp_buf_len\n",
    "\n",
    "        # The output is also buffered\n",
    "        self.inner_fifo = deque(maxlen=out_buf_len)\n",
    "        # Only predictions with a higher confidence make it inside the buffer\n",
    "        self.trust_confidence = confidence_threshold\n",
    "        # Need a confidence_number majority in the buffer to be returned as output\n",
    "        self.confidence_number = out_majority_threshold\n",
    "        # Previous predictions for the model\n",
    "        self.context = str(num_to_char[start_token_idx])\n",
    "\n",
    "    def process_frame(self, mp_holistic_result):\n",
    "        selected_landmarks_for_model = self.formatter.get_model_input(mp_holistic_result)\n",
    "        self.input.append(selected_landmarks_for_model)\n",
    "    \n",
    "        res = self.model.predict(self.input, self.context)\n",
    "        pred = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        prob = res[\"confidence\"].numpy()\n",
    "\n",
    "        if prob < self.trust_confidence:\n",
    "            return None\n",
    "\n",
    "        self.inner_fifo.append(pred)\n",
    "        pred_char, count = Counter(self.inner_fifo).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            if self.context[-1] != pred_char:\n",
    "                self.context += pred_char\n",
    "                self.context = self.context[-3:]\n",
    "                print(pred_char, end=\"\")\n",
    "                \n",
    "                # Predicted the end\n",
    "                # if pred_char == '>':\n",
    "                #     # restart the detection\n",
    "                #     self.context = str(num_to_char[start_token_idx])\n",
    "                #     self.inner_fifo.clear()\n",
    "                #     self.input.clear()\n",
    "\n",
    "                return pred_char\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8654e21a-4e54-4a51-ab05-d4ad7bcf9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2aaecdc5-2948-4514-8b41-4f3ca8a6817a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear"
     ]
    }
   ],
   "source": [
    "fs_model = ContinuousRecognitionModel(loaded_model, confidence_threshold=0.2, inp_buf_len=25, out_buf_len=10, out_majority_threshold=7)\n",
    "time.sleep(2)\n",
    "video_loop(os.path.join(\"test_videos\", \"bear.mp4\"), lambda data: fs_model.process_frame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a0ac9-bbc8-4f4c-93f2-d8e34c687cd8",
   "metadata": {},
   "source": [
    "#### Translate in long chunks\n",
    "\n",
    "This model performs well on single words that fit into the buffer. But for longer text it fails to translate well, presumable because the signs are cut off at the wrong positions. For longer text pause detection/signing detection is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a278ff2-c389-4e32-83f2-e07b6d25dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonContinuousRecognitionModel:\n",
    "    def __init__(self, model, max_out_length=31, confidence_threshold=0.2):\n",
    "        self.model = model\n",
    "        self.formatter = ModelInputFormatter(self.model)\n",
    "\n",
    "        self.max_out_length = max_out_length\n",
    "        # Only predictions with a higher confidence count as a predicted character\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.input = []\n",
    "\n",
    "    def reset_buffer(self):\n",
    "        self.input.clear()\n",
    "\n",
    "    def translate_buffer(self, reset_buffer=False):\n",
    "        res = None\n",
    "        if len(self.input) > 0:\n",
    "            res = self._generate_with_confidence()\n",
    "            \n",
    "        if reset_buffer:\n",
    "            self.reset_buffer()\n",
    "            \n",
    "        return res\n",
    "\n",
    "    def process_frame(self, mp_holistic_result):\n",
    "        selected_landmarks_for_model = self.formatter.get_model_input(mp_holistic_result)\n",
    "        self.input.append(selected_landmarks_for_model)\n",
    "\n",
    "    def _generate_with_confidence(self):\n",
    "        ctx = str(num_to_char[start_token_idx])\n",
    "        for i in range(self.max_out_length):\n",
    "            res = self.model.predict(self.input, ctx)\n",
    "            res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "            prob = res[\"confidence\"].numpy()\n",
    "            if prob > self.confidence_threshold:\n",
    "                ctx += res_char\n",
    "                if res_char == num_to_char[end_token_idx]:\n",
    "                    break\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d686a5eb-f859-4263-98ee-8aa97a2d1269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<alligator>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(loaded_model, max_out_length=31, confidence_threshold=0.0)\n",
    "time.sleep(2)\n",
    "video_loop(os.path.join(\"test_videos\", \"alligator.mp4\"), lambda data: fs_model.process_frame(data))\n",
    "fs_model.translate_buffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795541e-7611-4f9e-883f-d5aea74a7c2f",
   "metadata": {},
   "source": [
    "#### Translate in long chunks  with signing detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cad2ab0e-7887-4198-b546-4023a052152c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<monkey monkey>'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(loaded_model, max_out_length=MAX_PHRASE_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "#sign_detector = BufferedSigningDetectionModel()\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "\n",
    "video_loop(os.path.join(\"test_videos\", \"monkey.mp4\"), process_data)\n",
    "fs_model.translate_buffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fb70a-d66e-4787-849c-64bc8b273448",
   "metadata": {},
   "source": [
    "#### Translate longer sequences with stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3bf55500-bb63-40f8-b325-a9dc9221f318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<earr>\n",
      "<betyerenetel.net>\n",
      "<geniger r>\n",
      "<lex>\n",
      "<ebra>\n",
      "<leene.hu>\n",
      "<ynukt/\n",
      "<3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<angaro>'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(loaded_model, max_out_length=MAX_PHRASE_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "sign_detector = BufferedSigningDetectionModel(buffer_len=10, confidence_number=7)\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "    else:\n",
    "        res = fs_model.translate_buffer(reset_buffer=True)\n",
    "        if res and res != \"<\":\n",
    "            print(res)\n",
    "\n",
    "video_loop(os.path.join(\"test_videos\", \"fingerspelling_animals.mp4\"), process_data)\n",
    "fs_model.translate_buffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bde39c-08ef-480f-bc05-2ace53c091ed",
   "metadata": {},
   "source": [
    "### Correct with llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f6c8f2fb-d561-4e38-95bb-fe45beae8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.environ.get('OPEN_AI_API_KEY')\n",
    "if key is not None:\n",
    "    openai.api_key = key\n",
    "else:\n",
    "    print(\"Error: Please set a valid api key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b4bdbba8-547d-4e39-b04a-8ffda0018eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_output(pred):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\", \n",
    "       messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a machine that tries to correct the output of a fingerspelling recognition model. Some letters might be missing, but it's also possible that the given text has extra characters. Only reply the corrected text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"angaro angaro\"},\n",
    "        {\"role\": \"system\", \"content\": \"kangaroo\"},\n",
    "        {\"role\": \"user\", \"content\": \"beark\"},\n",
    "        {\"role\": \"system\", \"content\": \"bear\"},\n",
    "        {\"role\": \"user\", \"content\": \"6 halee hale\"},\n",
    "        {\"role\": \"system\", \"content\": \"whale\"},\n",
    "        {\"role\": \"user\", \"content\": pred},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e94ca388-1937-4af7-9532-ad3a865a1814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earth/tiger/tiger/angry/key/alligator/horse'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_output(\"earkh/tiger/tiger angar key ligator alligator h horse gro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c7d3ddbb-ba75-4de9-ae78-43f60fb8e09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ear\n",
      "betyerenetel.net\n",
      "ginger\n",
      "I apologize, but I couldn't understand the correction you were trying to make. Can you please provide more context or clarify your input?\n",
      "zebra\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<leene.hu>'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(loaded_model, max_out_length=MAX_PHRASE_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "sign_detector = BufferedSigningDetectionModel(buffer_len=10, confidence_number=7)\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "    else:\n",
    "        res = fs_model.translate_buffer(reset_buffer=True)\n",
    "        if res and res != \"<\":\n",
    "            print(correct_output(res[1:]))\n",
    "\n",
    "video_loop(os.path.join(\"test_videos\", \"fingerspelling_animals.mp4\"), process_data)\n",
    "fs_model.translate_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885df677-3186-4225-9691-b7299b2a096f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
