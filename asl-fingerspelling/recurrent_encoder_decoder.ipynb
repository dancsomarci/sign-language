{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670c23e9",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3357796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install tensorflow\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install mediapipe==0.9.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afa879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pyarrow.parquet as pq\n",
    "from tensorflow.keras import layers\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815f7292-d5cd-4eea-aef2-81a594138520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n",
      "TensorFlow v2.14.0\n",
      "Mediapipe v0.9.0.1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336326cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "cv2.setRNGSeed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dff822-75bc-493e-b175-14481c2abc76",
   "metadata": {},
   "source": [
    "# Fetch from TfRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4d1354-adc7-4073-a64b-3c72761e2e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 68 TFRecord files.\n"
     ]
    }
   ],
   "source": [
    "PATH_KAGGLE_DS = \"kaggle_dataset\"\n",
    "dataset_df = pd.read_csv(os.path.join(PATH_KAGGLE_DS, \"train.csv\"))\n",
    "PATH_TFRECORD_DS = os.path.join(PATH_KAGGLE_DS, \"train_tfrecords\")\n",
    "tf_records = dataset_df.file_id.map(lambda x: os.path.join(PATH_TFRECORD_DS, f\"{x}.tfrecord\")).unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f496051-1067-479f-8b5f-3aedbd4d3303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_right_hand_0',\n",
       " 'x_right_hand_1',\n",
       " 'x_right_hand_2',\n",
       " 'x_right_hand_3',\n",
       " 'x_right_hand_4',\n",
       " 'x_right_hand_5',\n",
       " 'x_right_hand_6',\n",
       " 'x_right_hand_7',\n",
       " 'x_right_hand_8',\n",
       " 'x_right_hand_9']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(PATH_TFRECORD_DS, \"feature_columns.json\"), 'r') as f:\n",
    "    json_str = f.read()\n",
    "FEATURE_COLUMNS = json.loads(json_str)\n",
    "FEATURE_COLUMNS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a432491-d637-4216-812b-7d8da9366295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These points represent the hands, elbows, and shoulders.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "\n",
    "# Facial information isn't necessary, but the nose will serve as a midpoint for normalizing the data, as it is usually located in the middle of the frame.\n",
    "FPOSE = [0] # Nose as midpoint\n",
    "\n",
    "# Collecting the indices of certain important/distinct sets of features.\n",
    "# This can be beneficial during the preprocessing step.\n",
    "RHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\n",
    "LHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\n",
    "RPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) in RPOSE]\n",
    "LPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) in LPOSE]\n",
    "MID_POINT_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) == 0] # Nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5699c6a-6b6e-4643-8a54-62183c475823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COLUMNS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n",
    "    # Transpose to maintain the original shape of landmarks data.\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "    \n",
    "    return landmarks, phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3895dc76-d15a-49a7-888a-31c5f92ec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default mapping that came with the dataset was changed:\n",
    "# padding is represented with the number 0\n",
    "# start_token is 60\n",
    "# end_token is 61\n",
    "with open (os.path.join(PATH_KAGGLE_DS, \"character_to_prediction_index.json\"), \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "    \n",
    "char_to_num = {c:char_to_num[c]+1 for c in char_to_num}\n",
    "\n",
    "# Add pad_token, start pointer and end pointer to the dict\n",
    "pad_token = 'P'\n",
    "pad_token_idx = 0\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "\n",
    "start_token = '<'\n",
    "start_token_idx = 60\n",
    "char_to_num[start_token] = start_token_idx\n",
    "\n",
    "end_token = '>'\n",
    "end_token_idx = 61\n",
    "char_to_num[end_token] = end_token_idx\n",
    "\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891424b9-95aa-478b-877b-5c99da8a8b26",
   "metadata": {},
   "source": [
    "## Preprocess phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f559ea97-8029-4754-af54-939aa7be587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(char_to_num.keys()),\n",
    "        values=list(char_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"tf_char_to_num\"\n",
    ")\n",
    "\n",
    "# Function to decode the characters and pad the phrases\n",
    "MAX_PHRASE_LEN = 31 + 2 # The start and end token take space as well\n",
    "def preprocess_phrase(phrase):\n",
    "    phrase = start_token + phrase + end_token\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "    \n",
    "    max_len_plus = MAX_PHRASE_LEN + 1\n",
    "    amount_to_pad = max_len_plus - tf.shape(phrase)[0]\n",
    "    \n",
    "    if amount_to_pad > 0:\n",
    "        phrase = tf.pad(phrase, paddings=[[0, amount_to_pad]], mode = 'CONSTANT', constant_values = pad_token_idx)\n",
    "    else:\n",
    "        phrase = phrase[:max_len_plus]\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf3bba-a24b-453b-b47e-396045f03925",
   "metadata": {},
   "source": [
    "## Preprocess landmark A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a815e8c9-6f0d-4e8e-8500-aaacac83584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LEN = 256\n",
    "def preprocess_landmark_a(x):\n",
    "    # Select distinct groups\n",
    "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
    "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
    "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
    "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
    "    \n",
    "    # Calculate how many unknown values there are for each hand\n",
    "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
    "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
    "    rnans = tf.math.count_nonzero(rnan_idx)\n",
    "    lnans = tf.math.count_nonzero(lnan_idx)\n",
    "    \n",
    "    # As fingerspelling gestures only use one hand, the model will only receive the dominant one. (=less NaN values)\n",
    "    # Also from the pose information, only the dominant side is needed.\n",
    "    is_right_hand = True\n",
    "    hand = rhand\n",
    "    pose = rpose\n",
    "    num_of_hand_lm = len(RHAND_IDX)\n",
    "    num_of_pose_lm = len(RPOSE_IDX)\n",
    "    if rnans > lnans:\n",
    "        is_right_hand = False\n",
    "        hand = lhand\n",
    "        pose = lpose\n",
    "        num_of_hand_lm = len(LHAND_IDX)\n",
    "        num_of_pose_lm = len(LPOSE_IDX)\n",
    "    \n",
    "    # Gather channels\n",
    "    hand_x = hand[:, 0*(num_of_hand_lm//3) : 1*(num_of_hand_lm//3)]\n",
    "    hand_y = hand[:, 1*(num_of_hand_lm//3) : 2*(num_of_hand_lm//3)]\n",
    "    hand_z = hand[:, 2*(num_of_hand_lm//3) : 3*(num_of_hand_lm//3)]\n",
    "    \n",
    "    # Flip the x axis if it's the left hand. (This makes it resemble the right hand more.)\n",
    "    if not is_right_hand:\n",
    "        hand_x = 1 - hand_x\n",
    "    \n",
    "    # Join along a new axis\n",
    "    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1) # (SEQ_LEN, LM_COUNT, 3)\n",
    "    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :] # Mean and std along the first axis (across LM_COUNT)\n",
    "    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n",
    "    hand = (hand - mean) / std\n",
    "    \n",
    "    # The same for pose\n",
    "    pose_x = pose[:, 0*(num_of_pose_lm//3) : 1*(num_of_pose_lm//3)]\n",
    "    pose_y = pose[:, 1*(num_of_pose_lm//3) : 2*(num_of_pose_lm//3)]\n",
    "    pose_z = pose[:, 2*(num_of_pose_lm//3) : 3*(num_of_pose_lm//3)]\n",
    "    \n",
    "    if not is_right_hand:\n",
    "        pose_x = 1 - pose_x\n",
    "    \n",
    "    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n",
    "    mean = tf.math.reduce_mean(pose, axis=1)[:, tf.newaxis, :]\n",
    "    std = tf.math.reduce_std(pose, axis=1)[:, tf.newaxis, :]\n",
    "    pose = (pose - mean) / std\n",
    "    \n",
    "    # Join the data\n",
    "    x = tf.concat([hand, pose], axis=1)\n",
    "    \n",
    "    # Replace NaN with 0.\n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "    \n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    \n",
    "    # x.shape == (FRAME_LEN, FEATURE_COUNT, 3)\n",
    "    \n",
    "    # Reshape to (FRAME_LEN, FEATURE_COUNT*3 == num_of_hand_lm + num_of_pose_lm)\n",
    "    x = tf.reshape(x, (FRAME_LEN, num_of_hand_lm + num_of_pose_lm))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3715de-25ce-4014-b0f5-8ecd02dfd538",
   "metadata": {},
   "source": [
    "## Preprocess landmark B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "50f64ec3-d8fa-4f7d-aabf-c74c93acebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LEN = 256\n",
    "\n",
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "   \n",
    "def preprocess_landmark_b(x):\n",
    "    # Select distinct groups\n",
    "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
    "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
    "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
    "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
    "    nose = tf.gather(x, MID_POINT_IDX, axis=1)\n",
    "    \n",
    "    # Calculate how many unknown values there are for each hand\n",
    "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
    "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
    "    rnans = tf.math.count_nonzero(rnan_idx)\n",
    "    lnans = tf.math.count_nonzero(lnan_idx)\n",
    "    \n",
    "    # As fingerspelling gestures only use one hand, the model will only receive the dominant one. (=less NaN values)\n",
    "    # Also from the pose information, only the dominant side is needed.\n",
    "    is_right_hand = True\n",
    "    hand = rhand\n",
    "    pose = rpose\n",
    "    num_of_hand_lm = len(RHAND_IDX)\n",
    "    num_of_pose_lm = len(RPOSE_IDX)\n",
    "    if rnans > lnans:\n",
    "        is_right_hand = False\n",
    "        hand = lhand\n",
    "        pose = lpose\n",
    "        num_of_hand_lm = len(LHAND_IDX)\n",
    "        num_of_pose_lm = len(LPOSE_IDX)\n",
    "    \n",
    "    # Gather channels\n",
    "    hand_x = hand[:, 0*(num_of_hand_lm//3) : 1*(num_of_hand_lm//3)]\n",
    "    hand_y = hand[:, 1*(num_of_hand_lm//3) : 2*(num_of_hand_lm//3)]\n",
    "    hand_z = hand[:, 2*(num_of_hand_lm//3) : 3*(num_of_hand_lm//3)]\n",
    "    pose_x = pose[:, 0*(num_of_pose_lm//3) : 1*(num_of_pose_lm//3)]\n",
    "    pose_y = pose[:, 1*(num_of_pose_lm//3) : 2*(num_of_pose_lm//3)]\n",
    "    pose_z = pose[:, 2*(num_of_pose_lm//3) : 3*(num_of_pose_lm//3)]\n",
    "    nose_x = nose[:, 0 : 1]\n",
    "    nose_y = nose[:, 1 : 2]\n",
    "    nose_z = nose[:, 2 : 3]\n",
    "    # Combine all x, y, z coordinates\n",
    "    combined_x = tf.concat([nose_x, hand_x, pose_x], axis=-1)\n",
    "    combined_y = tf.concat([nose_y, hand_y, pose_y], axis=-1)\n",
    "    combined_z = tf.concat([nose_z, hand_z, pose_z], axis=-1)\n",
    "    \n",
    "    # Flip the x axis if it's the left hand. (This makes it resemble the right hand more.)\n",
    "    if not is_right_hand:\n",
    "        hand_x = 1 - hand_x\n",
    "    # Join along a new axis\n",
    "    x = tf.concat([combined_x[..., tf.newaxis], combined_y[..., tf.newaxis], combined_z[..., tf.newaxis]], axis=-1) # (SEQ_LEN, LM_COUNT, 3)\n",
    "    # x.shape == (T, P, C)\n",
    "    # N: Batch size -> 1 for now\n",
    "    # T: Sequence length\n",
    "    # P: Number of points (or landmarks)\n",
    "    # C: Number of channels (features) per point\n",
    "    \n",
    "    x = x[None,...] # x.shape == (1, T, P, C)\n",
    "    \n",
    "    # Mean of midpoints (nose) through timesteps\n",
    "    mean = tf_nan_mean(tf.gather(x, [0], axis=2), axis=[1,2], keepdims=True)\n",
    "    mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
    "    # x.shape == (N, 1, 1, C)\n",
    "    \n",
    "    std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "    # std.shape == (N, 1, 1, C)\n",
    "    \n",
    "    x = (x - mean)/std\n",
    "    # x.shape == (N, T, P, C)\n",
    "\n",
    "    # Limit seq_len\n",
    "    x = x[:,:FRAME_LEN]\n",
    "    seq_length = tf.shape(x)[1]\n",
    "    x = x[...,:2]\n",
    "    # x.shape == (N, T, P, 2)\n",
    "\n",
    "    # Calc differences between consequtive frames and with skipping 1 in between\n",
    "    dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "    # dx.shape == (N, T, P, 2)\n",
    "    \n",
    "    x = tf.concat([\n",
    "        tf.reshape(x, (-1,seq_length,2*tf.shape(x)[2])),\n",
    "        tf.reshape(dx, (-1,seq_length,2*tf.shape(dx)[2])),\n",
    "    ], axis = -1)\n",
    "    # x.shape == (N, T, features) where T <= FRAME_LEN\n",
    "    \n",
    "    # Replace NaN values with zeros\n",
    "    x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
    "\n",
    "    x = x[0] # x.shape == (T, features)\n",
    "    \n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, [[0, FRAME_LEN-tf.shape(x)[0]], [0, 0]])\n",
    "\n",
    "    # x.shape == (FRAME_LEN, features)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d31ae-9fce-4344-a577-c87dcc0004fe",
   "metadata": {},
   "source": [
    "## Combined preprocessing for lm and phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2b7f6c7c-e242-499a-80f6-b1d70135f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(landmark, phrase):\n",
    "    phrase = preprocess_phrase(phrase)\n",
    "    return (preprocess_landmark_b(landmark), phrase[:-1]), phrase[1:] # Shifted phrase for encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e383e-6495-476f-bdc4-e8cb07f8a0d5",
   "metadata": {},
   "source": [
    "## Create TFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4e588aba-02b0-4dce-872b-890743689447",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_len = int(0.8 * len(tf_records))\n",
    "\n",
    "def get_dataset(tfrecords, batch_size=1, repeat=False, shuffle=False, drop_remainder=False, cache=False):\n",
    "    ds = tf.data.TFRecordDataset(tf_records)\n",
    "    ds = ds.map(decode_fn, tf.data.AUTOTUNE)\n",
    "    # Note: preprocessing can happen before and after the batching (if you can preprocess the whole batch at once to save computation time)\n",
    "    ds = ds.map(preprocess, tf.data.AUTOTUNE)\n",
    "    \n",
    "    if repeat: \n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_deterministic = (False)\n",
    "        ds = ds.with_options(options)\n",
    "\n",
    "    if batch_size > 1:\n",
    "        # There's also a padded_batch version of this function\n",
    "        ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "        \n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # If the system doesn't have enough RAM caching might slow down the process\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    \n",
    "    return ds\n",
    "\n",
    "train_ds = get_dataset(tf_records[:train_len], batch_size=batch_size, cache=True)\n",
    "valid_ds = get_dataset(tf_records[train_len:], batch_size=batch_size, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5523cc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "\n",
      "Saved shapes:\n",
      "lm_shape: 108\n",
      "phrase_shape: 32\n",
      "----------------------------------------\n",
      "\n",
      "Encoder input - first in batch (Landmarks:)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(32, 256, 108)\n",
      "tf.Tensor(\n",
      "[[ 0.04437668  0.02177401 -0.74466854 ...  0.21614158 -0.20177615\n",
      "   0.24252397]\n",
      " [ 0.04366023  0.02140689  0.         ...  0.03143382  0.15882957\n",
      "  -0.02012992]\n",
      " [ 0.04387136  0.02046395  0.         ... -0.2204709   0.14418939\n",
      "  -0.21183419]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]], shape=(256, 108), dtype=float32)\n",
      "----------------------------------------\n",
      "\n",
      "Decoder input (Context):\n",
      "(32, 33)\n",
      "tf.Tensor(\n",
      "[60 19  1 35 50 37 37 43 40 47 53 51 37 61  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n",
      "\n",
      "Model target output (Phrase):\n",
      "(32, 33)\n",
      "tf.Tensor(\n",
      "[19  1 35 50 37 37 43 40 47 53 51 37 61  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lm_shape = None\n",
    "phrase_shape = None\n",
    "\n",
    "# Create an iterator for the train and valid datasets\n",
    "train_iterator = iter(train_ds)\n",
    "\n",
    "# Print data points from the training dataset\n",
    "print(\"Training Data:\\n\")\n",
    "(landmarks, context), phrase = next(train_iterator)\n",
    "\n",
    "# Save shapes\n",
    "lm_shape = landmarks.shape[2]\n",
    "phrase_shape = phrase.shape[0]\n",
    "print(\"Saved shapes:\")\n",
    "print(f\"lm_shape: {lm_shape}\")\n",
    "print(f\"phrase_shape: {phrase_shape}\")\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Encoder input - first in batch (Landmarks:)\")\n",
    "print(type(landmarks))\n",
    "print(landmarks.shape)\n",
    "print(landmarks[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Decoder input (Context):\")\n",
    "print(context.shape)\n",
    "print(context[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Model target output (Phrase):\")\n",
    "print(phrase.shape)\n",
    "print(phrase[0])\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0fb83",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee33612-effd-47b9-8409-e4640ed4da80",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8c63717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    \n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1) \n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalTokenEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_future_input_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=max_future_input_size, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "class LandmarkEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, len_of_seq, d_model, num_conv_layers, filter_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.len_of_seq = len_of_seq\n",
    "        self.conv_block = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv1D(d_model, filter_size, padding=\"same\", activation=\"relu\")\n",
    "            for _ in range(num_conv_layers)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "\n",
    "# This layer is semantically incorrect, but was experimented with\n",
    "class PositionalLandmarkEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, len_of_seq, d_model, num_conv_layers, filter_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.len_of_seq = len_of_seq\n",
    "        self.conv_block = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv1D(d_model, filter_size, padding=\"same\", activation=\"relu\")\n",
    "            for _ in range(num_conv_layers)\n",
    "        ])\n",
    "        self.pos_encoding = positional_encoding(length=len_of_seq, depth=d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :self.len_of_seq, :]\n",
    "        return x\n",
    "    \n",
    "# class Seq2SeqLSTM(tf.keras.Model):\n",
    "#     def __init__(self, *, len_lm_seq, num_conv_layers, filter_size, d_model, input_vocab_size, target_vocab_size, max_future_input_size, dropout_rate=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.enc_pos_embedding = PositionalLandmarkEmbedding(len_lm_seq, d_model, num_conv_layers, filter_size)\n",
    "#         self.enc_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "#         self.enc_LSTM = tf.keras.layers.LSTM(d_model, return_state=True)\n",
    "        \n",
    "#         self.dec_pos_embedding = PositionalTokenEmbedding(vocab_size=input_vocab_size, d_model=d_model, max_future_input_size=max_future_input_size)\n",
    "#         self.dec_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "#         self.dec_LSTM = tf.keras.layers.LSTM(d_model, return_state=True, return_sequences=True)\n",
    "    \n",
    "#         self.classifier = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         landmark_seq, prev_gen_ctx  = inputs\n",
    "    \n",
    "#         encoded_lm_seq = self.enc_pos_embedding(landmark_seq)\n",
    "#         encoded_lm_seq = self.enc_dropout(encoded_lm_seq)\n",
    "#         encoder_outputs, state_h, state_c = self.enc_LSTM(encoded_lm_seq)\n",
    "\n",
    "#         embedded_ctx = self.dec_pos_embedding(prev_gen_ctx)\n",
    "#         embedded_ctx = self.dec_dropout(embedded_ctx)\n",
    "        \n",
    "#         mask = self.dec_pos_embedding.compute_mask(prev_gen_ctx)\n",
    "#         decoder_outputs, _, _ = self.dec_LSTM(embedded_ctx, initial_state=[state_h, state_c], mask=mask)\n",
    "        \n",
    "#         logits = self.classifier(decoder_outputs)\n",
    "\n",
    "#         try:\n",
    "#             # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "#             # b/250038731\n",
    "#             del logits._keras_mask\n",
    "#         except AttributeError:\n",
    "#             pass\n",
    "    \n",
    "#         return logits\n",
    "\n",
    "class Seq2SeqRecurrent(tf.keras.Model):\n",
    "    def __init__(self, *, recurrent_unit_type, num_recurrent_layers=1, len_lm_seq, num_conv_layers, filter_size, d_model, input_vocab_size, target_vocab_size, max_future_input_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        if recurrent_unit_type not in ['LSTM', 'SimpleRNN', 'GRU']:\n",
    "            raise ValueError(f\"Invalid recurrent_unit_type: {recurrent_unit_type}. Choose from ['LSTM', 'SimpleRNN', 'GRU']\")\n",
    "\n",
    "        # Positional embeddings and dropout\n",
    "        self.enc_pos_embedding = LandmarkEmbedding(len_lm_seq, d_model, num_conv_layers, filter_size)\n",
    "        self.enc_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_pos_embedding = PositionalTokenEmbedding(vocab_size=input_vocab_size, d_model=d_model, max_future_input_size=max_future_input_size)\n",
    "        self.dec_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        # Encoder recurrent units\n",
    "        recurrent_unit = {'LSTM': tf.keras.layers.LSTM, 'SimpleRNN': tf.keras.layers.SimpleRNN, 'GRU': tf.keras.layers.GRU}[recurrent_unit_type]\n",
    "        self.enc_recurrent_layers = [recurrent_unit(d_model, return_sequences=True, return_state=True) for _ in range(num_recurrent_layers)]\n",
    "        self.dec_recurrent_layers = [recurrent_unit(d_model, return_sequences=True, return_state=True) for _ in range(num_recurrent_layers)]\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        landmark_seq, prev_gen_ctx  = inputs\n",
    "\n",
    "        encoded_lm_seq = self.enc_pos_embedding(landmark_seq)\n",
    "        encoded_lm_seq = self.enc_dropout(encoded_lm_seq)\n",
    "\n",
    "        states = None\n",
    "        for layer in self.enc_recurrent_layers:\n",
    "            encoded_lm_seq, *states = layer(encoded_lm_seq, initial_state=states)\n",
    "\n",
    "        # states now contain the final state(s) of the encoder\n",
    "        embedded_ctx = self.dec_pos_embedding(prev_gen_ctx)\n",
    "        embedded_ctx = self.dec_dropout(embedded_ctx)\n",
    "\n",
    "        mask = self.dec_pos_embedding.compute_mask(prev_gen_ctx)\n",
    "        # Initialize the first decoder layer with the final state of the encoder\n",
    "        embedded_ctx, *_ = self.dec_recurrent_layers[0](embedded_ctx, initial_state=states, mask=mask)\n",
    "\n",
    "        for layer in self.dec_recurrent_layers[1:]:\n",
    "            embedded_ctx, *_ = layer(embedded_ctx, mask=mask)\n",
    "\n",
    "        logits = self.classifier(embedded_ctx)\n",
    "\n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "        return logits\n",
    "\n",
    "def get_model():\n",
    "    d_model = 256 # embedding size\n",
    "    max_future_input_size = MAX_PHRASE_LEN  # In the future the input can be longer than what it's trained on\n",
    "    \n",
    "    seq2seq_model = Seq2SeqRecurrent(\n",
    "        recurrent_unit_type=\"GRU\", # LSTM, SimpleRNN, GRU\n",
    "        num_recurrent_layers=2,\n",
    "        len_lm_seq=FRAME_LEN,\n",
    "        num_conv_layers=3,\n",
    "        filter_size=11,\n",
    "        d_model=d_model,\n",
    "        input_vocab_size=len(char_to_num),\n",
    "        target_vocab_size=len(char_to_num),\n",
    "        max_future_input_size=max_future_input_size,\n",
    "        dropout_rate=0.1)\n",
    "\n",
    "    return seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "587967b4-2a79-4eb9-91fd-c9fcbe419382",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "64de77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 108)\n",
      "(32, 33)\n",
      "(32, 33, 62)\n",
      "Model: \"seq2_seq_recurrent_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_2 (Land  multiple                  1746688   \n",
      " markEmbedding)                                                  \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " positional_token_embedding  multiple                  15872     \n",
      " _26 (PositionalTokenEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " gru_22 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_23 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_24 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_25 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " additive_attention_12 (Add  multiple                  0 (unused)\n",
      " itiveAttention)                                                 \n",
      "                                                                 \n",
      " dense_26 (Dense)            multiple                  15934     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3357502 (12.81 MB)\n",
      "Trainable params: 3357502 (12.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example inference to build the model\n",
    "(lm, ctx), _label = next(iter(train_ds))\n",
    "output = seq2seq_model((lm, ctx))\n",
    "\n",
    "print(lm.shape)\n",
    "print(ctx.shape)\n",
    "print(output.shape)\n",
    "\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8efb6-0a40-40e0-9ac4-50a459176da4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d831b5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    \n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e05ffa5-8fa5-4177-b727-d5a50a2964a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != pad_token_idx\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != pad_token_idx\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec20b8c5-a458-43a6-a62f-b49febbf6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "908d1e3c-7a66-45f9-834b-aeab590c323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_weights_checkpoint.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "48d23bc3-d78b-4a24-839a-478e88686817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seq2seq_model.fit(train_ds,\n",
    "                validation_data=valid_ds,\n",
    "                epochs=30,\n",
    "                callbacks=[checkpoint_callback],\n",
    "                verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f224f29-7b9f-4a9f-901f-6e60fdfcbcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with original preprocessing\n",
    "Epoch 1/20\n",
    "\n",
    "Epoch 1: val_loss improved from inf to 2.35214, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 140s - loss: 2.8293 - masked_accuracy: 0.2098 - val_loss: 2.3521 - val_masked_accuracy: 0.2946 - 140s/epoch - 111ms/step\n",
    "Epoch 2/20\n",
    "\n",
    "Epoch 2: val_loss improved from 2.35214 to 2.14052, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 64s - loss: 2.2375 - masked_accuracy: 0.3197 - val_loss: 2.1405 - val_masked_accuracy: 0.3471 - 64s/epoch - 51ms/step\n",
    "Epoch 3/20\n",
    "\n",
    "Epoch 3: val_loss improved from 2.14052 to 1.99580, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 63s - loss: 2.0647 - masked_accuracy: 0.3680 - val_loss: 1.9958 - val_masked_accuracy: 0.3902 - 63s/epoch - 50ms/step\n",
    "Epoch 4/20\n",
    "\n",
    "Epoch 4: val_loss improved from 1.99580 to 1.87133, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 62s - loss: 1.9208 - masked_accuracy: 0.4124 - val_loss: 1.8713 - val_masked_accuracy: 0.4291 - 62s/epoch - 49ms/step\n",
    "Epoch 5/20\n",
    "\n",
    "Epoch 5: val_loss improved from 1.87133 to 1.81366, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 63s - loss: 1.8091 - masked_accuracy: 0.4461 - val_loss: 1.8137 - val_masked_accuracy: 0.4479 - 63s/epoch - 50ms/step\n",
    "Epoch 6/20\n",
    "\n",
    "Epoch 6: val_loss improved from 1.81366 to 1.73752, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 62s - loss: 1.7284 - masked_accuracy: 0.4700 - val_loss: 1.7375 - val_masked_accuracy: 0.4709 - 62s/epoch - 49ms/step\n",
    "Epoch 7/20\n",
    "\n",
    "Epoch 7: val_loss improved from 1.73752 to 1.69962, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 65s - loss: 1.6629 - masked_accuracy: 0.4890 - val_loss: 1.6996 - val_masked_accuracy: 0.4828 - 65s/epoch - 51ms/step\n",
    "Epoch 8/20\n",
    "\n",
    "Epoch 8: val_loss improved from 1.69962 to 1.66101, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 63s - loss: 1.6073 - masked_accuracy: 0.5053 - val_loss: 1.6610 - val_masked_accuracy: 0.4937 - 63s/epoch - 50ms/step\n",
    "Epoch 9/20\n",
    "\n",
    "Epoch 9: val_loss improved from 1.66101 to 1.63128, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 63s - loss: 1.5573 - masked_accuracy: 0.5200 - val_loss: 1.6313 - val_masked_accuracy: 0.5025 - 63s/epoch - 50ms/step\n",
    "Epoch 10/20\n",
    "\n",
    "Epoch 10: val_loss improved from 1.63128 to 1.60701, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 63s - loss: 1.5171 - masked_accuracy: 0.5324 - val_loss: 1.6070 - val_masked_accuracy: 0.5117 - 63s/epoch - 50ms/step\n",
    "Epoch 11/20\n",
    "\n",
    "Epoch 11: val_loss improved from 1.60701 to 1.58705, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 62s - loss: 1.4777 - masked_accuracy: 0.5443 - val_loss: 1.5870 - val_masked_accuracy: 0.5172 - 62s/epoch - 49ms/step\n",
    "Epoch 12/20\n",
    "\n",
    "Epoch 12: val_loss improved from 1.58705 to 1.56979, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 62s - loss: 1.4447 - masked_accuracy: 0.5538 - val_loss: 1.5698 - val_masked_accuracy: 0.5227 - 62s/epoch - 49ms/step\n",
    "Epoch 13/20\n",
    "\n",
    "Epoch 13: val_loss improved from 1.56979 to 1.55880, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 60s - loss: 1.4123 - masked_accuracy: 0.5636 - val_loss: 1.5588 - val_masked_accuracy: 0.5274 - 60s/epoch - 47ms/step\n",
    "Epoch 14/20\n",
    "\n",
    "Epoch 14: val_loss improved from 1.55880 to 1.54515, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 63s - loss: 1.3827 - masked_accuracy: 0.5725 - val_loss: 1.5452 - val_masked_accuracy: 0.5336 - 63s/epoch - 50ms/step\n",
    "Epoch 15/20\n",
    "\n",
    "Epoch 15: val_loss improved from 1.54515 to 1.53530, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 63s - loss: 1.3567 - masked_accuracy: 0.5803 - val_loss: 1.5353 - val_masked_accuracy: 0.5369 - 63s/epoch - 50ms/step\n",
    "Epoch 16/20\n",
    "\n",
    "Epoch 16: val_loss improved from 1.53530 to 1.53161, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 59s - loss: 1.3317 - masked_accuracy: 0.5881 - val_loss: 1.5316 - val_masked_accuracy: 0.5390 - 59s/epoch - 47ms/step\n",
    "Epoch 17/20\n",
    "\n",
    "Epoch 17: val_loss improved from 1.53161 to 1.52362, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 60s - loss: 1.3068 - masked_accuracy: 0.5955 - val_loss: 1.5236 - val_masked_accuracy: 0.5417 - 60s/epoch - 47ms/step\n",
    "Epoch 18/20\n",
    "\n",
    "Epoch 18: val_loss improved from 1.52362 to 1.51798, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 63s - loss: 1.2860 - masked_accuracy: 0.6025 - val_loss: 1.5180 - val_masked_accuracy: 0.5454 - 63s/epoch - 50ms/step\n",
    "Epoch 19/20\n",
    "\n",
    "Epoch 19: val_loss improved from 1.51798 to 1.51251, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 60s - loss: 1.2643 - masked_accuracy: 0.6088 - val_loss: 1.5125 - val_masked_accuracy: 0.5491 - 60s/epoch - 47ms/step\n",
    "Epoch 20/20\n",
    "\n",
    "Epoch 20: val_loss improved from 1.51251 to 1.51186, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 57s - loss: 1.2445 - masked_accuracy: 0.6156 - val_loss: 1.5119 - val_masked_accuracy: 0.5502 - 57s/epoch - 45ms/step\n",
    "CPU times: user 21min 31s, sys: 1min 6s, total: 22min 38s\n",
    "Wall time: 25min 40s\n",
    "<keras.callbacks.History at 0x7ee928303850>\n",
    "\n",
    "    Epoch 1/20\n",
    "\n",
    "Epoch 1: val_loss improved from 1.51186 to 1.50952, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 64s - loss: 1.2251 - masked_accuracy: 0.6211 - val_loss: 1.5095 - val_masked_accuracy: 0.5535 - 64s/epoch - 51ms/step\n",
    "Epoch 2/20\n",
    "\n",
    "Epoch 2: val_loss did not improve from 1.50952\n",
    "1263/1263 - 63s - loss: 1.2062 - masked_accuracy: 0.6273 - val_loss: 1.5155 - val_masked_accuracy: 0.5528 - 63s/epoch - 50ms/step\n",
    "Epoch 3/20\n",
    "\n",
    "Epoch 3: val_loss improved from 1.50952 to 1.50760, saving model to model_weights_checkpoint_128_128.h5\n",
    "1263/1263 - 62s - loss: 1.1890 - masked_accuracy: 0.6328 - val_loss: 1.5076 - val_masked_accuracy: 0.5567 - 62s/epoch - 49ms/step\n",
    "Epoch 4/20\n",
    "\n",
    "Epoch 4: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.1721 - masked_accuracy: 0.6376 - val_loss: 1.5102 - val_masked_accuracy: 0.5574 - 62s/epoch - 49ms/step\n",
    "Epoch 5/20\n",
    "\n",
    "Epoch 5: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.1571 - masked_accuracy: 0.6425 - val_loss: 1.5159 - val_masked_accuracy: 0.5569 - 62s/epoch - 49ms/step\n",
    "Epoch 6/20\n",
    "\n",
    "Epoch 6: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.1408 - masked_accuracy: 0.6472 - val_loss: 1.5198 - val_masked_accuracy: 0.5572 - 62s/epoch - 49ms/step\n",
    "Epoch 7/20\n",
    "\n",
    "Epoch 7: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.1262 - masked_accuracy: 0.6521 - val_loss: 1.5226 - val_masked_accuracy: 0.5587 - 62s/epoch - 49ms/step\n",
    "Epoch 8/20\n",
    "\n",
    "Epoch 8: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.1131 - masked_accuracy: 0.6558 - val_loss: 1.5269 - val_masked_accuracy: 0.5590 - 62s/epoch - 49ms/step\n",
    "Epoch 9/20\n",
    "\n",
    "Epoch 9: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.0978 - masked_accuracy: 0.6608 - val_loss: 1.5258 - val_masked_accuracy: 0.5607 - 62s/epoch - 49ms/step\n",
    "Epoch 10/20\n",
    "\n",
    "Epoch 10: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.0859 - masked_accuracy: 0.6638 - val_loss: 1.5242 - val_masked_accuracy: 0.5618 - 62s/epoch - 49ms/step\n",
    "Epoch 11/20\n",
    "\n",
    "Epoch 11: val_loss did not improve from 1.50760\n",
    "1263/1263 - 61s - loss: 1.0745 - masked_accuracy: 0.6676 - val_loss: 1.5398 - val_masked_accuracy: 0.5594 - 61s/epoch - 48ms/step\n",
    "Epoch 12/20\n",
    "\n",
    "Epoch 12: val_loss did not improve from 1.50760\n",
    "1263/1263 - 60s - loss: 1.0619 - masked_accuracy: 0.6717 - val_loss: 1.5361 - val_masked_accuracy: 0.5614 - 60s/epoch - 47ms/step\n",
    "Epoch 13/20\n",
    "\n",
    "Epoch 13: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.0496 - masked_accuracy: 0.6761 - val_loss: 1.5410 - val_masked_accuracy: 0.5623 - 62s/epoch - 49ms/step\n",
    "Epoch 14/20\n",
    "\n",
    "Epoch 14: val_loss did not improve from 1.50760\n",
    "1263/1263 - 59s - loss: 1.0391 - masked_accuracy: 0.6787 - val_loss: 1.5386 - val_masked_accuracy: 0.5626 - 59s/epoch - 47ms/step\n",
    "Epoch 15/20\n",
    "\n",
    "Epoch 15: val_loss did not improve from 1.50760\n",
    "1263/1263 - 61s - loss: 1.0295 - masked_accuracy: 0.6817 - val_loss: 1.5450 - val_masked_accuracy: 0.5620 - 61s/epoch - 48ms/step\n",
    "Epoch 16/20\n",
    "\n",
    "Epoch 16: val_loss did not improve from 1.50760\n",
    "1263/1263 - 59s - loss: 1.0198 - masked_accuracy: 0.6849 - val_loss: 1.5536 - val_masked_accuracy: 0.5637 - 59s/epoch - 47ms/step\n",
    "Epoch 17/20\n",
    "\n",
    "Epoch 17: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 1.0100 - masked_accuracy: 0.6871 - val_loss: 1.5522 - val_masked_accuracy: 0.5642 - 62s/epoch - 49ms/step\n",
    "Epoch 18/20\n",
    "\n",
    "Epoch 18: val_loss did not improve from 1.50760\n",
    "1263/1263 - 63s - loss: 1.0012 - masked_accuracy: 0.6905 - val_loss: 1.5506 - val_masked_accuracy: 0.5655 - 63s/epoch - 50ms/step\n",
    "Epoch 19/20\n",
    "\n",
    "Epoch 19: val_loss did not improve from 1.50760\n",
    "1263/1263 - 59s - loss: 0.9902 - masked_accuracy: 0.6934 - val_loss: 1.5606 - val_masked_accuracy: 0.5644 - 59s/epoch - 47ms/step\n",
    "Epoch 20/20\n",
    "\n",
    "Epoch 20: val_loss did not improve from 1.50760\n",
    "1263/1263 - 62s - loss: 0.9826 - masked_accuracy: 0.6959 - val_loss: 1.5640 - val_masked_accuracy: 0.5640 - 62s/epoch - 49ms/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b6bb1a77-a4ca-4780-8494-1589a4c199e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual inference on test input\n",
    "test_inp = np.zeros((1, FRAME_LEN, lm_shape))\n",
    "test_ctx = np.array([[start_token_idx]])\n",
    "logits = seq2seq_model((test_inp, test_ctx))\n",
    "pred_idx = np.argmax(tf.nn.softmax(logits)[0][0])\n",
    "num_to_char[pred_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae568f75-c578-4e32-a203-9bd9bc31089d",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9830e3a4-b7c0-490a-847c-bd188364efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_chars = tf.constant(list(char_to_num.keys()), dtype=tf.string)\n",
    "tf_nums = tf.constant(list(char_to_num.values()), dtype=tf.int64)\n",
    "\n",
    "class Seq2SeqModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        self.tf_char_to_num = tf.lookup.StaticHashTable(\n",
    "            initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=tf_chars,\n",
    "                values=tf_nums,\n",
    "            ),\n",
    "            default_value=tf.constant(-1, dtype=tf.int64),\n",
    "            name=\"tf_char_to_num_lut\"\n",
    "        )\n",
    "        \n",
    "        self.tf_num_to_char = tf.lookup.StaticHashTable(\n",
    "            initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=tf_nums,\n",
    "                values=tf_chars,\n",
    "            ),\n",
    "            default_value=tf.constant(\"unknown\", dtype=tf.string),\n",
    "            name=\"tf_num_to_char_lut\"\n",
    "        )\n",
    "\n",
    "        self.ctx_len = tf.constant(max_future_input_size, dtype=tf.int32)\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[tf.constant(FRAME_LEN), tf.constant(lm_shape)], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.string), # The input string is trimmed to max size set when initializing model\n",
    "    ])\n",
    "    def predict(self, landmarks, ctx):\n",
    "        # Prepare input\n",
    "        ctx_length = tf.strings.length(ctx)\n",
    "        starting_pos = tf.maximum(ctx_length - self.ctx_len, 0)\n",
    "        ctx = tf.strings.substr(ctx, starting_pos, self.ctx_len)\n",
    "        \n",
    "        ctx_chars = tf.strings.unicode_split(ctx, input_encoding='UTF-8')\n",
    "        ctx_tokens = self.tf_char_to_num.lookup(ctx_chars)\n",
    "        batched_ctx_tokens = tf.expand_dims(ctx_tokens, axis=0) # Adds first (\"batch\") dimension to the tensor\n",
    "        batched_landmarks = tf.expand_dims(landmarks, axis=0)\n",
    "        \n",
    "        # Inference\n",
    "        logits = self.model((batched_landmarks, batched_ctx_tokens), training=False)\n",
    "        logits = logits[:, -1:, :][0][0] # Select the last element in the middle dimension (the first is the batch dim, the last is the num_of_classes dim)\n",
    "        \n",
    "        # Parse result\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        pred_prob = tf.reduce_max(probabilities)\n",
    "        pred_idx = tf.argmax(probabilities)\n",
    "        pred_char = self.tf_num_to_char.lookup(pred_idx)\n",
    "        \n",
    "        return {'result' : pred_char, 'confidence': pred_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8d270af3-e942-4bce-9103-63ea511aa3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = Seq2SeqModel(seq2seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "429e7979-2e1c-4da6-9785-c6bccc32dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 219 ms\n",
      "Wall time: 57 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result': <tf.Tensor: shape=(), dtype=string, numpy=b'd'>,\n",
       " 'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.024407035>}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_wrapper.predict(np.zeros((FRAME_LEN, lm_shape)), \"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46554d-b85a-427b-b8fc-6d8dfe97c4b0",
   "metadata": {},
   "source": [
    "Note: first time execution is usually slower than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6d675d6f-b90d-4c1e-9390-0ff87bf4bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.attention.additive_attention.AdditiveAttention object at 0x00000257583DFE20>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.attention.additive_attention.AdditiveAttention object at 0x00000257583DFE20>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "save_model_name = \"saved_model\"\n",
    "if os.path.isdir(save_model_name):\n",
    "    print(f\"A model with the same name has already been saved!\")\n",
    "else:\n",
    "    tf.saved_model.save(model_wrapper, export_dir=save_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5134bee-834e-4c51-9c0e-6171544801e7",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19523338-3c1c-42bc-aebf-a2ccdb6f7587",
   "metadata": {},
   "source": [
    "## From saved weights\n",
    "\n",
    "1. Create model object with same parameters\n",
    "2. Build the model (run inference on it, to initialize the parameters)\n",
    "3. Load the weights\n",
    "4. Convert it to tf.Module or run manual inference on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "734e396a-eb2b-40ae-9499-366f0238fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dd880874-8945-43e7-9784-b8a614af2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 108)\n",
      "(32, 33)\n",
      "(32, 33, 62)\n",
      "Model: \"seq2_seq_recurrent_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_3 (Land  multiple                  1746688   \n",
      " markEmbedding)                                                  \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " positional_token_embedding  multiple                  15872     \n",
      " _27 (PositionalTokenEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " gru_26 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_27 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_28 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_29 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " additive_attention_13 (Add  multiple                  0 (unused)\n",
      " itiveAttention)                                                 \n",
      "                                                                 \n",
      " dense_27 (Dense)            multiple                  15934     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3357502 (12.81 MB)\n",
      "Trainable params: 3357502 (12.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example inference to build the model\n",
    "(lm, ctx), _label = next(iter(train_ds))\n",
    "output = new_model((lm, ctx))\n",
    "\n",
    "print(lm.shape)\n",
    "print(ctx.shape)\n",
    "print(output.shape)\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a41aa238-925b-48c6-bc42-b4a213ed79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights(\"model_weights_checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fdfde474-15c5-4d1b-949c-a432aeb15c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual inference on test input\n",
    "test_inp = np.zeros((1, FRAME_LEN, lm_shape))\n",
    "test_ctx = np.array([[start_token_idx]])\n",
    "logits = new_model((test_inp, test_ctx), training=False)\n",
    "logits = logits[:, -1:, :][0][0]\n",
    "pred_idx = np.argmax(tf.nn.softmax(logits))\n",
    "num_to_char[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "56c067e6-32fd-437a-b659-fa69ff208da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = FingerSpellingTransformer(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d174c620-c2dc-493a-9e2b-66fd05cc4b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': <tf.Tensor: shape=(), dtype=string, numpy=b'm'>,\n",
       " 'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.09073451>}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(np.zeros((FRAME_LEN, lm_shape)), \"<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd57bf-2295-4854-a91f-dcdf00747d5f",
   "metadata": {},
   "source": [
    "## From saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9eca4e3f-2ee8-44a1-bcd7-9551ab17caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b6b88b3d-b223-41ff-adac-70083196180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': <tf.Tensor: shape=(), dtype=string, numpy=b'_'>,\n",
       " 'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.02006547>}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(np.zeros((FRAME_LEN, lm_shape)), \"<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc4af1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## On Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bde7f632-d38d-43d4-821e-8e4a5c9ff9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(fs_model, inp, max_len):\n",
    "    ctx = str(num_to_char[start_token_idx])\n",
    "    for i in range(max_len):\n",
    "        res = fs_model.predict(inp, ctx)\n",
    "        res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        ctx += res_char\n",
    "\n",
    "        if res_char == num_to_char[end_token_idx]:\n",
    "            break\n",
    "    return ctx\n",
    "\n",
    "def generate_teacher_forcing(fs_model, inp, expected):\n",
    "    pred = str(num_to_char[start_token_idx])\n",
    "    ctx = str(num_to_char[start_token_idx])\n",
    "    for e in expected:\n",
    "        if e == 'P':\n",
    "            break\n",
    "        res = fs_model.predict(inp, ctx)\n",
    "        res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        pred += res_char\n",
    "        ctx += e\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "61f60471-b3c1-43f9-9378-e76071361a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: golfcourseweddings>PPPPPPPPPPPPP\n",
      "Gen on own: <+61-05-206-18>\n",
      "Gen teacher forcing: <+obf-rtro wodey>g..\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 9396 yuchi lane>PPPPPPPPPPPPPPPP\n",
      "Gen on own: <93396 yuchi lane>\n",
      "Gen teacher forcing: <9336 yuchi lane>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 506-993-4525>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <506-993-4525>\n",
      "Gen teacher forcing: <506-993-4525>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 428823 winter house>PPPPPPPPPPPP\n",
      "Gen on own: <428823 winter horse>\n",
      "Gen teacher forcing: <428823 winter horse>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: america_2>PPPPPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <anmrica.12>\n",
      "Gen teacher forcing: <anrrica.1>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 628-137-7392>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <628-137-7392>\n",
      "Gen teacher forcing: <628-137-7392>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 370 ivy spring>PPPPPPPPPPPPPPPPP\n",
      "Gen on own: <370 vys pring>\n",
      "Gen teacher forcing: <370 vvy spring>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 889-022-9065>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <889-022-9065>\n",
      "Gen teacher forcing: <889-022-9065>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: ronnie simon>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <randie orraie>\n",
      "Gen teacher forcing: <rarnee>oteent\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: oscar lowery>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <scarlowerry>\n",
      "Gen teacher forcing: <sscarllowerr>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 98 delanna felder>PPPPPPPPPPPPPP\n",
      "Gen on own: <98 deland felde r>\n",
      "Gen teacher forcing: <98 delandaffelde >\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 555 sam hill mine ct>PPPPPPPPPPP\n",
      "Gen on own: <55 samhill mine ct>\n",
      "Gen teacher forcing: <55  samhhill mine ct>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 6559 circle west>PPPPPPPPPPPPPPP\n",
      "Gen on own: <6599 circlwest>\n",
      "Gen teacher forcing: <65999circlwwwest>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: yarzarmg.blogspot.com/ecobonus>P\n",
      "Gen on own: <rzartalana.blogspot.com.ar>\n",
      "Gen teacher forcing: <rrrzanaabblogspot.com.ecobaue>>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 991508 alcoda>PPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <91508 aloda>\n",
      "Gen teacher forcing: <911508 alooda>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: mrjamesmay/>PPPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <mumri.mesmy/>\n",
      "Gen teacher forcing: <mumamesmyy/>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 769-110-5964>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <699-710-5964>\n",
      "Gen teacher forcing: <689-500-5964>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 864 maries valley lane>PPPPPPPPP\n",
      "Gen on own: <7645 maires dey ave>\n",
      "Gen teacher forcing: <7645maiies diraay aane>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: www.heimaslod.is>PPPPPPPPPPPPPPP\n",
      "Gen on own: <www.heimasleod.is>\n",
      "Gen teacher forcing: <www.heimasled.is>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 411-498-1536>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <410-498-1536>\n",
      "Gen teacher forcing: <410-498-1536>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 241-900-8965>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <+241-00-89-65>\n",
      "Gen teacher forcing: <+41-000-8965>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 816 shadow royal>PPPPPPPPPPPPPPP\n",
      "Gen on own: <816 roya>\n",
      "Gen teacher forcing: <816 rwolowsroy>>>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: +66-1473-055>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <+66-573-051473-05>\n",
      "Gen teacher forcing: <+66-5473-051>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: https://www.lwqshop.com>PPPPPPPP\n",
      "Gen on own: <https://www.lw.shop.com>\n",
      "Gen teacher forcing: <https://www.lw..hop.com>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 65 cbr lane>PPPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <65 br drive>\n",
      "Gen teacher forcing: <65 b r dene>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 630 daffodil ridge>PPPPPPPPPPPPP\n",
      "Gen on own: <6301 foodi prings>\n",
      "Gen teacher forcing: <6301difoo i irivge>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 3142 glasva school>PPPPPPPPPPPPP\n",
      "Gen on own: <3142 glas school>\n",
      "Gen teacher forcing: <3142 glas a school>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: www.nbkoyo.com/allenexcelldude>P\n",
      "Gen on own: <www.nexceldudenexxcem/alenexx>\n",
      "Gen teacher forcing: <www.nek/.u.com/aleenexxelloede>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 829 englewood lake>PPPPPPPPPPPPP\n",
      "Gen on own: <299 ennatl lane>\n",
      "Gen teacher forcing: <2229ennlanaod lane>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: dexter estes>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <eter ester>\n",
      "Gen teacher forcing: <eetter ester>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 3843 dunleavy place>PPPPPPPPPPPP\n",
      "Gen on own: <3843 dunle avillace>\n",
      "Gen teacher forcing: <3843 dunle vy place>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 83 private road 5334>PPPPPPPPPPP\n",
      "Gen on own: <83 prippro 2534>\n",
      "Gen teacher forcing: <83 pripatr road 5344>\n",
      "\n",
      "~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(inp_batch, _ctx), expected_batch = random.choice([batch for batch in valid_ds])\n",
    "for seq, expected in zip(inp_batch, expected_batch):\n",
    "    expected = \"\".join([num_to_char[num.numpy()] for num in expected])\n",
    "\n",
    "    print(\"Expected: \" + expected)\n",
    "    print(\"Gen on own: \" + generate(loaded_model, seq, MAX_PHRASE_LEN))\n",
    "    print(\"Gen teacher forcing: \" + generate_teacher_forcing(loaded_model, seq, expected))\n",
    "    print('\\n~~~\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c854fb4",
   "metadata": {},
   "source": [
    "## Real life testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c4cd-a892-42c2-945f-f56247be98b8",
   "metadata": {},
   "source": [
    "### Util for handling video feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e8053562-23ae-4813-878a-7d3be6bc57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_landmarks_on_image(image, results):\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f9a3dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_result(res):\n",
    "    # Extract specific pose landmarks if available\n",
    "    px = []\n",
    "    py = []\n",
    "    pz = []\n",
    "    if res.pose_landmarks:\n",
    "        for i in POSE:\n",
    "            lm = res.pose_landmarks.landmark[i]\n",
    "            px.append(lm.x)\n",
    "            py.append(lm.y)\n",
    "            pz.append(lm.z)\n",
    "    else:\n",
    "        px = [0.0]*len(POSE)\n",
    "        py = [0.0]*len(POSE)\n",
    "        pz = [0.0]*len(POSE)\n",
    "\n",
    "    # Extract left hand landmarks if available\n",
    "    lx = []\n",
    "    ly = []\n",
    "    lz = []\n",
    "    if res.left_hand_landmarks:\n",
    "        for lm in res.left_hand_landmarks.landmark:\n",
    "            lx.append(lm.x)\n",
    "            ly.append(lm.y)\n",
    "            lz.append(lm.z)\n",
    "    else:\n",
    "        lx = [0.0]*21\n",
    "        ly = [0.0]*21\n",
    "        lz = [0.0]*21\n",
    "\n",
    "    # Extract right hand landmarks if available\n",
    "    rx = []\n",
    "    ry = []\n",
    "    rz = []\n",
    "    if res.right_hand_landmarks:\n",
    "        for lm in res.right_hand_landmarks.landmark:\n",
    "            rx.append(lm.x)\n",
    "            ry.append(lm.y)\n",
    "            rz.append(lm.z)\n",
    "    else:\n",
    "        rx = [0.0]*21\n",
    "        ry = [0.0]*21\n",
    "        rz = [0.0]*21\n",
    "\n",
    "    return list(chain(rx, lx, px, ry, ly, py, rz, lz, pz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8a638eb3-c919-4c9c-9131-fdf6e4b1eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_loop(source, process_data_func):\n",
    "    video = cv2.VideoCapture(source)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    try:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "            while True:\n",
    "                _, frame = video.read()\n",
    "    \n",
    "                if frame is None:\n",
    "                    break\n",
    "    \n",
    "                #image = cv2.resize(frame, (360, 240))\n",
    "                image=frame\n",
    "    \n",
    "                # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "                image.flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                data = extract_from_result(results)\n",
    "\n",
    "                process_data_func(data)\n",
    "    \n",
    "                # Draw landmark annotation on the image.\n",
    "                image = draw_landmarks_on_image(image, results)\n",
    "    \n",
    "                image = cv2.flip(image, 1)\n",
    "                _, image = cv2.imencode('.jpeg', image)\n",
    "                display_handle.update(Image(data=image.tobytes()))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        video.release()\n",
    "        display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ea833-fa1d-4703-9711-6ab969e68ad4",
   "metadata": {},
   "source": [
    "### Util for handling models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52fc59-c54b-4051-a3eb-b4bc0ac10305",
   "metadata": {},
   "source": [
    "#### Signing detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7efb8a27-d26f-4097-8398-b2ce313aeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load once\n",
    "signing_detection_model = tf.saved_model.load(\"signing_detection_model\")\n",
    "\n",
    "class SigningDetectionModel:\n",
    "    def __init__(self):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "\n",
    "    def is_signing(self, inp_lm):\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        return signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy() == 1\n",
    "\n",
    "class BufferedSigningDetectionModel:\n",
    "    def __init__(self, buffer_len=5, confidence_number=3):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "        self.signing_detector_buffer = deque(maxlen=buffer_len)\n",
    "        self.confidence_number = confidence_number \n",
    "\n",
    "    def is_signing(self, inp_lm):\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        pred = signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy()\n",
    "        self.signing_detector_buffer.append(pred)\n",
    "        buffered_pred, count = Counter(self.signing_detector_buffer).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            return buffered_pred == 1\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284ff73-1c6c-472d-bdda-3e0b7917fa74",
   "metadata": {},
   "source": [
    "#### Fingerspelling recognition models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c69bcc5e-1560-4d3e-be01-d385b99f1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load once\n",
    "loaded_transformer_model = loaded_model\n",
    "\n",
    "class ContinuousRecognitionModel:\n",
    "    def __init__(self, confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7):\n",
    "        self.input = []\n",
    "        self.trust_confidence = confidence_threshold\n",
    "        self.inp_len = inp_buf_len\n",
    "        self.inner_fifo = deque(maxlen=out_buf_len)\n",
    "        self.confidence_number = out_majority_threshold\n",
    "        self.context = str(num_to_char[start_token_idx])\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        if len(self.input) >= self.inp_len:\n",
    "            self.input.pop(0)\n",
    "        self.input.append(frame)\n",
    "    \n",
    "        inp = pre_process(self.input)\n",
    "        res = loaded_transformer_model.predict(inp, self.context)\n",
    "        pred = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        prob = res[\"confidence\"].numpy()\n",
    "\n",
    "        if prob < self.trust_confidence:\n",
    "            return\n",
    "\n",
    "        self.inner_fifo.append(pred)\n",
    "        pred_char, count = Counter(self.inner_fifo).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            if self.context[-1] != pred_char:\n",
    "                self.context += pred_char\n",
    "                print(pred_char, end=\"\")\n",
    "                \n",
    "                # Predicted the end\n",
    "                if pred_char == '>':\n",
    "                    # restart the detection\n",
    "                    self.context = str(num_to_char[start_token_idx])\n",
    "                    self.inner_fifo.clear()\n",
    "                    self.input.clear()\n",
    "\n",
    "class NonContinuousRecognitionModel:\n",
    "    def __init__(self, max_out_length=MAX_PHRASE_LEN, max_input_length=FRAME_LEN, confidence_threshold=0.2):\n",
    "        self.max_out_length = max_out_length\n",
    "        self.max_input_length = max_input_length\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.input = []\n",
    "\n",
    "    def reset_buffer(self):\n",
    "        self.input.clear()\n",
    "\n",
    "    def translate_buffered_content(self, reset_buffer=False):\n",
    "        if len(self.input) > 0:\n",
    "            inp = pre_process(self.input)\n",
    "            print(self._generate_with_confidence(inp))\n",
    "        if reset_buffer:\n",
    "            self.reset_buffer()\n",
    "\n",
    "    def process_frame(self, inp_lm):\n",
    "        self.input.append(inp_lm)\n",
    "        \n",
    "        if len(self.input) >= FRAME_LEN:\n",
    "            self.translate_buffered_content()\n",
    "            self.reset_buffer()\n",
    "\n",
    "    def _generate_with_confidence(self, inp):\n",
    "        ctx = str(num_to_char[start_token_idx])\n",
    "        for i in range(self.max_out_length):\n",
    "            res = loaded_model.predict(inp, ctx)\n",
    "            res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "            prob = res[\"confidence\"].numpy()\n",
    "            if prob > self.confidence_threshold:\n",
    "                ctx += res_char\n",
    "                if res_char == num_to_char[end_token_idx]:\n",
    "                    break\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba5da1-cc4b-42b4-8fae-1d5705f0a7e8",
   "metadata": {},
   "source": [
    "### Running diffrent configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb7332-f214-4946-a80e-77f262439c3a",
   "metadata": {},
   "source": [
    "#### Continuous model without signing detection\n",
    "\n",
    "Works well for isolated sequences. Can't handle sudden pauses, and stops.\n",
    "Extremely sensitive to window size. Also, the training data was from professional signers. For beginners who sign slower the same window size isn't suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b6b6fc2b-4255-4945-9c2e-f2b4243f2087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beark"
     ]
    }
   ],
   "source": [
    "fs_model = ContinuousRecognitionModel(confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7)\n",
    "video_loop(\"test_videos/bear.mp4\", lambda data: fs_model.process_frame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df92a3-b760-46c5-8a9d-e648ea4cda71",
   "metadata": {},
   "source": [
    "#### Continuous model with signing detection\n",
    "\n",
    "Handles breaks at the start and end, but doesn't account for breaks mid-signing, or multiple words, as the model's buffer is only filled when signing is detected. This can lead to jumps in the buffer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "80ca41cf-da3e-4fb4-9b3d-e418056fe553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------*******************************--------*********a*******l*******************************i*******g*******a*******t*******o*******r*******>--------------*****"
     ]
    }
   ],
   "source": [
    "sign_detector = SigningDetectionModel()\n",
    "#sign_detector = BufferedSigningDetectionModel()\n",
    "fs_model = ContinuousRecognitionModel(confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7)\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        print(\"*\", end=\"\")\n",
    "        fs_model.process_frame(data)\n",
    "    else:\n",
    "        print(\"-\", end=\"\")\n",
    "\n",
    "video_loop(\"test_videos/alligator.mp4\", process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1368de0-d61c-41d7-89c2-a607b4599cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO model that takes input frames all the time but only predicts when signing is detected!!!!!\n",
    "# Continue the train of thought for the rest!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4fe0e6-c163-461a-9b79-076a9f8fec7f",
   "metadata": {},
   "source": [
    "#### Translate in long chunks\n",
    "\n",
    "This model performs well on single words that fit into the buffer. But for longer text it fails to translate well, presumable because the signs are cut off at the wrong positions. For longer text pause detection/signing detection is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "95ace07c-a3fc-49fb-85f9-8397fc8df6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<beark>\n"
     ]
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(max_out_length=MAX_PHRASE_LEN, max_input_length=FRAME_LEN, confidence_threshold=0.2)\n",
    "video_loop(\"test_videos/bear.mp4\", lambda data: fs_model.process_frame(data))\n",
    "fs_model.translate_buffered_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5602c-a076-4d9e-92c2-2559310564b1",
   "metadata": {},
   "source": [
    "#### Translate in long chunks  with signing detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "55175fc3-7a78-44f6-be9c-8da04278431e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<monkey >\n"
     ]
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(max_out_length=MAX_PHRASE_LEN, max_input_length=FRAME_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "#sign_detector = BufferedSigningDetectionModel()\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "\n",
    "video_loop(\"test_videos/monkey.mp4\", process_data)\n",
    "fs_model.translate_buffered_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb94e63-4e15-4e2c-9fd1-41e536e9d823",
   "metadata": {},
   "source": [
    "#### Translate longer sequences with stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "4aa9fd48-102b-4bb4-8d0b-4d20f81be733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<earkn>\n",
      "<\n",
      "<tigerr>\n",
      "<\n",
      "<ebraa>\n",
      "<\n",
      "<yena>\n",
      "<\n",
      "<angaroo hangaroo>\n",
      "<monkey >\n",
      "<\n",
      "<lyonn parks>\n",
      "<s\n",
      "<alligator>\n",
      "<horse horse>\n",
      "<\n"
     ]
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(max_out_length=MAX_PHRASE_LEN, max_input_length=FRAME_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "#sign_detector = BufferedSigningDetectionModel()\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "    else:\n",
    "        fs_model.translate_buffered_content(reset_buffer=True)\n",
    "\n",
    "video_loop(\"test_videos/fingerspelling_animals.mp4\", process_data)\n",
    "fs_model.translate_buffered_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29899b-a167-496c-94dd-2e1dde5d6626",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9976f-3618-4351-a12f-935d767dd921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ab6f2a40-51a1-4248-92a1-ad6e1795b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5abe44cf-87bc-4759-907a-76ed62da70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "35962a56-944c-415f-aaa7-54cac9182a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Please set a valid api key!\n"
     ]
    }
   ],
   "source": [
    "key = os.environ.get('OPEN_AI_API_KEY')\n",
    "if key is not None:\n",
    "    openai.api_key = key\n",
    "else:\n",
    "    print(\"Error: Please set a valid api key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4d1aa220-b829-4e0d-a05b-6929d0877a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_output(pred):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\", \n",
    "       messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a machine that tries to correct the output of a fingerspelling recognition model. Some letters might be missing, but it's also possible that the given text has extra characters. Only reply the corrected text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"angaro angaro\"},\n",
    "        {\"role\": \"system\", \"content\": \"kangaroo\"},\n",
    "        {\"role\": \"user\", \"content\": \"beark\"},\n",
    "        {\"role\": \"system\", \"content\": \"bear\"},\n",
    "        {\"role\": \"user\", \"content\": \"6 halee hale\"},\n",
    "        {\"role\": \"system\", \"content\": \"whale\"},\n",
    "        {\"role\": \"user\", \"content\": pred},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a5becc88-a2f8-4cbf-9926-868725581db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earth/tiger/angary ligator alligator horse gro'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_output(\"earkh/tiger/tiger angar key ligator alligator h horse gro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a5f3b-7033-434f-8197-098396375c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493c35f-5609-4088-8039-49bcb7438ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b883a33-fae7-4e1a-901d-9402ca93db7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
