{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670c23e9",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3357796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install tensorflow\n",
    "!pip install protobuf==3.20.*\n",
    "!pip install mediapipe==0.9.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afa879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pyarrow.parquet as pq\n",
    "from tensorflow.keras import layers\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815f7292-d5cd-4eea-aef2-81a594138520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n",
      "TensorFlow v2.14.0\n",
      "Mediapipe v0.9.0.1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336326cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "cv2.setRNGSeed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dff822-75bc-493e-b175-14481c2abc76",
   "metadata": {},
   "source": [
    "# Fetch from TfRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4d1354-adc7-4073-a64b-3c72761e2e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 68 TFRecord files.\n"
     ]
    }
   ],
   "source": [
    "PATH_KAGGLE_DS = \"kaggle_dataset\"\n",
    "dataset_df = pd.read_csv(os.path.join(PATH_KAGGLE_DS, \"train.csv\"))\n",
    "PATH_TFRECORD_DS = os.path.join(PATH_KAGGLE_DS, \"train_tfrecords\")\n",
    "tf_records = dataset_df.file_id.map(lambda x: os.path.join(PATH_TFRECORD_DS, f\"{x}.tfrecord\")).unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f496051-1067-479f-8b5f-3aedbd4d3303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_right_hand_0',\n",
       " 'x_right_hand_1',\n",
       " 'x_right_hand_2',\n",
       " 'x_right_hand_3',\n",
       " 'x_right_hand_4',\n",
       " 'x_right_hand_5',\n",
       " 'x_right_hand_6',\n",
       " 'x_right_hand_7',\n",
       " 'x_right_hand_8',\n",
       " 'x_right_hand_9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(PATH_TFRECORD_DS, \"feature_columns.json\"), 'r') as f:\n",
    "    json_str = f.read()\n",
    "FEATURE_COLUMNS = json.loads(json_str)\n",
    "FEATURE_COLUMNS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a432491-d637-4216-812b-7d8da9366295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These points represent the hands, elbows, and shoulders.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "\n",
    "# Facial information isn't necessary, but the nose will serve as a midpoint for normalizing the data, as it is usually located in the middle of the frame.\n",
    "FPOSE = [0] # Nose as midpoint\n",
    "\n",
    "# Collecting the indices of certain important/distinct sets of features.\n",
    "# This can be beneficial during the preprocessing step.\n",
    "RHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\n",
    "LHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\n",
    "RPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) in RPOSE]\n",
    "LPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) in LPOSE]\n",
    "MID_POINT_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col.split(\"_\")[-1]) == 0] # Nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5699c6a-6b6e-4643-8a54-62183c475823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COLUMNS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n",
    "    # Transpose to maintain the original shape of landmarks data.\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "    \n",
    "    return landmarks, phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3895dc76-d15a-49a7-888a-31c5f92ec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default mapping that came with the dataset was changed:\n",
    "# padding is represented with the number 0\n",
    "# start_token is 60\n",
    "# end_token is 61\n",
    "with open (os.path.join(PATH_KAGGLE_DS, \"character_to_prediction_index.json\"), \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "    \n",
    "char_to_num = {c:char_to_num[c]+1 for c in char_to_num}\n",
    "\n",
    "# Add pad_token, start pointer and end pointer to the dict\n",
    "pad_token = 'P'\n",
    "pad_token_idx = 0\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "\n",
    "start_token = '<'\n",
    "start_token_idx = 60\n",
    "char_to_num[start_token] = start_token_idx\n",
    "\n",
    "end_token = '>'\n",
    "end_token_idx = 61\n",
    "char_to_num[end_token] = end_token_idx\n",
    "\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891424b9-95aa-478b-877b-5c99da8a8b26",
   "metadata": {},
   "source": [
    "## Preprocess phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f559ea97-8029-4754-af54-939aa7be587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(char_to_num.keys()),\n",
    "        values=list(char_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"tf_char_to_num\"\n",
    ")\n",
    "\n",
    "# Function to decode the characters and pad the phrases\n",
    "MAX_PHRASE_LEN = 31 + 2 # The start and end token take space as well\n",
    "def preprocess_phrase(phrase):\n",
    "    phrase = start_token + phrase + end_token\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "    \n",
    "    max_len_plus = MAX_PHRASE_LEN + 1\n",
    "    amount_to_pad = max_len_plus - tf.shape(phrase)[0]\n",
    "    \n",
    "    if amount_to_pad > 0:\n",
    "        phrase = tf.pad(phrase, paddings=[[0, amount_to_pad]], mode = 'CONSTANT', constant_values = pad_token_idx)\n",
    "    else:\n",
    "        phrase = phrase[:max_len_plus]\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf3bba-a24b-453b-b47e-396045f03925",
   "metadata": {},
   "source": [
    "## Preprocess landmark A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a815e8c9-6f0d-4e8e-8500-aaacac83584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LEN = 256\n",
    "def preprocess_landmark_a(x):\n",
    "    # Select distinct groups\n",
    "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
    "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
    "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
    "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
    "    \n",
    "    # Calculate how many unknown values there are for each hand\n",
    "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
    "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
    "    rnans = tf.math.count_nonzero(rnan_idx)\n",
    "    lnans = tf.math.count_nonzero(lnan_idx)\n",
    "    \n",
    "    # As fingerspelling gestures only use one hand, the model will only receive the dominant one. (=less NaN values)\n",
    "    # Also from the pose information, only the dominant side is needed.\n",
    "    is_right_hand = True\n",
    "    hand = rhand\n",
    "    pose = rpose\n",
    "    num_of_hand_lm = len(RHAND_IDX)\n",
    "    num_of_pose_lm = len(RPOSE_IDX)\n",
    "    if rnans > lnans:\n",
    "        is_right_hand = False\n",
    "        hand = lhand\n",
    "        pose = lpose\n",
    "        num_of_hand_lm = len(LHAND_IDX)\n",
    "        num_of_pose_lm = len(LPOSE_IDX)\n",
    "    \n",
    "    # Gather channels\n",
    "    hand_x = hand[:, 0*(num_of_hand_lm//3) : 1*(num_of_hand_lm//3)]\n",
    "    hand_y = hand[:, 1*(num_of_hand_lm//3) : 2*(num_of_hand_lm//3)]\n",
    "    hand_z = hand[:, 2*(num_of_hand_lm//3) : 3*(num_of_hand_lm//3)]\n",
    "    \n",
    "    # Flip the x axis if it's the left hand. (This makes it resemble the right hand more.)\n",
    "    if not is_right_hand:\n",
    "        hand_x = 1 - hand_x\n",
    "    \n",
    "    # Join along a new axis\n",
    "    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1) # (SEQ_LEN, LM_COUNT, 3)\n",
    "    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :] # Mean and std along the first axis (across LM_COUNT)\n",
    "    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n",
    "    hand = (hand - mean) / std\n",
    "    \n",
    "    # The same for pose\n",
    "    pose_x = pose[:, 0*(num_of_pose_lm//3) : 1*(num_of_pose_lm//3)]\n",
    "    pose_y = pose[:, 1*(num_of_pose_lm//3) : 2*(num_of_pose_lm//3)]\n",
    "    pose_z = pose[:, 2*(num_of_pose_lm//3) : 3*(num_of_pose_lm//3)]\n",
    "    \n",
    "    if not is_right_hand:\n",
    "        pose_x = 1 - pose_x\n",
    "    \n",
    "    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n",
    "    mean = tf.math.reduce_mean(pose, axis=1)[:, tf.newaxis, :]\n",
    "    std = tf.math.reduce_std(pose, axis=1)[:, tf.newaxis, :]\n",
    "    pose = (pose - mean) / std\n",
    "    \n",
    "    # Join the data\n",
    "    x = tf.concat([hand, pose], axis=1)\n",
    "    \n",
    "    # Replace NaN with 0.\n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "    \n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    \n",
    "    # x.shape == (FRAME_LEN, FEATURE_COUNT, 3)\n",
    "    \n",
    "    # Reshape to (FRAME_LEN, FEATURE_COUNT*3 == num_of_hand_lm + num_of_pose_lm)\n",
    "    x = tf.reshape(x, (FRAME_LEN, num_of_hand_lm + num_of_pose_lm))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3715de-25ce-4014-b0f5-8ecd02dfd538",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocess landmark B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f64ec3-d8fa-4f7d-aabf-c74c93acebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LEN = 256\n",
    "\n",
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "   \n",
    "def preprocess_landmark_b(x):\n",
    "    # Select distinct groups\n",
    "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
    "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
    "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
    "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
    "    nose = tf.gather(x, MID_POINT_IDX, axis=1)\n",
    "    \n",
    "    # Calculate how many unknown values there are for each hand\n",
    "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
    "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
    "    rnans = tf.math.count_nonzero(rnan_idx)\n",
    "    lnans = tf.math.count_nonzero(lnan_idx)\n",
    "    \n",
    "    # As fingerspelling gestures only use one hand, the model will only receive the dominant one. (=less NaN values)\n",
    "    # Also from the pose information, only the dominant side is needed.\n",
    "    is_right_hand = True\n",
    "    hand = rhand\n",
    "    pose = rpose\n",
    "    num_of_hand_lm = len(RHAND_IDX)\n",
    "    num_of_pose_lm = len(RPOSE_IDX)\n",
    "    if rnans > lnans:\n",
    "        is_right_hand = False\n",
    "        hand = lhand\n",
    "        pose = lpose\n",
    "        num_of_hand_lm = len(LHAND_IDX)\n",
    "        num_of_pose_lm = len(LPOSE_IDX)\n",
    "    \n",
    "    # Gather channels\n",
    "    hand_x = hand[:, 0*(num_of_hand_lm//3) : 1*(num_of_hand_lm//3)]\n",
    "    hand_y = hand[:, 1*(num_of_hand_lm//3) : 2*(num_of_hand_lm//3)]\n",
    "    hand_z = hand[:, 2*(num_of_hand_lm//3) : 3*(num_of_hand_lm//3)]\n",
    "    pose_x = pose[:, 0*(num_of_pose_lm//3) : 1*(num_of_pose_lm//3)]\n",
    "    pose_y = pose[:, 1*(num_of_pose_lm//3) : 2*(num_of_pose_lm//3)]\n",
    "    pose_z = pose[:, 2*(num_of_pose_lm//3) : 3*(num_of_pose_lm//3)]\n",
    "    nose_x = nose[:, 0 : 1]\n",
    "    nose_y = nose[:, 1 : 2]\n",
    "    nose_z = nose[:, 2 : 3]\n",
    "    # Combine all x, y, z coordinates\n",
    "    combined_x = tf.concat([nose_x, hand_x, pose_x], axis=-1)\n",
    "    combined_y = tf.concat([nose_y, hand_y, pose_y], axis=-1)\n",
    "    combined_z = tf.concat([nose_z, hand_z, pose_z], axis=-1)\n",
    "    \n",
    "    # Flip the x axis if it's the left hand. (This makes it resemble the right hand more.)\n",
    "    if not is_right_hand:\n",
    "        hand_x = 1 - hand_x\n",
    "    # Join along a new axis\n",
    "    x = tf.concat([combined_x[..., tf.newaxis], combined_y[..., tf.newaxis], combined_z[..., tf.newaxis]], axis=-1) # (SEQ_LEN, LM_COUNT, 3)\n",
    "    # x.shape == (T, P, C)\n",
    "    # N: Batch size -> 1 for now\n",
    "    # T: Sequence length\n",
    "    # P: Number of points (or landmarks)\n",
    "    # C: Number of channels (features) per point\n",
    "    \n",
    "    x = x[None,...] # x.shape == (1, T, P, C)\n",
    "    \n",
    "    # Mean of midpoints (nose) through timesteps\n",
    "    mean = tf_nan_mean(tf.gather(x, [0], axis=2), axis=[1,2], keepdims=True)\n",
    "    mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
    "    # x.shape == (N, 1, 1, C)\n",
    "    \n",
    "    std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "    # std.shape == (N, 1, 1, C)\n",
    "    \n",
    "    x = (x - mean)/std\n",
    "    # x.shape == (N, T, P, C)\n",
    "\n",
    "    # Limit seq_len\n",
    "    x = x[:,:FRAME_LEN]\n",
    "    seq_length = tf.shape(x)[1]\n",
    "    x = x[...,:2]\n",
    "    # x.shape == (N, T, P, 2)\n",
    "\n",
    "    # Calc differences between consequtive frames and with skipping 1 in between\n",
    "    dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "    # dx.shape == (N, T, P, 2)\n",
    "    \n",
    "    x = tf.concat([\n",
    "        tf.reshape(x, (-1,seq_length,2*tf.shape(x)[2])),\n",
    "        tf.reshape(dx, (-1,seq_length,2*tf.shape(dx)[2])),\n",
    "    ], axis = -1)\n",
    "    # x.shape == (N, T, features) where T <= FRAME_LEN\n",
    "    \n",
    "    # Replace NaN values with zeros\n",
    "    x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
    "\n",
    "    x = x[0] # x.shape == (T, features)\n",
    "    \n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, [[0, FRAME_LEN-tf.shape(x)[0]], [0, 0]])\n",
    "\n",
    "    # x.shape == (FRAME_LEN, features)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d31ae-9fce-4344-a577-c87dcc0004fe",
   "metadata": {},
   "source": [
    "## Combined preprocessing for lm and phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "2b7f6c7c-e242-499a-80f6-b1d70135f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(landmark, phrase):\n",
    "    phrase = preprocess_phrase(phrase)\n",
    "    return (preprocess_landmark_a(landmark), phrase[:-1]), phrase[1:] # Shifted phrase for encoder-decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e383e-6495-476f-bdc4-e8cb07f8a0d5",
   "metadata": {},
   "source": [
    "## Create TFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "4e588aba-02b0-4dce-872b-890743689447",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_len = int(0.8 * len(tf_records))\n",
    "\n",
    "def get_dataset(tfrecords, batch_size=1, repeat=False, shuffle=False, drop_remainder=False, cache=False):\n",
    "    ds = tf.data.TFRecordDataset(tf_records)\n",
    "    ds = ds.map(decode_fn, tf.data.AUTOTUNE)\n",
    "    # Note: preprocessing can happen before and after the batching (if you can preprocess the whole batch at once to save computation time)\n",
    "    ds = ds.map(preprocess, tf.data.AUTOTUNE)\n",
    "    \n",
    "    if repeat: \n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_deterministic = (False)\n",
    "        ds = ds.with_options(options)\n",
    "\n",
    "    if batch_size > 1:\n",
    "        # There's also a padded_batch version of this function\n",
    "        ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "        \n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # If the system doesn't have enough RAM caching might slow down the process\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    \n",
    "    return ds\n",
    "\n",
    "train_ds = get_dataset(tf_records[:train_len], batch_size=batch_size, cache=True)\n",
    "valid_ds = get_dataset(tf_records[train_len:], batch_size=batch_size, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5523cc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "\n",
      "Saved shapes:\n",
      "lm_shape: 78\n",
      "phrase_shape: 32\n",
      "----------------------------------------\n",
      "\n",
      "Encoder input - first in batch (Landmarks:)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(32, 256, 78)\n",
      "tf.Tensor(\n",
      "[[-0.27655613  1.5076838   2.3224409  ...  0.86484635 -0.640369\n",
      "  -0.31775153]\n",
      " [ 0.          0.          0.         ...  0.9932173  -0.4573373\n",
      "  -0.3150256 ]\n",
      " [ 0.          0.          0.         ...  0.8487494  -0.6870108\n",
      "  -0.31365472]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]], shape=(256, 78), dtype=float32)\n",
      "----------------------------------------\n",
      "\n",
      "Decoder input (Context):\n",
      "(32, 33)\n",
      "tf.Tensor(\n",
      "[60 19  1 35 50 37 37 43 40 47 53 51 37 61  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n",
      "\n",
      "Model target output (Phrase):\n",
      "(32, 33)\n",
      "tf.Tensor(\n",
      "[19  1 35 50 37 37 43 40 47 53 51 37 61  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0], shape=(33,), dtype=int32)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lm_shape = None\n",
    "phrase_shape = None\n",
    "\n",
    "# Create an iterator for the train and valid datasets\n",
    "train_iterator = iter(train_ds)\n",
    "\n",
    "# Print data points from the training dataset\n",
    "print(\"Training Data:\\n\")\n",
    "(landmarks, context), phrase = next(train_iterator)\n",
    "\n",
    "# Save shapes\n",
    "lm_shape = landmarks.shape[2]\n",
    "phrase_shape = phrase.shape[0]\n",
    "print(\"Saved shapes:\")\n",
    "print(f\"lm_shape: {lm_shape}\")\n",
    "print(f\"phrase_shape: {phrase_shape}\")\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Encoder input - first in batch (Landmarks:)\")\n",
    "print(type(landmarks))\n",
    "print(landmarks.shape)\n",
    "print(landmarks[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Decoder input (Context):\")\n",
    "print(context.shape)\n",
    "print(context[0])\n",
    "print(\"-\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"Model target output (Phrase):\")\n",
    "print(phrase.shape)\n",
    "print(phrase[0])\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee33612-effd-47b9-8409-e4640ed4da80",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37404a62-14c6-4a6b-9edb-7976811a8ebf",
   "metadata": {},
   "source": [
    "## Common elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "eb1091fd-6869-48f7-85d5-2e541705d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "    \n",
    "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1) \n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalTokenEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_future_input_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=max_future_input_size, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "class LandmarkEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, len_of_seq, d_model, num_conv_layers, filter_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.conv_block = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv1D(d_model, filter_size, padding=\"same\", activation=\"relu\")\n",
    "            for _ in range(num_conv_layers)\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "\n",
    "class PositionalLandmarkEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, len_of_seq, d_model, num_conv_layers, filter_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.len_of_seq = len_of_seq\n",
    "        self.conv_block = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv1D(d_model, filter_size, padding=\"same\", activation=\"relu\")\n",
    "            for _ in range(num_conv_layers)\n",
    "        ])\n",
    "        self.pos_encoding = positional_encoding(length=len_of_seq, depth=d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :self.len_of_seq, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fcad54-ad86-4b8e-af25-272c6b3ec140",
   "metadata": {},
   "source": [
    "## Encoder-Decoder using recurrent units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "8c63717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRecurrent(tf.keras.Model):\n",
    "    def __init__(self, *, recurrent_unit_type, num_recurrent_layers=1, len_lm_seq, num_conv_layers, filter_size, d_model, input_vocab_size, target_vocab_size, max_future_input_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        if recurrent_unit_type not in ['LSTM', 'SimpleRNN', 'GRU']:\n",
    "            raise ValueError(f\"Invalid recurrent_unit_type: {recurrent_unit_type}. Choose from ['LSTM', 'SimpleRNN', 'GRU']\")\n",
    "\n",
    "        self.max_future_input_size = max_future_input_size\n",
    "\n",
    "        # Positional embeddings and dropout\n",
    "        self.enc_pos_embedding = LandmarkEmbedding(len_lm_seq, d_model, num_conv_layers, filter_size)\n",
    "        self.enc_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_pos_embedding = PositionalTokenEmbedding(vocab_size=input_vocab_size, d_model=d_model, max_future_input_size=max_future_input_size)\n",
    "        self.dec_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        # Encoder recurrent units\n",
    "        recurrent_unit = {'LSTM': tf.keras.layers.LSTM, 'SimpleRNN': tf.keras.layers.SimpleRNN, 'GRU': tf.keras.layers.GRU}[recurrent_unit_type]\n",
    "        self.enc_recurrent_layers = [recurrent_unit(d_model, return_sequences=True, return_state=True) for _ in range(num_recurrent_layers)]\n",
    "        self.dec_recurrent_layers = [recurrent_unit(d_model, return_sequences=True, return_state=True) for _ in range(num_recurrent_layers)]\n",
    "\n",
    "        # Final classifier\n",
    "        self.classifier = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        landmark_seq, prev_gen_ctx  = inputs\n",
    "\n",
    "        encoded_lm_seq = self.enc_pos_embedding(landmark_seq)\n",
    "        encoded_lm_seq = self.enc_dropout(encoded_lm_seq)\n",
    "\n",
    "        states = None\n",
    "        for layer in self.enc_recurrent_layers:\n",
    "            encoded_lm_seq, *states = layer(encoded_lm_seq, initial_state=states)\n",
    "\n",
    "        # states now contain the final state(s) of the encoder\n",
    "        embedded_ctx = self.dec_pos_embedding(prev_gen_ctx)\n",
    "        embedded_ctx = self.dec_dropout(embedded_ctx)\n",
    "\n",
    "        mask = self.dec_pos_embedding.compute_mask(prev_gen_ctx)\n",
    "        # Initialize the first decoder layer with the final state of the encoder\n",
    "        embedded_ctx, *_ = self.dec_recurrent_layers[0](embedded_ctx, initial_state=states, mask=mask)\n",
    "\n",
    "        for layer in self.dec_recurrent_layers[1:]:\n",
    "            embedded_ctx, *_ = layer(embedded_ctx, mask=mask)\n",
    "\n",
    "        logits = self.classifier(embedded_ctx)\n",
    "\n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "        return logits\n",
    "\n",
    "# This function is important in case the model needs to be loaded only from the saved weights.\n",
    "# The exact model needs to be recreated, with all parameters matching!\n",
    "def get_model(d_model):\n",
    "    max_future_input_size = MAX_PHRASE_LEN  # In the future the input can be longer than what it's trained on\n",
    "    \n",
    "    seq2seq_model = Seq2SeqRecurrent(\n",
    "        recurrent_unit_type=\"GRU\", # LSTM, SimpleRNN, GRU\n",
    "        num_recurrent_layers=1,\n",
    "        len_lm_seq=FRAME_LEN,\n",
    "        num_conv_layers=3,\n",
    "        filter_size=11,\n",
    "        d_model=d_model,\n",
    "        input_vocab_size=len(char_to_num),\n",
    "        target_vocab_size=len(char_to_num),\n",
    "        max_future_input_size=max_future_input_size,\n",
    "        dropout_rate=0.1)\n",
    "\n",
    "    return seq2seq_model\n",
    "\n",
    "D_MODEL = 256 # embedding size\n",
    "\n",
    "model = get_model(D_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a837-86b2-485e-b832-b378b79dc0ae",
   "metadata": {},
   "source": [
    "## General transformer\n",
    "\n",
    "Source: https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb#scrollTo=1Rz82wEs5biZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b823726e-3c68-4115-818d-3800f3023d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "       \n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "    \n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask = True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x\n",
    "    \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "    \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, len_of_seq, num_layers, d_model, num_heads,\n",
    "               dff, num_conv_layers, filter_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.pos_embedding = PositionalLandmarkEmbedding(\n",
    "            len_of_seq, d_model, num_conv_layers, filter_size)\n",
    "    \n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is landmark sequences with shape: (batch, seq_len, features)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "    \n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "    \n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "    \n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "    \n",
    "        # Cache the last attention scores for plotting later\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "    \n",
    "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, max_future_input_size, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.pos_embedding = PositionalTokenEmbedding(vocab_size=vocab_size, d_model=d_model, max_future_input_size=max_future_input_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "    \n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # `x` is token-IDs shape (batch, target_seq_len)\n",
    "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x  = self.dec_layers[i](x, context)\n",
    "    \n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "    \n",
    "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "        return x\n",
    "    \n",
    "class GeneralTransformer(tf.keras.Model):\n",
    "    def __init__(self, *, len_lm_seq, num_enc_layers, num_conv_layers, filter_size, num_dec_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, max_future_input_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.max_future_input_size = max_future_input_size\n",
    "        \n",
    "        self.encoder = Encoder(len_of_seq=len_lm_seq,\n",
    "                               num_layers=num_enc_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               num_conv_layers=num_conv_layers,\n",
    "                               filter_size=filter_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.decoder = Decoder(num_layers=num_dec_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               max_future_input_size=max_future_input_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "    \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "        # first argument.\n",
    "        landmark_seq, prev_gen_context  = inputs\n",
    "    \n",
    "        encoded_lm_seq = self.encoder(landmark_seq)  # (batch_size, landmark_seq_len, d_model)\n",
    "        x = self.decoder(prev_gen_context, encoded_lm_seq)  # (batch_size, target_len, d_model)\n",
    "        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    \n",
    "        return logits\n",
    "\n",
    "# This function is important in case the model needs to be loaded only from the saved weights.\n",
    "# The exact model needs to be recreated, with all parameters matching!\n",
    "def get_model(d_model):\n",
    "    max_future_input_size = MAX_PHRASE_LEN  # In the future the input can be longer than what it's trained on\n",
    "    \n",
    "    general_transformer = GeneralTransformer(\n",
    "        len_lm_seq=FRAME_LEN,\n",
    "        num_enc_layers=2,\n",
    "        num_conv_layers=3,\n",
    "        filter_size=11,\n",
    "        num_dec_layers=1,\n",
    "        d_model=d_model,\n",
    "        num_heads=4,\n",
    "        dff=256, \n",
    "        input_vocab_size=len(char_to_num),\n",
    "        target_vocab_size=len(char_to_num),\n",
    "        max_future_input_size=max_future_input_size,\n",
    "        dropout_rate=0.1)\n",
    "\n",
    "    return general_transformer\n",
    "\n",
    "D_MODEL = 256 # embedding size\n",
    "\n",
    "model = get_model(D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "64de77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 78)\n",
      "(32, 33)\n",
      "(32, 33, 62)\n",
      "Model: \"seq2_seq_recurrent_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_3 (Land  multiple                  1662208   \n",
      " markEmbedding)                                                  \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " positional_token_embedding  multiple                  15872     \n",
      " _6 (PositionalTokenEmbeddi                                      \n",
      " ng)                                                             \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " gru_12 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_13 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " dense_24 (Dense)            multiple                  15934     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2483518 (9.47 MB)\n",
      "Trainable params: 2483518 (9.47 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example inference to build the model\n",
    "(lm, ctx), _label = next(iter(train_ds))\n",
    "output = model((lm, ctx))\n",
    "\n",
    "print(lm.shape)\n",
    "print(ctx.shape)\n",
    "print(output.shape)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8efb6-0a40-40e0-9ac4-50a459176da4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8cf57-c546-4c53-a72b-e46bf66f4311",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f8900067-0797-4b12-920e-806271455347",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9b117-d236-4c8c-8928-c17985d83085",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9162832-30e7-4770-a830-58bcc13d84d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Linear warm-up inverse square root decay\n",
    "\n",
    "The learning rate formula used in the \"Attention Is All You Need\" paper, where the Transformer architecture was introduced. This learning rate schedule is often referred to as the Transformer learning rate schedule or the Noam learning rate schedule (after one of the paper's authors, Noam Shazeer).\r\n",
    "\r\n",
    "The schedule first increases the learning rate linearly for warmup_steps steps, and then decreases it proportionally to the inverse square root of the step count. The rate at which it decreases is modulated by the model's depth (d_model). This combination ensures a rapid increase at the beginning of training, allowing the model to learn quickly, and then a gradual decrease to fine-tune as training progresses.\r\n",
    "\r\n",
    "In the context of the Transformer architecture, this learning rate schedule was found to be effective. The warm-up steps help stabilize training early on, while the decay helps in convergence in the later stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d831b5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.sqrt_d_model = tf.math.rsqrt(tf.cast(d_model, tf.float32))\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return self.sqrt_d_model * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "transformer_schedule = TransformerSchedule(D_MODEL)\n",
    "\n",
    "plt.plot(transformer_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb7c3bc-00f7-4c3d-a926-82a94044997d",
   "metadata": {},
   "source": [
    "#### Linear warm-up cosine decay\n",
    "\n",
    "For the Seq2Seq model, especially when trained on tasks like neural machine translation or any other sequence generation tasks, a combination of warm-up followed by a cosine or polynomial decay often works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b3ec27ba-da91-40a2-a70e-7164b631837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not the ideal way to do this, but it's managable with the size of the dataset\n",
    "steps_per_epoch = sum(1 for _ in train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a5787482-1afd-46fa-9ff6-50e04855f4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAGwCAYAAACJjDBkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABpaklEQVR4nO3deVxUVf8H8M+dGWaGdYCQTRExd8UNdcTUFiksM03LJVIzt0zLslV/peVjj2b2VJZlpmmWe5mVqWW4pSIo4oKouSDgAgjIsMg6c35/EGOTqAwClxk+79drXsq95858DwPOx3vPPUcSQggQERERkVUUchdAREREZIsYooiIiIiqgCGKiIiIqAoYooiIiIiqgCGKiIiIqAoYooiIiIiqgCGKiIiIqApUchdgz0wmEy5dugRXV1dIkiR3OURERFQJQgjk5ubC398fCsXNzzcxRNWgS5cuISAgQO4yiIiIqApSUlLQqFGjm+5niKpBrq6uAMreBDc3N5mrISIiosrIyclBQECA+XP8ZhiialD5JTw3NzeGKCIiIhtzu6E4HFhOREREVAUMUURERERVwBBFREREVAUMUURERERVwBBFREREVAUMUURERERVwBBFREREVAUMUURERERVwBBFREREVAUMUURERERVIHuIWrhwIZo0aQKtVgu9Xo+YmJhbtl+/fj1atWoFrVaL4OBgbN682WK/EAIzZsyAn58fHB0dERYWhtOnT1u0ee+999CjRw84OTnB3d29wtdJTk5Gv3794OTkBG9vb7z22msoLS29o74SERGR/ZA1RK1duxZTp07FzJkzcejQIXTo0AHh4eFIT0+vsP2+ffswfPhwjBkzBnFxcRg4cCAGDhyI+Ph4c5t58+ZhwYIFWLRoEaKjo+Hs7Izw8HAUFhaa2xQXF+PJJ5/ExIkTK3wdo9GIfv36obi4GPv27cM333yD5cuXY8aMGdX7DSAiIiLbJWTUrVs3MWnSJPPXRqNR+Pv7izlz5lTYfsiQIaJfv34W2/R6vZgwYYIQQgiTySR8fX3FBx98YN6fnZ0tNBqNWL169Q3Pt2zZMqHT6W7YvnnzZqFQKERqaqp52xdffCHc3NxEUVFRpftnMBgEAGEwGCp9jL0ymUzicnaBSDUUiIzcQmEoKBbXikpFcalRmEwmucsjIiIyq+znt0qu8FZcXIzY2FhMmzbNvE2hUCAsLAxRUVEVHhMVFYWpU6dabAsPD8fGjRsBAImJiUhNTUVYWJh5v06ng16vR1RUFIYNG1ap2qKiohAcHAwfHx+L15k4cSKOHz+OTp06VXhcUVERioqKzF/n5ORU6vXqgxk/Hce3+5Mq3OeglOCqdYCrVgVXrQouGhXctA5o4KpBA1cNvF21f/+pgZ+u7O+3W1mbiIiopskWojIyMmA0Gi2CCgD4+Pjg5MmTFR6TmppaYfvU1FTz/vJtN2tTGTd7nX++RkXmzJmDd999t9KvU5/s+usKAECSACEs95UYBbLyi5GVX1yp59KoFAjwdELjvx+BdzmhubcrWvi6oIELAxYREdUO2UKUPZo2bZrFmbKcnBwEBATIWFHdUGI04WJ2AQAg6s0+8HbVoMRkQqlRoMRowrViI3ILS5FbWILcwlLkFJYgp6AEV/KKcSW3EFdyi5CeW4T0nCKk5xaiqNSEM+l5OJOed8NruTs5oMXfgaq1nxs6NHJHS19XOChlv4eCiIjsjGwhysvLC0qlEmlpaRbb09LS4OvrW+Exvr6+t2xf/mdaWhr8/Pws2nTs2LHStfn6+t5wl2D5696sNgDQaDTQaDSVfp364nJ2IYwmAY1KAW9XDRQKCRqFEpq/f/rcnSr/XCVGEy5lFyA565r5kXglH6fT85CUmY/sayWIOZ+FmPNZ5mM0KgXa+ruhfSN3dAjQoVOABwLvcuIZKyIiuiOyhSi1Wo2QkBBERkZi4MCBAACTyYTIyEhMnjy5wmNCQ0MRGRmJl156ybxt27ZtCA0NBQAEBQXB19cXkZGR5tCUk5OD6Ojom96Jd7PXee+995Ceng5vb2/z67i5uaFNmzbWd7aeS8rKBwAEeDpBobiz4OKgVCDwLmcE3uV8w77CEiPOpOfhdHouTqXmIf6iAUcvZCOnsBSHkrNxKDnb3NbHTYPuTe8yP5owVBERkZVkvZw3depUjBo1Cl26dEG3bt3w8ccfIz8/H6NHjwYAjBw5Eg0bNsScOXMAAFOmTMG9996LDz/8EP369cOaNWtw8OBBLF68GAAgSRJeeuklzJ49G82bN0dQUBDefvtt+Pv7m4MaUDYHVFZWFpKTk2E0GnH48GEAQLNmzeDi4oKHHnoIbdq0wYgRIzBv3jykpqbirbfewqRJk3imqQqSs64BABp7WnHKqQq0Dkq0a6hDu4Y68zaTSeB8Zj6OXjDgyIVsHEnJRvzFHKTlFOGnw5fw0+FLAABvVw16NvPCfa280bu5F9yd1DVaKxER2T5ZQ9TQoUNx5coVzJgxA6mpqejYsSO2bt1qHsSdnJwMheL6WJYePXpg1apVeOuttzB9+nQ0b94cGzduRLt27cxtXn/9deTn52P8+PHIzs5Gz549sXXrVmi1WnObGTNm4JtvvjF/XX633Y4dO3DfffdBqVRi06ZNmDhxIkJDQ+Hs7IxRo0Zh1qxZNf0tsUu1FaIqolBIaNrABU0buGBgp4YAys5YHUq+iv3nsrD/XCYOJ2cjPbcIG+IuYkPcRSgkoHNjD9zfyhv3tWyANn5uPEtFREQ3kIT4971SVF1ycnKg0+lgMBjg5uYmdzmymfhdLLbEp2LGo23wbM8gucu5QXmo2vXXFew8eQWn0nIt9vu6adG3nS8eCfZDSKAHlHd4SZKIiOq2yn5+8+48qnHlZ6IC76r9M1GVoXVQosfdXuhxtxemPdwaF7MLsPNUOnacvIK9ZzKQmlOI5fvOY/m+82jgqsHD7XzxcDs/dAvyZKAiIqrHGKKoRgkhkJwp3+W8qmjo7ogIfSAi9IEoLDFi75kMbD6Wit8TUnEltwgropKwIioJXi4aPNreD0+ENEJbf17yIyKqbxiiqEZlXytBblHZws0BNhKi/knroESf1j7o09oHxaXB2Hs2A5uPXsbvCWnIyCsyn6Fq6eOKwSENMbBjQ3i7aW//xEREZPM4JqoGcUwUcDglGwMX7oWPmwbR08Nuf4CNKDGasOd0Br4/dAHbEtJQXGoCACgkoHeLBhjSJQAPtvHhJJ9ERDaIY6KoTjCPh/K8cV4nW+agVOD+Vt64v5U3DNdKsOnYJfwQewGHkrOx89QV7Dx1Bd6uGgzrGoDh+sbw0znKXTIREVUzhiiqUcmZ1yfatFc6JwfzGKpzV/Lww6ELWHvgAtJzi7Bg+xks3HkWfVp54+nugejZzOuOJxwlIqK6gSGKapScc0TJoWkDF7wW3gpT+rTAb8dT8d3+JEQnZuH3hDT8npCGIC9nPHtPEzwREgBHtVLucomI6A4wRFGNquvTG9QUtUqB/h380b+DP/5Ky8XK/UnYcOgiEjPy8fZPx/Hhtr/wtD4QI3sEwtuVA9GJiGwRR71SjSqf3sCeL+fdTgsfV7w7oB32T++Dd/q3QWNPJ2RfK8FnO86g59wdeG39Efz1rwk+iYio7mOIohpTVGrE5ZxCAPXvTFRFnDUqPHNPEHa8eh++iOiMzo3dUWw0YX3sBTz00W5M+PYg4i8a5C6TiIgqiZfzqMZcuFoAIQAntRJ3OXNB33JKhYSHg/3wcLAfYpOuYsmf57D1eCp+O56G346n4YFW3pj8QDN0buwhd6lERHQLDFFUY/45qJyzeVcsJNADIYEhOJ2Wi4U7zuDnI5ew/WQ6tp9MR89mXnjhgWbQN71L7jKJiKgCvJxHNcbWlnuRU3MfV3w8rBMiX7kPQ7o0gkohYc+ZDAxdvB9PfbUfcclX5S6RiIj+hSGKakx9m96gOgR5OWPeEx2w49X7EKFvDLVSgX1nM/H45/swbsVBnErlAHQiorqCIYpqTH2d3qA6BHg64b3Hg7H91XvxZEgjKCRgW0Ia+n6yGy+vPWw+y0dERPJhiKIaw+kN7lwjDyd88GQH/P7yvXgk2BdCAD/GXcQDH+7EzJ/ikZVfLHeJRET1FkMU1QghxD/ORNnXunlyaObtgs8jQvDL5J7o3aIBSk0C30Ql4d4PduCr3edQVGqUu0QionqHIYpqxJW8IhSUGKGQgIbuXHy3ugQ30mHFs92waqwebfzckFtYivc2n8CD/9uNLccuQwghd4lERPUGQxTViJS/z0L56RyhVvHHrLr1aOaFX17oiXlPtIe3qwbJWdcwceUhDPkyCkdSsuUuj4ioXuCnG9WIJE5vUOOUCglDugRgx6v3YUqf5tA6KHDg/FUMWLgXr39/BJl5RXKXSERk1xiiqEZweoPa46xR4eUHW2DHq/dhUOeGAIB1By/g/vk78W3UeRhNvMRHRFQTGKKoRphDFKc3qDV+Okf8b0hH/DAxFG383JBTWIq3fzqOAQv34BAn6yQiqnYMUVQjOFu5fEICPfHLCz0xa0BbuGpViL+Yg0Gf7+MlPiKiasYQRTWCE23KS6mQMDK0CXa8eh+eDGkEoOwSX9j/duGH2Au8i4+IqBowRFG1Kyg2Ij237IwHz0TJy8tFgw+e7IAfJvZAK19XXL1WglfWH8HIr2M46zkR0R1iiKJql3K17MPZTauCu5Na5moIAEICPfDLCz3xet+W0KgU+PN0Bh76eBe+3HUWpUaT3OUREdkkhiiqdubpDXgpr05xUCrw/H3NsPWl3uhx910oLDFhzpaTGLBwL+IvGuQuj4jI5jBEUbUzj4fy5HIvdVGQlzNWjtVj3hPtoXN0wPFLOXjssz2Yu+UkCku4fAwRUWUxRFG1K5+tnAsP112SVDZR5x9T70X/Dv4wCWDRrrPo/+keHLvAs1JERJXBEEXVLikzHwAHlduCBq4afDq8ExaPCIGXixqn0/Mw8PO9+N/vp1BcyrFSRES3whBF1Y7TG9ieh9r64veX78Wj7f1gNAks2H4GAxbuRcKlHLlLIyKqsxiiqFqZTAIpVwsA8EyUrfF0VuOzpzpj4VOd4eHkgBOXczBg4R58GnkaJbyDj4joBgxRVK3ScgtRXGqCSiHBT6eVuxyqgn7t/fD7y/civK0PSowCH277C08sisL5jHy5SyMiqlMYoqhalU9v0NDDESolf7xsVQNXDRY9HYJPhnWEm1aFIynZeGTBn1h3MIWznRMR/Y2fclStzAsP81KezZMkCQM6NsTWl3pDH+SJa8VGvP79UUxeFYfsa8Vyl0dEJDuGKKpWXHjY/vi7O2LVuO54vW9LqBQSfj12GQ9/8if2nc2QuzQiIlkxRFG14pko+6RUSHj+vmbY8HwPBHk547KhEBFLojF3y0lOhUBE9RZDFFUrTm9g39o3csevL/bE8G4BEH9P0PnEon1czJiI6iWGKKpWyZyt3O45qVWYM6g9Fj0dAncnBxy9YEC/BX9iy7HLcpdGRFSrGKKo2uQWliArv2zAMS/n2b++7Xzx64u9EBLogdyiUkxceQgzf4pHUSnX3yOi+oEhiqpN+VkoT2c1XLUOMldDtaGhuyPWjO+OCfc2BQB8E5WEwV/s45xSRFQvMERRtUnhoPJ6yUGpwLSHW2PZM13h4eSA+Is5ePTTPdh09JLcpRER1SiGKKo2SZzeoF67v5U3Nk/pha5NPJBXVIrJq+Lw1sZjvLxHRHaLIYqqDac3ID+dI1aP647n77sbAPDd/mQM+XI/LmUXyFwZEVH1Y4iiamMOUZzeoF5TKRV4vW8rLB/dFTpHBxxJyUb/T/dg3xlOzklE9oUhiqoNz0TRP93X0hubXuiJNn5uyMwvxtNLo/HlrrNce4+I7AZDFFWLUqMJF6+WXbLhRJtULsDTCRue74HBnRvBJIA5W05i0qpDyCsqlbs0IqI7xhBF1eKyoRClJgG1SgEfV63c5VAdonVQYv6T7fGfge3goJSw+VgqBi7ci7NX8uQujYjojjBEUbUwz1Tu4QiFQpK5GqprJEnCiO6BWDM+FD5uGpxJz8OAz/Zia3yq3KUREVUZQxRVC05vQJUREuiBTS/0gj7IE3lFpXjuu1h88NtJmEwcJ0VEtochiqrF9YWHnWWuhOq6Bq4arByrx7heQQCAhTvOYsJ3sRwnRUQ2hyGKqkUKFx4mK6iUCvxfvzb4aGgHqFUKbEtIw+DP95l/joiIbAFDFFWLpKyytdJ4OY+s8XinRlg7vju8XTU4lZaLxz7bg/3nMuUui4ioUhiiqFokZ5ZfzmOIIut0auyBnyf3RPtGOly9VoKnl0RjZXSS3GUREd0WQxTdsexrxcgpLBvPEuDBEEXW89VpsW5CKPp38EepSeD/fozHjJ/iUWI0yV0aEdFNMUTRHSsfVO7tqoGjWilzNWSrtA5KLBjWEa+FtwQArIhKwqivY3A1v1jmyoiIKsYQRXeM0xtQdZEkCZPub4bFI0LgrFZi39lMDPx8L86kc2JOIqp7ZA9RCxcuRJMmTaDVaqHX6xETE3PL9uvXr0erVq2g1WoRHByMzZs3W+wXQmDGjBnw8/ODo6MjwsLCcPr0aYs2WVlZiIiIgJubG9zd3TFmzBjk5Vn+I/3bb7+he/fucHV1RYMGDTB48GCcP3++Wvpsb7jwMFW3h9r64ofne6CRhyOSMq9h0Od7EXWWA86JqG6RNUStXbsWU6dOxcyZM3Ho0CF06NAB4eHhSE9Pr7D9vn37MHz4cIwZMwZxcXEYOHAgBg4ciPj4eHObefPmYcGCBVi0aBGio6Ph7OyM8PBwFBYWmttERETg+PHj2LZtGzZt2oTdu3dj/Pjx5v2JiYkYMGAAHnjgARw+fBi//fYbMjIyMGjQoJr7ZtiwFC48TDWgla8bfpp0Dzo3dkdOYSlGfh2N72MvyF0WEdF1QkbdunUTkyZNMn9tNBqFv7+/mDNnToXthwwZIvr162exTa/XiwkTJgghhDCZTMLX11d88MEH5v3Z2dlCo9GI1atXCyGESEhIEADEgQMHzG22bNkiJEkSFy9eFEIIsX79eqFSqYTRaDS3+fnnn4UkSaK4uPim/SksLBQGg8H8SElJEQCEwWCo7LfEJg37MkoEvrFJ/BCbIncpZIcKikvF8ytjReAbm0TgG5vEh7+dFCaTSe6yiMiOGQyGSn1+y3Ymqri4GLGxsQgLCzNvUygUCAsLQ1RUVIXHREVFWbQHgPDwcHP7xMREpKamWrTR6XTQ6/XmNlFRUXB3d0eXLl3MbcLCwqBQKBAdHQ0ACAkJgUKhwLJly2A0GmEwGPDtt98iLCwMDg4ON+3TnDlzoNPpzI+AgAArvyu26fps5TwTRdVP66DEp8M64fn77gYALNh+Bi+tPYzCEqPMlRFRfSdbiMrIyIDRaISPj4/Fdh8fH6SmVrwoaWpq6i3bl/95uzbe3t4W+1UqFTw9Pc1tgoKC8Pvvv2P69OnQaDRwd3fHhQsXsG7dulv2adq0aTAYDOZHSkrKLdvbg+JSEy4bCgBwtnKqOQqFhNf7tsK8we2hUkj46fAljFgajSzeuUdEMpJ9YHldlJqainHjxmHUqFE4cOAAdu3aBbVajSeeeAJC3HyhVI1GAzc3N4uHvbuYXQCTABwdlGjgopG7HLJzQ7oG4Jtnu8FVq8KB81cx6PO9OHeFd+4RkTxkC1FeXl5QKpVIS0uz2J6WlgZfX98Kj/H19b1l+/I/b9fm3wPXS0tLkZWVZW6zcOFC6HQ6zJs3D506dULv3r3x3XffITIy0nzJj8okZV5f7kWSJJmrofrgnmZe2DCx7M6985nXMOiLfThwPkvusoioHpItRKnVaoSEhCAyMtK8zWQyITIyEqGhoRUeExoaatEeALZt22ZuHxQUBF9fX4s2OTk5iI6ONrcJDQ1FdnY2YmNjzW22b98Ok8kEvV4PALh27RoUCstvjVKpNNdI16VwegOSQXMfV/z4/D3oEOCO7GsliFgSja3xl+Uui4jqGVkv502dOhVfffUVvvnmG5w4cQITJ05Efn4+Ro8eDQAYOXIkpk2bZm4/ZcoUbN26FR9++CFOnjyJd955BwcPHsTkyZMBlE3U99JLL2H27Nn4+eefcezYMYwcORL+/v4YOHAgAKB169bo27cvxo0bh5iYGOzduxeTJ0/GsGHD4O/vDwDo168fDhw4gFmzZuH06dM4dOgQRo8ejcDAQHTq1Kl2v0l1HCfaJLk0cNVgzbjuCGvtg+JSEyauPIQVUeflLouI6hFZQ9TQoUMxf/58zJgxAx07dsThw4exdetW88Dw5ORkXL58/X+XPXr0wKpVq7B48WJ06NAB33//PTZu3Ih27dqZ27z++ut44YUXMH78eHTt2hV5eXnYunUrtFqtuc3KlSvRqlUr9OnTB4888gh69uyJxYsXm/c/8MADWLVqFTZu3IhOnTqhb9++0Gg02Lp1KxwdHWvhO2M7kjlHFMnIUa3Eoqc74yl9YwgBzPjpOOZtPXnLsYtERNVFEvzXpsbk5ORAp9PBYDDY7SDzvh/vxsnUXCwb3RX3t/S+/QFENUAIgc+2n8GH2/4CAAzq3BDvD24PByXvnSEi61X285v/wlCVCSF4JorqBEmS8EKf5pj3RHsoFRI2HLqIZ5cfQF5RqdylEZEdY4iiKsvML8a1YiMkCWjkwcucJL8hXQKwZFQXODoo8efpDAxbHIX03MLbH0hEVAUMUVRl5YPK/dy00KiUMldDVOb+lt5YM7477nJWI/5iDgZ/sY9zSRFRjWCIoirj9AZUV3UIcMcPE3sg8C4npGQV4IlFUTh6IVvusojIzjBEUZVxegOqy5p4OeOHiT0Q3FCHrPxiDF+8H/vOZshdFhHZEYYoqrLrCw87y1wJUcW8XDRYNU6P0KZ3Ib/YiGe+PoDfjle8NicRkbUYoqjKyi/nceFhqstctQ5YNrorHmrjg2KjCRO/i8W6g/a/ODgR1TyGKKqypKzr6+YR1WVaByU+j+iMJ0MawSSA178/iiV/npO7LCKycQxRVCWFJUak5RQBAAIZosgGqJQKzHuiPcb1CgIAzP71BGc3J6I7whBFVVJ+Kc9Vo4K7k4PM1RBVjiRJmP5Ia7zetyUA4POdZzH9x3gYTQxSRGQ9hiiqkuR/TG8gSZLM1RBVniRJeP6+Zvjv48GQJGB1TDJeXB2HolKj3KURkY1hiKIq4fQGZOue0jfGZ8M7w0Ep4ddjlzH2m4PI5zIxRGQFhiiqkmROtEl2oF97Pywd1dW8TMzTS6NhKCiRuywishEMUVQlKVx4mOxE7xYNsHKcHjpHB8QlZ+Opr/YjK79Y7rKIyAYwRFGVJDFEkR3p3NgDq8eVrbd3/FJO2cLFOVy4mIhujSGKrGYyCfOZqEBPzlZO9qGNvxvWTgiFj5sGf6XlYciXUbiYXSB3WURUhzFEkdXSc4tQVGqCUiHBz10rdzlE1aaZtwvWT+iBRh6OOJ95DUMWRSEpM1/usoiojmKIIquVDypv6O4IByV/hMi+NL7LCesmhKKplzMuZhfgyUVROJ2WK3dZRFQH8ROQrFb+P3OOhyJ75e/uiDUTuqOljyvSc4swdPF+HL9kkLssIqpjGKLIaimc3oDqAW9XLdaM747ghjpk5Rdj+OL9iEu+KndZRFSHMESR1ZJ5Zx7VEx7Oaqwcp0eXQA/kFJbi6SXR2H8uU+6yiKiOYIgiq3F6A6pP3LQOWDGmG+5pdhfyi414ZlkMdv11Re6yiKgOYIgiq3GiTapvnNQqLB3VFQ+08kZhiQnjvjmIHSfT5S6LiGTGEEVWySsqRUZe2WzOHBNF9YnWQYlFT4fg4Xa+KDaaMP7bg/gjIU3usohIRgxRZJXys1AeTg5w0zrIXA1R7VKrFFgwvBP6BfuhxCgwcWUsfjueKndZRCQThiiySlImL+VR/eagVOCTYR3Rv4M/SowCk1YewpZjl+Uui4hkwBBFVrk+vQGXe6H6S6VU4KMhHTCwoz9KTQKTV8dh09FLcpdFRLWMIYqscn16A0eZKyGSl0qpwIdDOmJQ54YwmgSmrDmMnw5flLssIqpFDFFklSQuPExkplRI+OCJDngypBGMJoGX1x7Gj3EX5C6LiGoJQxRZpfxyXgDHRBEBKAtS7w9uj2FdA2ASwNR1R/B9LIMUUX3AEEWVZjQJXLjKJV+I/k2hkPDfx4MRoW8MIYDXvj+CdQdS5C6LiGoYQxRV2mVDAUqMAmqlAr5uWrnLIapTFAoJswe2w8jQQAgBvP7DUayOSZa7LCKqQQxRVGnJf09v0MjDEUqFJHM1RHWPJEl497G2eKZHEwDAtA3HsCqaQYrIXjFEUaWZ78zjpTyim5IkCTP7t8GYnkEAgOk/HsPaAwxSRPaIIYoqjQsPE1WOJEl4q19rjL6nCQDgzQ3HsP4gx0gR2RuGKKq0ZIYookqTJAkzHm2DUf8YI7XhEO/aI7InDFFUaSkMUURWkSQJ7zzWFk93L7tr79X1RzghJ5EdYYiiSjOvm8cxUUSVJkkSZj3WDsO7NYZJAC+vPYyfj3CJGCJ7wBBFlWK4VgJDQQkAnokispZCIeG9ge3ME3K+vPYwfj3KRYuJbB1DFFVK+XgoLxcNnNQqmashsj3lE3I+8fcSMS+uicOWYwxSRLaMIYoqpTxEBfJSHlGVKf5eImZQp7JFi19YHYet8alyl0VEVcQQRZWSlJUPgJfyiO6UUiHhgyc7YGBHf5SaBCavOoRtCWlyl0VEVcAQRZXCO/OIqo9SIWH+kx3Qv0NZkHp+ZSwiTzBIEdkahiiqFM4RRVS9VEoFPhrSAf3a+6HEKDDxu0PY/dcVucsiIiswRFGlcHoDouqnUirw8dCOeLidL4qNJoxbcRBRZzPlLouIKokhim6rxGjCpewCAEAgz0QRVSsHpQKfDOuEPq28UVRqwphvDuDg+Sy5yyKiSrijEFVYWFhddVAddvFqAUwC0Doo0MBVI3c5RHZHrVJgYURn9GruhWvFRoxedgBHUrLlLouIbsPqEGUymfCf//wHDRs2hIuLC86dOwcAePvtt7F06dJqL5Dk98/xUJIkyVwNkX3SOiixeEQX6IM8kVtUipFfxyDhUo7cZRHRLVgdombPno3ly5dj3rx5UKvV5u3t2rXDkiVLqrU4qhuSOKicqFY4qpX4+pmu6NzYHYaCEjy9NBqn03LlLouIbsLqELVixQosXrwYERERUCqV5u0dOnTAyZMnq7U4qhuuT2/gLHMlRPbPWaPC8me7oX0jHbLyi/HUkmgkZuTLXRYRVcDqEHXx4kU0a9bshu0mkwklJSXVUhTVLcnld+Z5OspcCVH94KZ1wIpnu6GVryuu5Bbhqa/2m/8zQ0R1h9Uhqk2bNvjzzz9v2P7999+jU6dO1VIU1S1J5iVfeCaKqLa4O6nx3Vg9mnm74LKhEMO/2m++S5aI6garV5KdMWMGRo0ahYsXL8JkMmHDhg04deoUVqxYgU2bNtVEjSQjIYT5f8ABHBNFVKu8XDRYNVaPIV9G4XzmNTz11X6smxAKbzet3KUREapwJmrAgAH45Zdf8Mcff8DZ2RkzZszAiRMn8Msvv+DBBx+siRpJRln5xcgrKoUkAY08eDmPqLZ5u2mxalx3NPJwLAtSS6KRkVckd1lEhCqciQKAXr16Ydu2bdVdC9VB5dMb+LppoXVQ3qY1EdUEf3dHrB7XHU8uisKZ9Dw8vSQaa8Z3h7uT+vYHE1GNsfpMVNOmTZGZeeOyBNnZ2WjatGm1FEV1RzIv5RHVCQGeTlg1Tg8vFw1OpuZi1LIDyCsqlbssonrN6hB1/vx5GI3GG7YXFRXh4sWLVhewcOFCNGnSBFqtFnq9HjExMbdsv379erRq1QparRbBwcHYvHmzxX4hBGbMmAE/Pz84OjoiLCwMp0+ftmiTlZWFiIgIuLm5wd3dHWPGjEFeXt4NzzN//ny0aNECGo0GDRs2xHvvvWd1/2xd+Z15XO6FSH5NG7hg1Tg9PJwccCQlG2OWH0BB8Y3/HhNR7aj05byff/7Z/PfffvsNOp3O/LXRaERkZCSaNGli1YuvXbsWU6dOxaJFi6DX6/Hxxx8jPDwcp06dgre39w3t9+3bh+HDh2POnDl49NFHsWrVKgwcOBCHDh1Cu3btAADz5s3DggUL8M033yAoKAhvv/02wsPDkZCQAK22bDBmREQELl++jG3btqGkpASjR4/G+PHjsWrVKvNrTZkyBb///jvmz5+P4OBgZGVlISur/q1nlcyJNonqlBY+rljxrB5PfbUf0YlZmLgyFotHdIFaxaVQiWqdqCRJkoQkSUKhUJj/Xv5Qq9WiRYsW4pdffqns0wkhhOjWrZuYNGmS+Wuj0Sj8/f3FnDlzKmw/ZMgQ0a9fP4tter1eTJgwQQghhMlkEr6+vuKDDz4w78/OzhYajUasXr1aCCFEQkKCACAOHDhgbrNlyxYhSZK4ePGiuY1KpRInT560qj+FhYXCYDCYHykpKQKAMBgMVj1PXfLkon0i8I1NYmPcBblLIaJ/iEnMFC3f2iwC39gknvv2oCgpNcpdEpHdMBgMlfr8rvR/XUwmE0wmExo3boz09HTz1yaTCUVFRTh16hQeffTRSoe34uJixMbGIiwszLxNoVAgLCwMUVFRFR4TFRVl0R4AwsPDze0TExORmppq0Uan00Gv15vbREVFwd3dHV26dDG3CQsLg0KhQHR0NADgl19+QdOmTbFp0yYEBQWhSZMmGDt27G3PRM2ZMwc6nc78CAgIqPT3o65K4ZkoojqpaxNPfDWyC9RKBbbEp+L1H47CZBJyl0VUr1h9/jcxMRFeXl53/MIZGRkwGo3w8fGx2O7j44PU1NQKj0lNTb1l+/I/b9fm35cKVSoVPD09zW3OnTuHpKQkrF+/HitWrMDy5csRGxuLJ5544pZ9mjZtGgwGg/mRkpJyy/Z1XWGJEak5hQAYoojqol7NG+CzpzpBqZCw4dBFzPz5OIRgkCKqLVWa4iA/Px+7du1CcnIyiouLLfa9+OKL1VKYnMrPrq1YsQItWrQAACxduhQhISE4deoUWrZsWeFxGo0GGo2mNkutUReuFkAIwEWjgqczb6UmqoseauuLD5/sgJfXHca3+5PgrFHhjb4tIUmS3KUR2T2rQ1RcXBweeeQRXLt2Dfn5+fD09ERGRgacnJzg7e1d6RDl5eUFpVKJtLQ0i+1paWnw9fWt8BhfX99bti//My0tDX5+fhZtOnbsaG6Tnp5u8RylpaXIysoyH+/n5weVSmUOUADQunVrAEBycvJNQ5S9Sc4qW/Q0wNOJ/yAT1WEDOzXEtWIjpv94DIt2nYWLRonJDzSXuywiu2f15byXX34Z/fv3x9WrV+Ho6Ij9+/cjKSkJISEhmD9/fqWfR61WIyQkBJGRkeZtJpMJkZGRCA0NrfCY0NBQi/YAsG3bNnP7oKAg+Pr6WrTJyclBdHS0uU1oaCiys7MRGxtrbrN9+3aYTCbo9XoAwD333IPS0lKcPXvW3Oavv/4CAAQGBla6j7aO0xsQ2Y6n9I3xf4+U/Wdv/u9/4es9iTJXRFQPWDtiXafTme9a0+l0IiEhQQghxP79+0XLli2teq41a9YIjUYjli9fLhISEsT48eOFu7u7SE1NFUIIMWLECPHmm2+a2+/du1eoVCoxf/58ceLECTFz5kzh4OAgjh07Zm4zd+5c4e7uLn766Sdx9OhRMWDAABEUFCQKCgrMbfr27Ss6deokoqOjxZ49e0Tz5s3F8OHDzfuNRqPo3Lmz6N27tzh06JA4ePCg0Ov14sEHH7Sqf5Ud3V9XvfNzvAh8Y5N479cEuUshokr63++nROAbm0TgG5vE2phkucshskmV/fy2+nKeg4MDFIqyE1je3t5ITk5G69atodPprB5IPXToUFy5cgUzZsxAamoqOnbsiK1bt5oHhicnJ5tfCwB69OiBVatW4a233sL06dPRvHlzbNy40TxHFAC8/vrryM/Px/jx45GdnY2ePXti69at5jmiAGDlypWYPHky+vTpA4VCgcGDB2PBggXm/QqFAr/88gteeOEF9O7dG87Oznj44Yfx4YcfWvvtsmm8M4/I9rwU1hz5RaVYsicRb2w4Cke1Ev07+MtdFpFdkoSw7laOhx56CM888wyeeuopjBs3DkePHsWLL76Ib7/9FlevXjVPE0BllxJ1Oh0MBgPc3NzkLsdqD320C3+l5WHFs93Qu0UDucshokoSQmD6j/FYHZMMlULCoqdDENbG5/YHEhGAyn9+Wz0m6r///a950PZ7770HDw8PTJw4EVeuXMGXX35Z9YqpThFCcLZyIhslSRJmD2yHAR39UWoSeH7VIew9kyF3WUR2x+rLef+cpNLb2xtbt26t1oKobriSW4TCEhMUEtDQw1HucojISkqFhPlPdsC1YiO2JaRh3IqD+HaMHiGBHnKXRmQ3qm2xpUOHDlk1YznVbUl/n4Xyd3eEg5JrchHZIgelAp891Qm9mnvhWrERzyyLQfxFg9xlEdkNqz4df/vtN7z66quYPn06zp07BwA4efIkBg4ciK5du8JkMtVIkVT7zNMb3MVLeUS2TKNS4ssRIejaxAO5haUY+XUMzqTnyl0WkV2odIhaunQpHn74YSxfvhzvv/8+unfvju+++w6hoaHw9fVFfHw8Nm/eXJO1Ui1K4ngoIrvhpFZh6TNdEdxQh6z8Yjy9JMZ89y0RVV2lQ9Qnn3yC999/HxkZGVi3bh0yMjLw+eef49ixY1i0aJF5Rm+yD9enN3CWuRIiqg5uWgd882w3NPN2QWpOIUYsjUZ6bqHcZRHZtEqHqLNnz+LJJ58EAAwaNAgqlQoffPABGjVqVGPFkXx4Zx6R/fF0VuO7MXo08nDE+cxrGLk0BoZrJXKXRWSzKh2iCgoK4ORU9oEqSRI0Go3F+nRkX5I4JorILvnqtPhujB4NXDU4mZqLZ5bHIL+oVO6yiGySVVMcLFmyBC4uLgDKFu1dvnw5vLy8LNpUdgFiqruuFZciI68IQNniw0RkX5p4OePbMd0w9Mv9iEvOxoRvY7H0mS7QqJRyl0ZkUyo9Y3mTJk0gSdKtn0ySzHftke3OWH4yNQd9P/4TOkcHHJn5kNzlEFENiUu+iogl0bhWbER4Wx8sfKozVJzShKjSn9+VPhN1/vz56qiLbACnNyCqHzo19sBXI7tg9LID+O14Gt7ccAzzBreHQnHr/zATURn+l4NuUD6onJfyiOzfPc288OlTnaBUSPg+9gJmbUqAlUuqEtVbDFF0g/IQFcgQRVQvhLf1xbzB7QEAy/edx8d/nJa5IiLbwBBFN+D0BkT1z+CQRninfxsAwCeRp7F0T6LMFRHVfQxRdIPyMVGNOSaKqF555p4gvPJgCwDAfzYlYN3BFJkrIqrbGKLIgtEkcOFqAQCeiSKqjyY/0AxjewYBAN784Si2HLssc0VEdZdV80QBZbf9VaR8Ak61Wn3HRZF8UnMKUWw0wUEpwU/nKHc5RFTLJEnC//VrjdzCUqw9mIIX18RhqUaF3i0ayF0aUZ1j9Zkod3d3eHh43PBwd3eHo6MjAgMDMXPmTJhMppqol2pY+aW8Rh5OUPI2Z6J6SZIk/HdQMPoF+6HEKDDh21jEJmXJXRZRnWN1iFq+fDn8/f0xffp0bNy4ERs3bsT06dPRsGFDfPHFFxg/fjwWLFiAuXPn1kS9VMOSs/IBcHoDovpOqZDw0dCOuLdFAxSUGPHMsgNIuFTxlQii+srqy3nffPMNPvzwQwwZMsS8rX///ggODsaXX36JyMhING7cGO+99x6mT59ercVSzeP0BkRUTq1SYNHTIRixNBoHk65i5NfRWP9cDwR5OctdGlGdYPWZqH379qFTp043bO/UqROioqIAAD179kRycvKdV0e1LjmLg8qJ6DpHtRJLn+mKNn5uyMgrxtNLonEpu0DusojqBKtDVEBAAJYuXXrD9qVLlyIgIAAAkJmZCQ8PjzuvjmpdcmbZ5TxOb0BE5XSODlgxphuaejnjYnYBnl4abV6knKg+s/py3vz58/Hkk09iy5Yt6Nq1KwDg4MGDOHnyJL7//nsAwIEDBzB06NDqrZRqBSfaJKKKeLlo8O1YPZ78Yh/OXcnHqK9jsHp8d7hpHeQujUg2kqjCIkmJiYn48ssv8ddffwEAWrZsiQkTJqBJkybVXZ9Nq+wq0HVFTmEJ2r/zOwAg/t1wuGiszthEZOfOXcnDk4uikJlfjK5NPLDiWT0c1Uq5yyKqVpX9/K5SiKLKsbUQFX/RgEc/3QMvFzUOvvWg3OUQUR11/JIBwxbvR25hKe5r2QCLR3SBWsW5m8l+VPbzu0qnGrKzsxETE4P09PQb5oMaOXJkVZ6S6oDyS3mc3oCIbqWtvw7LnumKEUtjsPPUFby87jAWDOvEueWo3rE6RP3yyy+IiIhAXl4e3NzcIEnXf2kkSWKIsmGc3oCIKqtLE08sGhGCsd8cwK9HL8NVo8KcQcEWnwlE9s7q86+vvPIKnn32WeTl5SE7OxtXr141P7KyOKOtLUvK5KByIqq8e1s0wCfDOkEhAWsOpGDOlpPgCBGqT6wOURcvXsSLL74IJyd+0NqblPI78+7iRHpEVDmPBPth7qD2AIDFu8/h851nZa6IqPZYHaLCw8Nx8ODBmqiFZMbpDYioKoZ0DcBb/VoDAD747RRWRJ2XtyCiWmL1mKh+/frhtddeQ0JCAoKDg+HgYDlHyGOPPVZtxVHtKTGacPHvWYgDOdEmEVlpbK+myCksxYLI05jx03G4alV4vFMjucsiqlFWh6hx48YBAGbNmnXDPkmSYDQa77wqqnWXswthNAloVAo0cNHIXQ4R2aCXw5ojp6AEy/edx6vrj8JZrcJDbX3lLouoxlh9Oc9kMt30wQBlu5KyypZ7CfB0goK3KRNRFUiShBmPtsHgzo1gNAlMXhWHvWcy5C6LqMZwdjQCwOkNiKh6KBQS3h8cjL5tfVFsNGHcioM4lHxV7rKIakSlLuctWLAA48ePh1arxYIFC27Z9sUXX6yWwqh2JWdyok0iqh4qpQKfDO+Isd8cxJ+nM/DM1zFYOyEUrf3q/soNRNao1LIvQUFBOHjwIO666y4EBQXd/MkkCefOnavWAm2ZLS37MvG7WGyJT8XM/m0w+p6bv8dERJV1rbgUI5bGIDbpKrxcNFj/XCiCvDiFCtV91brsS2JiYoV/J/vB6Q2IqLo5qVX4+pmuGL54PxIu5+DpJdFY/1wo/N0d5S6NqFpwTBRBCGG+nMfpDYioOukcHbBiTDc09XLGxewCPL00Ghl5RXKXRVQtrJ7iwGg0Yvny5YiMjKxwAeLt27dXW3FUO7KvlSC3qBQA0MiDIYqIqpeXiwbfjdXjyUVROHclHyOXxmD1+O7QOTrc/mCiOszqM1FTpkzBlClTYDQa0a5dO3To0MHiQbYn6e9LeT5uGmgdlDJXQ0T2yN/dEd+O6QYvFzUSLudgzPIDuFZcKndZRHfE6jNRa9aswbp16/DII4/URD0kg+vTG3DAJxHVnKYNXLDiWT2GLY7CwaSrmPBtLJaM6gKNiv95I9tk9ZkotVqNZs2a1UQtJJPkzOsTbRIR1aQ2/m5YNrobHB2U+PN0Bl5acxilRtPtDySqg6wOUa+88go++eQTVGJmBLIR5jNRHFRORLUgJNADX43sArVSgS3xqXhzwzGYTPxMIdtj9eW8PXv2YMeOHdiyZQvatm17wwLEGzZsqLbiqHZwegMiqm09m3thwfBOmLTqEL6PvQBXrQozHm0DSeKyU2Q7rA5R7u7uePzxx2uiFpJJ+fQGjXkmiohqUd92vpg3uD1eWX8Ey/aeh5vWAS8/2ELusogqzaoQVVpaivvvvx8PPfQQfH25Mrc9KCo14nJOIQCeiSKi2jc4pBHyikox8+fj+CTyNFy1Kozt1VTusogqxaoxUSqVCs899xyKijhRmr24cLUAQgBOaiXuclbLXQ4R1UOjejTBqw+VnYGa/esJrD2QLHNFRJVj9cDybt26IS4uriZqIRn8czwUxyIQkVwm3d8ME3qXnYGatuEYfj16WeaKiG7P6jFRzz//PF555RVcuHABISEhcHa2nFuoffv21VYc1TzzeCheyiMiGUmShDcfboWcwhKsjknBS2vj4KRR4v6W3nKXRnRTVoeoYcOGAQBefPFF8zZJkiCEgCRJMBqN1Vcd1ThOb0BEdYUkSZg9MBi5haXYdPQyJn4XixXP6tEtyFPu0ogqZHWISkxMrIk6SCac3oCI6hKlQsJHQzviWrER20+m49nlB7B6XHcEN9LJXRrRDawOUYGBgTVRB8nk+vQGXPKFiOoGB6UCn0d0xqivYxCdmIVRy2KwbkJ3NPN2lbs0IgtWh6hyCQkJSE5ORnFxscX2xx577I6LotohhOCZKCKqk7QOSiwZ1QURS6Jx9IIBEUui8f1zPbg8FdUpVoeoc+fO4fHHH8exY8fMY6EAmO/s4pgo23ElrwgFJUYoJKChu6Pc5RARWXDVOuCb0d0w5MsonE7PQ8SSaKx/LhQ+blq5SyMCUIUpDqZMmYKgoCCkp6fDyckJx48fx+7du9GlSxfs3LmzBkqkmpLy91koP50j1CqrfxSIiGqch7Ma343Vo7GnE5KzriFiSTQy8zhXIdUNVn9yRkVFYdasWfDy8oJCoYBCoUDPnj0xZ84cizv2qO5L4vQGRGQDfNy0WDlWDz+dFmfS8/D00hgYrpXIXRaR9SHKaDTC1bVscJ+XlxcuXboEoGzA+alTp6pUxMKFC9GkSRNotVro9XrExMTcsv369evRqlUraLVaBAcHY/PmzRb7hRCYMWMG/Pz84OjoiLCwMJw+fdqiTVZWFiIiIuDm5gZ3d3eMGTMGeXl5Fb7emTNn4OrqCnd39yr1r67i9AZEZCsCPJ2wcqweXi4anLicg1HLYpBXVCp3WVTPWR2i2rVrhyNHjgAA9Ho95s2bh71792LWrFlo2tT69Y7Wrl2LqVOnYubMmTh06BA6dOiA8PBwpKenV9h+3759GD58OMaMGYO4uDgMHDgQAwcORHx8vLnNvHnzsGDBAixatAjR0dFwdnZGeHg4CgsLzW0iIiJw/PhxbNu2DZs2bcLu3bsxfvz4G16vpKQEw4cPR69evazuW11XHqI4UJOIbEHTBi5YOVYPdycHHE7JxrPLD6CgmONwSUbCSlu3bhU//PCDEEKI06dPi5YtWwpJkoSXl5eIjIy09ulEt27dxKRJk8xfG41G4e/vL+bMmVNh+yFDhoh+/fpZbNPr9WLChAlCCCFMJpPw9fUVH3zwgXl/dna20Gg0YvXq1UIIIRISEgQAceDAAXObLVu2CEmSxMWLFy2e+/XXXxdPP/20WLZsmdDpdFb1zWAwCADCYDBYdVxtGfz5XhH4xibxy5GLt29MRFRHHE3JFu1mbBWBb2wSTy/ZLwpLSuUuiexMZT+/rT4TFR4ejkGDBgEAmjVrhpMnTyIjIwPp6el44IEHrHqu4uJixMbGIiwszLxNoVAgLCwMUVFRFR4TFRVl0b68pvL2iYmJSE1NtWij0+mg1+vNbaKiouDu7o4uXbqY24SFhUGhUCA6Otq8bfv27Vi/fj0WLlxYqf4UFRUhJyfH4lGXcXoDIrJFwY10WDa6KxwdlPjzdAYmr4pDidEkd1lUD1X5lqwzZ87gt99+Q0FBATw9qzYlf0ZGBoxGI3x8fCy2+/j4IDU1tcJjUlNTb9m+/M/btfH2tlyPSaVSwdPT09wmMzMTzzzzDJYvXw43N7dK9WfOnDnQ6XTmR0BAQKWOk0NBsRHpuWV3uAR6cqJNIrItXZp4YsmoLlCrFNiWkIap647AaBJyl0X1jNUhKjMzE3369EGLFi3wyCOP4PLlspW2x4wZg1deeaXaC5TLuHHj8NRTT6F3796VPmbatGkwGAzmR0pKSg1WeGdSrpadhXLTqqBzcpC5GiIi693TzAuLnu4MB6WEX45cwrQNR2FikKJaZHWIevnll+Hg4IDk5GQ4OV2/DDR06FBs3brVqufy8vKCUqlEWlqaxfa0tDT4+vpWeIyvr+8t25f/ebs2/x64XlpaiqysLHOb7du3Y/78+VCpVFCpVBgzZgwMBgNUKhW+/vrrCmvTaDRwc3OzeNRV5ukNeGceEdmwB1r54JNhnaCQgHUHL2DWpgTzJNBENc3qEPX777/j/fffR6NGjSy2N2/eHElJSVY9l1qtRkhICCIjI83bTCYTIiMjERoaWuExoaGhFu0BYNu2beb2QUFB8PX1tWiTk5OD6Ohoc5vQ0FBkZ2cjNjbW3Gb79u0wmUzQ6/UAysZNHT582PyYNWsWXF1dcfjwYTz++ONW9bMuMk9vwEt5RGTjHgn2w/wnO0CSgOX7zmPeb6cYpKhWWL3sS35+vsUZqHJZWVnQaDRWFzB16lSMGjUKXbp0Qbdu3fDxxx8jPz8fo0ePBgCMHDkSDRs2xJw5cwCUzZh+77334sMPP0S/fv2wZs0aHDx4EIsXLwZQtvzMSy+9hNmzZ6N58+YICgrC22+/DX9/fwwcOBAA0Lp1a/Tt2xfjxo3DokWLUFJSgsmTJ2PYsGHw9/c3t/mngwcPQqFQoF27dlb3sS5KzswHwOkNiMg+DOrcCAUlRvzfj/H4YudZODko8UKf5nKXRXbO6hDVq1cvrFixAv/5z38AlIUWk8mEefPm4f7777e6gKFDh+LKlSuYMWMGUlNT0bFjR2zdutU8MDw5ORkKxfUTZj169MCqVavw1ltvYfr06WjevDk2btxoEW5ef/115OfnY/z48cjOzkbPnj2xdetWaLXX11tauXIlJk+ejD59+kChUGDw4MFYsGCB1fXbKk60SUT2JkIfiIJiI2b/egIfbvsLjmolxvayfv5CosqShJXnPOPj49GnTx907twZ27dvx2OPPYbjx48jKysLe/fuxd13311TtdqcnJwc6HQ6GAyGOjc+qs+HO3H2Sj5WjtXjnmZecpdDRFRtPo08jQ+3/QUAeO/xdojQB8pcEdmayn5+V2nG8r/++gs9e/bEgAEDkJ+fj0GDBiEuLo4BykaYTAIpVwsAcI4oIrI/kx9ohon3lX0evbUxHhsOXZC5IrJXVl/OA8omr/y///s/i20XLlzA+PHjzWOTqO5Kyy1EcakJKoUEP5329gcQEdkQSZLwenhLFBQbsXzfeby6/gi0Dko8Euwnd2lkZ6o82ea/ZWZmYunSpdX1dFSDyqc3aOjhCJWy2n4EiIjqDEmSMOPRNhjaJQAmAby4Og6RJ9JufyCRFfgJWg9xuRciqg8UCgn/HRSMAR39UWoSmPjdIew8VfHi9kRVwRBVDyVnMkQRUf2gVEj48MkOeCTYF8VGE8Z/G4s9pzPkLovsBENUPcTpDYioPlEpFfhkWCc82MYHxaUmjF1xAFFnM+Uui+xApQeWDxo06Jb7s7Oz77QWqiW8nEdE9Y2DUoHPnuqEid8dwvaT6Xh2+QF882w3dAvylLs0smGVPhOl0+lu+QgMDMTIkSNrslaqJtdDFJd8IaL6Q6NS4vOIzujdogEKSowYvSwGsUlX5S6LbFilz0QtW7asJuugWpJbWIKs/GIAXHyYiOofrYMSi0eEYMw3B7D3TCae+ToG347Vo2OAu9ylkQ3imKh6pvws1F3OarhoqjRNGBGRTdM6KLFkZFfogzyRW1SKkUujEX/RIHdZZIMYouqZlL9DFBceJqL6zFGtxNfPdEWXQA/kFJbi6aXRSLiUI3dZZGMYouqZJE5vQEQEAHDWqLBsdFd0auyO7GsleHppNE6l5spdFtkQhqh6htMbEBFd56p1wPLR3dC+kQ5Z+cWIWLIfZ9Lz5C6LbARDVD2TzMt5REQWdI4OWPFsN7Txc0NGXjGe+mo/EjPy5S6LbABDVD1jPhPFEEVEZObupMZ3Y/Vo5euK9NwiDF+8H+cZpOg2GKLqkVKjCRevFgDg9AZERP/m6VwWpJp7uyA1pxDDGKToNhii6pHLhkKUmgTUKgV8XLVyl0NEVOd4uWiwalx3c5AaujiKl/bophii6hHzeCgPRygUkszVEBHVTQ1cy4JUCx8XpOUUYdjiKJy7wsHmdCOGqHqE0xsQEVVOeZBq6eP6d5Daj7MMUvQvDFH1yPXpDbhmHhHR7ZRd2tOjpc/1weac/oD+iSGqHuFs5URE1rnr7yBlvmvvKwYpuo4hqh5JyiobHMnpDYiIKu+uvwebt/J1xZXcskt7Z9I5szkxRNUryeVjoji9ARGRVTyd1Vg1rjta+7khI68sSJ1OY5Cq7xii6onsa8XIKSwFAAR4MEQREVnL01mNVWP15pnNh3+1H38xSNVrDFH1RPmgcm9XDRzVSpmrISKyTR7Oaqwcq0db/7+D1OL9XLS4HmOIqic4vQERUfUoD1LtGrohM79srT0GqfqJIaqeKD8TxfFQRER3zt1Jje/G6BHcUIfM/LJLe8cvGeQui2oZQ1Q9kcwzUURE1ao8SHVopENWftmlvcMp2XKXRbWIIaqeuD7RJkMUEVF10Tk54NuxeoQEeiCnsBRPL4nGgfNZcpdFtYQhqp4wX87jmSgiomrlpnXAime7IbTpXcgrKsXIpTHYeyZD7rKoFjBE1QPFpSZcNhQAABp7cskXIqLq5qxRYdnorujdogEKSowYvfwAdpxKl7ssqmEMUfXAxewCmATgpFbCy0UtdzlERHZJ66DEVyND8GAbHxSXmjB+xUFsjU+VuyyqQQxR9UBSZtlyL409nSBJkszVEBHZL41Kic8jOqNfez+UGAUmrTqEn49ckrssqiEMUfUAFx4mIqo9DkoFPhnaEYM6NYTRJPDSmjh8H3tB7rKoBjBE1QOcaJOIqHaplArMf7IDhncLgEkAr64/gpXRSXKXRdWMIaoe4PQGRES1T6GQ8N/Hg/FMjyYAgP/7MR5L9yTKWxRVK4aoeiCZl/OIiGQhSRJm9m+D5+69GwDwn00J+PiPvyCEkLkyqg4MUXZOCHH9TBRDFBFRrZMkCW/0bYmpD7YAAHz8x2nM2pQAk4lBytYxRNm5zPxiXCs2QpKAhh6OcpdDRFQvSZKEF/s0xzv92wAAlu09j9e+P4pSo0nmyuhOMETZufJB5f46R2hUSpmrISKq3565Jwj/G9IBSoWEHw5dwPMrD6GwxCh3WVRFDFF27vr0BjwLRURUFwzq3AhfRHSGWqXA7wlpeHb5AeQVlcpdFlUBQ5Sd4/QGRER1z0NtfbF8dFc4q5XYdzYTEUuicTW/WO6yyEoMUXbu+vQGXDOPiKgu6XG3F1aN6w53JwccScnG0MVRSMsplLsssgJDlJ3jbOVERHVXhwB3rJsQCh83Df5Ky8MTi/aZl+qiuo8hys4lZZX9MnJ6AyKiuqmFjyu+f64HAu9yQkpWAZ5YFIVTqblyl0WVwBBlxwpLjEjLKQLAMVFERHVZgKcT1k8IRStfV1zJLcKQL6Nw8HyW3GXRbTBE2bHyS3muWhXcnRxkroaIiG7F202LNeO7o3NjdxgKShCxJBrbEtLkLotugSHKjpUPKm/s6QRJkmSuhoiIbsfdSY2VY7ujTytvFJWaMOHbg1gTkyx3WXQTDFF2jNMbEBHZHke1El+OCMGQLo1gEsCbG45hQeRprrdXBzFE2THzmai7GKKIiGyJSqnA+4Pb44UHmgEA/rftL7y1MR5GrrdXpzBE2bGULJ6JIiKyVZIk4ZWHWuI/A9pCkoCV0cl4fmUsl4mpQxii7FhS+USbnpxok4jIVo0IbYLPn+oMtVKB346nYeTSGBiulchdFoEhym6ZTIJnooiI7MTDwX5YMaYbXLUqxJzPwpAvo3DZUCB3WfUeQ5SdSs8tQlGpCUqFBH93rdzlEBHRHere9C6sf65sdvNTabkY9Pk+nLicI3dZ9RpDlJ0qH1Te0N0RKiXfZiIie9DK1w0/TOyBZt4uuGwoxJOLorD7rytyl1Vv8dPVTpWvvRTIO/OIiOxKIw8n/PBcD3Rv6om8olKMXn4Aaw9wLik51IkQtXDhQjRp0gRarRZ6vR4xMTG3bL9+/Xq0atUKWq0WwcHB2Lx5s8V+IQRmzJgBPz8/ODo6IiwsDKdPn7Zok5WVhYiICLi5ucHd3R1jxoxBXl6eef/OnTsxYMAA+Pn5wdnZGR07dsTKlSurr9M1jAsPExHZL52TA1Y8q8egTg1hNAm88cMxzP/tFOeSqmWyh6i1a9di6tSpmDlzJg4dOoQOHTogPDwc6enpFbbft28fhg8fjjFjxiAuLg4DBw7EwIEDER8fb24zb948LFiwAIsWLUJ0dDScnZ0RHh6OwsJCc5uIiAgcP34c27Ztw6ZNm7B7926MHz/e4nXat2+PH374AUePHsXo0aMxcuRIbNq0qea+GdUoiYPKiYjsmlqlwIdDOmBKn+YAgM92nMGUNYdRVMopEGqLJGSOrXq9Hl27dsVnn30GADCZTAgICMALL7yAN99884b2Q4cORX5+vkWY6d69Ozp27IhFixZBCAF/f3+88sorePXVVwEABoMBPj4+WL58OYYNG4YTJ06gTZs2OHDgALp06QIA2Lp1Kx555BFcuHAB/v7+Fdbar18/+Pj44Ouvv65U33JycqDT6WAwGODm5mbV9+VOPf75XsQlZ+OLiM54ONivVl+biIhq1/exF/DmD0dRahLo1sQTX44IgYezWu6ybFZlP79lPRNVXFyM2NhYhIWFmbcpFAqEhYUhKiqqwmOioqIs2gNAeHi4uX1iYiJSU1Mt2uh0Ouj1enObqKgouLu7mwMUAISFhUGhUCA6Ovqm9RoMBnh6et50f1FREXJyciwecuHlPCKi+uOJkEb45tnrUyAM/mKfeWws1RxZQ1RGRgaMRiN8fHwstvv4+CA1NbXCY1JTU2/ZvvzP27Xx9va22K9SqeDp6XnT1123bh0OHDiA0aNH37Q/c+bMgU6nMz8CAgJu2rYm5RWVIiOvGACXfCEiqi/uaeaFHyb2QEN3R5zLyMfjn+9DTGKW3GXZNdnHRNmCHTt2YPTo0fjqq6/Qtm3bm7abNm0aDAaD+ZGSklKLVV5XfhbKw8kBbloHWWogIqLa18LHFT8+3wPBDXXIyi9GxJL9WHdQns+i+kDWEOXl5QWlUom0tDSL7WlpafD19a3wGF9f31u2L//zdm3+PXC9tLQUWVlZN7zurl270L9/f3z00UcYOXLkLfuj0Wjg5uZm8ZBDUmb5wsNc7oWIqL7xdtNi3YRQ9Av2Q4lR4PXvj+K9XxO4eHENkDVEqdVqhISEIDIy0rzNZDIhMjISoaGhFR4TGhpq0R4Atm3bZm4fFBQEX19fizY5OTmIjo42twkNDUV2djZiY2PNbbZv3w6TyQS9Xm/etnPnTvTr1w/vv/++xZ17dR2XeyEiqt8c1Up8OryT+c69r/5MxLgVB5FbyDX3qpPsl/OmTp2Kr776Ct988w1OnDiBiRMnIj8/3zz2aOTIkZg2bZq5/ZQpU7B161Z8+OGHOHnyJN555x0cPHgQkydPBlC26vVLL72E2bNn4+eff8axY8cwcuRI+Pv7Y+DAgQCA1q1bo2/fvhg3bhxiYmKwd+9eTJ48GcOGDTPfmbdjxw7069cPL774IgYPHozU1FSkpqYiK6vuX19OyiobTNjY01HmSoiISC4KhYSXH2yBT4d3gkalwPaT6Rj8xT4k/321gqqBqAM+/fRT0bhxY6FWq0W3bt3E/v37zfvuvfdeMWrUKIv269atEy1atBBqtVq0bdtW/Prrrxb7TSaTePvtt4WPj4/QaDSiT58+4tSpUxZtMjMzxfDhw4WLi4twc3MTo0ePFrm5ueb9o0aNEgBueNx7772V7pfBYBAAhMFgqPw3oxqMWBotAt/YJNbGJNfq6xIRUd10OPmq6Dp7mwh8Y5PoNOt3EX0uU+6S6rTKfn7LPk+UPZNrnqj75+9EYkY+Vo/rjtC776q11yUioror1VCIcSsO4thFAxyUEt4bGIwhXeW5i7yus4l5oqj6GU0CF66WnarlunlERFTOV/evAec/HMW7vxxHidEkd2k2iyHKzlw2FKDEKKBWKuDjppW7HCIiqkP+PeB82d7zeHpJNDLyimSuzDYxRNmZ8gGDjTwdoVRIMldDRER1TfmA80VPh8BZrUR0Yhb6f7oHR1Ky5S7N5jBE2ZlkTm9ARESV0LedL36afA+aNnDGZUMhnvwyihNzWokhys4kMUQREVElNfN2xcZJ9yCstQ+KS014/fujeHtjPIpLOU6qMhii7AzPRBERkTXctA5YPCIEL4e1AAB8uz8JT321H+m5hTJXVvcxRNkZzlZORETWUigkTAlrjqWjusBVo8LBpKt4dMEexCbV/Qmm5cQQZWfK180L5Lp5RERkpT6tffDT5HvQ3NsF6blFGPrlfiz58xw4pWTFGKLsiOFaCQwFZesiBXDJFyIiqoKmDVzw46R70K+9H0pNArN/PYEJ38aaP1/oOoYoO1I+HqqBqwZOapXM1RARka1y0ajw2fBOmDWgLRyUEn5PSMOjn/6JYxcMcpdWpzBE2REOKiciouoiSRJGhjbB98/1QCMPR6RkFWDwF/vw3f4kXt77G0OUHUnKygcABDJEERFRNekQ4I5fX+hVNg2C0YS3NsZjyprDyCsqlbs02TFE2ZHyO/MCGKKIiKga6Zwc8NXIEEx/pBWUCgk/H7mExz7bg4RLOXKXJiuGKDvCy3lERFRTJEnC+N53Y+347vB10+LclXwMXLgXy/Ym1tvLewxRduT69AYMUUREVDO6NPHEry/2RJ9W3ig2mvDuLwkY881BZNbDRYwZouxEidGES9kFAHgmioiIatZdLhosGdUF7z7WFmqVAttPpqPvJ3/iz9NX5C6tVjFE2YmLVwtgEoDWQYEGrhq5yyEiIjsnSRJG9WiCnyaVTc55JbcII5bGYM7mE/Vm7T2GKDvxz/FQkiTJXA0REdUXrf3c8PPknojQNwYAfLn7HAZ/sQ/nruTJXFnNY4iyE0nmEMXlXoiIqHY5qpV47/FgfDkiBO5ODjh20YBHFvyJ5XsTYTLZ76Bzhig7wYWHiYhIbuFtfbFlSi/0bOaFwhIT3vklAU8vjcbFv8fs2huGKDuRlFk20WZjrplHREQy8tM5YsWz3TBrQFtoHRTYdzYTfT/aje9jL9jdVAgMUXYiOass5Qfexct5REQkL4WibMmYLVN6o1Njd+QWleLV9Ucw4dtYZNjRVAgMUXZACMHZyomIqM4J8nLG+gmheC28pXkh4/CPdmNr/GW5S6sWDFF2ICu/GHlFpZAkoJEHL+cREVHdoVIqMOn+ZvhpUk+08nVFZn4xnvvuECZ+F4v03EK5y7sjDFF2oHx6A183LbQOSpmrISIiulEbfzf8NPkeTLr/bigVErbEpyLsw11YdzDFZsdKMUTZAa6ZR0REtkCjUuK18Fb4efI9aNfQDTmFpXj9+6MYsTQGyX8vXWZLGKLsQPkPHkMUERHZgrb+Omx8/h5Me7gVNCoF9pzJQPjHu7Hkz3Mw2tC8UgxRdiCJZ6KIiMjGqJQKTLj3bvz2Um90b+qJghIjZv96AoO+2If4iwa5y6sUhig7YL6cdxdDFBER2ZYmXs5YNbY75gwKhqtGhSMp2Xjssz2Y8VM8DAUlcpd3SwxRdoCzlRMRkS1TKCQM79YYf7xyLx7r4A+TAFZEJaHPhzux4VDdnaSTIcrGFZYYkZpTdosoJ9okIiJb5uOmxYLhnbByrB5NGzgjI68YU9cdwdDF+3EqNVfu8m7AEGXjLlwtgBCAi0YFDycHucshIiK6Y/c088LWKb3xWnhLaB0UiEnMQr8Ff+K9XxOQW1h3LvExRNm45KzyNfOcIEmSzNUQERFVD7WqbJLOP6bei4fa+KDUJPDVn4m4f/5OrIlJrhN38TFE2ThOb0BERPaskYcTFo/sgmXPdEVTr7JLfG9uOIbHPtuD6HOZstbGEGXjyqc3COSdeUREZMfub+WNrS/1xlv9WsNVq8LxSzkYuni/rNMhqGR7ZaoWXHiYiIjqC7VKgbG9muLxTg3x4ba/kJ5TiHYNdbLVwxBl47jkCxER1Td3uWjw38eDZR8Xxct5NkwIYQ5RvJxHRET1jVIh7w1VDFE27EpuEQpLTFAqJPi7O8pdDhERUb3CEGXDygeV+7tr4aDkW0lERFSb+Mlrwzi9ARERkXwYomxYknlQOZd7ISIiqm0MUTaMCw8TERHJhyHKhnF6AyIiIvkwRNmwpExOb0BERCQXhigbda24FBl5RQA4WzkREZEcGKJsVPmlPHcnB+gcHWSuhoiIqP5hiLJRnN6AiIhIXgxRNoqDyomIiOTFEGWjGKKIiIjkxRBloxiiiIiI5MUQZaPMY6I4vQEREZEsGKJskNEkcOFqAQCeiSIiIpILQ5QNSs0pRLHRBAelBD+do9zlEBER1UsMUTao/FJeIw8nKBWSzNUQERHVTwxRNig5Kx8AL+URERHJqU6EqIULF6JJkybQarXQ6/WIiYm5Zfv169ejVatW0Gq1CA4OxubNmy32CyEwY8YM+Pn5wdHREWFhYTh9+rRFm6ysLERERMDNzQ3u7u4YM2YM8vLyLNocPXoUvXr1glarRUBAAObNm1c9Hb5DvDOPiIhIfrKHqLVr12Lq1KmYOXMmDh06hA4dOiA8PBzp6ekVtt+3bx+GDx+OMWPGIC4uDgMHDsTAgQMRHx9vbjNv3jwsWLAAixYtQnR0NJydnREeHo7CwkJzm4iICBw/fhzbtm3Dpk2bsHv3bowfP968PycnBw899BACAwMRGxuLDz74AO+88w4WL15cc9+MSuLCw0RERHWAkFm3bt3EpEmTzF8bjUbh7+8v5syZU2H7IUOGiH79+lls0+v1YsKECUIIIUwmk/D19RUffPCBeX92drbQaDRi9erVQgghEhISBABx4MABc5stW7YISZLExYsXhRBCfP7558LDw0MUFRWZ27zxxhuiZcuWle6bwWAQAITBYKj0MZXx2Kd/isA3Nomt8Zer9XmJiIio8p/fsp6JKi4uRmxsLMLCwszbFAoFwsLCEBUVVeExUVFRFu0BIDw83Nw+MTERqampFm10Oh30er25TVRUFNzd3dGlSxdzm7CwMCgUCkRHR5vb9O7dG2q12uJ1Tp06hatXr1ZYW1FREXJyciweNYGX84iIiOQna4jKyMiA0WiEj4+PxXYfHx+kpqZWeExqauot25f/ebs23t7eFvtVKhU8PT0t2lT0HP98jX+bM2cOdDqd+REQEFBxx+9AQbERalXZ28YQRUREJB/Zx0TZk2nTpsFgMJgfKSkp1f4ajmoloqeH4eR/+sJZo6r25yciIqLKkTVEeXl5QalUIi0tzWJ7WloafH19KzzG19f3lu3L/7xdm38PXC8tLUVWVpZFm4qe45+v8W8ajQZubm4Wj5qidVDW2HMTERHR7ckaotRqNUJCQhAZGWneZjKZEBkZidDQ0AqPCQ0NtWgPANu2bTO3DwoKgq+vr0WbnJwcREdHm9uEhoYiOzsbsbGx5jbbt2+HyWSCXq83t9m9ezdKSkosXqdly5bw8PC4w54TERGRzaulge43tWbNGqHRaMTy5ctFQkKCGD9+vHB3dxepqalCCCFGjBgh3nzzTXP7vXv3CpVKJebPny9OnDghZs6cKRwcHMSxY8fMbebOnSvc3d3FTz/9JI4ePSoGDBgggoKCREFBgblN3759RadOnUR0dLTYs2ePaN68uRg+fLh5f3Z2tvDx8REjRowQ8fHxYs2aNcLJyUl8+eWXle5bTd2dR0RERDWnsp/fsocoIYT49NNPRePGjYVarRbdunUT+/fvN++79957xahRoyzar1u3TrRo0UKo1WrRtm1b8euvv1rsN5lM4u233xY+Pj5Co9GIPn36iFOnTlm0yczMFMOHDxcuLi7Czc1NjB49WuTm5lq0OXLkiOjZs6fQaDSiYcOGYu7cuVb1iyGKiIjI9lT281sSQgh5z4XZr5ycHOh0OhgMhhodH0VERETVp7Kf37w7j4iIiKgKGKKIiIiIqoAhioiIiKgKGKKIiIiIqoAhioiIiKgKGKKIiIiIqoAhioiIiKgKGKKIiIiIqoAhioiIiKgKVHIXYM/KJ4PPycmRuRIiIiKqrPLP7dst6sIQVYNyc3MBAAEBATJXQkRERNbKzc2FTqe76X6unVeDTCYTLl26BFdXV0iSVG3Pm5OTg4CAAKSkpNjlmnz23j/A/vto7/0D7L+P7J/ts/c+1mT/hBDIzc2Fv78/FIqbj3zimagapFAo0KhRoxp7fjc3N7v8xShn7/0D7L+P9t4/wP77yP7ZPnvvY03171ZnoMpxYDkRERFRFTBEEREREVUBQ5QN0mg0mDlzJjQajdyl1Ah77x9g/3209/4B9t9H9s/22Xsf60L/OLCciIiIqAp4JoqIiIioChiiiIiIiKqAIYqIiIioChiiiIiIiKqAIcoGLVy4EE2aNIFWq4Ver0dMTIzcJd3gnXfegSRJFo9WrVqZ9xcWFmLSpEm466674OLigsGDByMtLc3iOZKTk9GvXz84OTnB29sbr732GkpLSy3a7Ny5E507d4ZGo0GzZs2wfPnyGunP7t270b9/f/j7+0OSJGzcuNFivxACM2bMgJ+fHxwdHREWFobTp09btMnKykJERATc3Nzg7u6OMWPGIC8vz6LN0aNH0atXL2i1WgQEBGDevHk31LJ+/Xq0atUKWq0WwcHB2Lx5c6308ZlnnrnhPe3bt6/N9HHOnDno2rUrXF1d4e3tjYEDB+LUqVMWbWrz57K6f48r07/77rvvhvfwueees4n+ffHFF2jfvr15YsXQ0FBs2bLFvN+W37vK9tGW37+KzJ07F5Ik4aWXXjJvs7n3UZBNWbNmjVCr1eLrr78Wx48fF+PGjRPu7u4iLS1N7tIszJw5U7Rt21ZcvnzZ/Lhy5Yp5/3PPPScCAgJEZGSkOHjwoOjevbvo0aOHeX9paalo166dCAsLE3FxcWLz5s3Cy8tLTJs2zdzm3LlzwsnJSUydOlUkJCSITz/9VCiVSrF169Zq78/mzZvF//3f/4kNGzYIAOLHH3+02D937lyh0+nExo0bxZEjR8Rjjz0mgoKCREFBgblN3759RYcOHcT+/fvFn3/+KZo1ayaGDx9u3m8wGISPj4+IiIgQ8fHxYvXq1cLR0VF8+eWX5jZ79+4VSqVSzJs3TyQkJIi33npLODg4iGPHjtV4H0eNGiX69u1r8Z5mZWVZtKnLfQwPDxfLli0T8fHx4vDhw+KRRx4RjRs3Fnl5eeY2tfVzWRO/x5Xp37333ivGjRtn8R4aDAab6N/PP/8sfv31V/HXX3+JU6dOienTpwsHBwcRHx8vhLDt966yfbTl9+/fYmJiRJMmTUT79u3FlClTzNtt7X1kiLIx3bp1E5MmTTJ/bTQahb+/v5gzZ46MVd1o5syZokOHDhXuy87OFg4ODmL9+vXmbSdOnBAARFRUlBCi7ANdoVCI1NRUc5svvvhCuLm5iaKiIiGEEK+//rpo27atxXMPHTpUhIeHV3NvLP07YJhMJuHr6ys++OAD87bs7Gyh0WjE6tWrhRBCJCQkCADiwIED5jZbtmwRkiSJixcvCiGE+Pzzz4WHh4e5f0II8cYbb4iWLVuavx4yZIjo16+fRT16vV5MmDChRvsoRFmIGjBgwE2PsbU+pqenCwBi165dQoja/bmsjd/jf/dPiLIP4X9+YP2bLfVPCCE8PDzEkiVL7O69q6iPQtjP+5ebmyuaN28utm3bZtEnW3wfeTnPhhQXFyM2NhZhYWHmbQqFAmFhYYiKipKxsoqdPn0a/v7+aNq0KSIiIpCcnAwAiI2NRUlJiUU/WrVqhcaNG5v7ERUVheDgYPj4+JjbhIeHIycnB8ePHze3+edzlLep7e9FYmIiUlNTLWrR6XTQ6/UW/XF3d0eXLl3MbcLCwqBQKBAdHW1u07t3b6jVanOb8PBwnDp1ClevXjW3kbPPO3fuhLe3N1q2bImJEyciMzPTvM/W+mgwGAAAnp6eAGrv57K2fo//3b9yK1euhJeXF9q1a4dp06bh2rVr5n220j+j0Yg1a9YgPz8foaGhdvfeVdTHcvbw/k2aNAn9+vW7oQ5bfB+5ALENycjIgNFotPjhAQAfHx+cPHlSpqoqptfrsXz5crRs2RKXL1/Gu+++i169eiE+Ph6pqalQq9Vwd3e3OMbHxwepqakAgNTU1Ar7Wb7vVm1ycnJQUFAAR0fHGuqdpfJ6Kqrln7V6e3tb7FepVPD09LRoExQUdMNzlO/z8PC4aZ/Ln6Mm9e3bF4MGDUJQUBDOnj2L6dOn4+GHH0ZUVBSUSqVN9dFkMuGll17CPffcg3bt2plfvzZ+Lq9evVrjv8cV9Q8AnnrqKQQGBsLf3x9Hjx7FG2+8gVOnTmHDhg020b9jx44hNDQUhYWFcHFxwY8//og2bdrg8OHDdvPe3ayPgO2/fwCwZs0aHDp0CAcOHLhhny3+DjJEUY14+OGHzX9v37499Ho9AgMDsW7duloLN1S9hg0bZv57cHAw2rdvj7vvvhs7d+5Enz59ZKzMepMmTUJ8fDz27Nkjdyk14mb9Gz9+vPnvwcHB8PPzQ58+fXD27FncfffdtV2m1Vq2bInDhw/DYDDg+++/x6hRo7Br1y65y6pWN+tjmzZtbP79S0lJwZQpU7Bt2zZotVq5y6kWvJxnQ7y8vKBUKm+4UyEtLQ2+vr4yVVU57u7uaNGiBc6cOQNfX18UFxcjOzvbos0/++Hr61thP8v33aqNm5tbrQa18npu9b74+voiPT3dYn9paSmysrKqpc9yvP9NmzaFl5cXzpw5Y67NFvo4efJkbNq0CTt27ECjRo3M22vr57Kmf49v1r+K6PV6ALB4D+ty/9RqNZo1a4aQkBDMmTMHHTp0wCeffGI3792t+lgRW3v/YmNjkZ6ejs6dO0OlUkGlUmHXrl1YsGABVCoVfHx8bO59ZIiyIWq1GiEhIYiMjDRvM5lMiIyMtLhmXhfl5eXh7Nmz8PPzQ0hICBwcHCz6cerUKSQnJ5v7ERoaimPHjll8KG/btg1ubm7mU9uhoaEWz1Hepra/F0FBQfD19bWoJScnB9HR0Rb9yc7ORmxsrLnN9u3bYTKZzP8QhoaGYvfu3SgpKTG32bZtG1q2bAkPDw9zm7rQZwC4cOECMjMz4efnZ66tLvdRCIHJkyfjxx9/xPbt22+4rFhbP5c19Xt8u/5V5PDhwwBg8R7W1f5VxGQyoaioyObfu8r0sSK29v716dMHx44dw+HDh82PLl26ICIiwvx3m3sfrRqGTrJbs2aN0Gg0Yvny5SIhIUGMHz9euLu7W9ypUBe88sorYufOnSIxMVHs3btXhIWFCS8vL5Geni6EKLuNtXHjxmL79u3i4MGDIjQ0VISGhpqPL7+N9aGHHhKHDx8WW7duFQ0aNKjwNtbXXntNnDhxQixcuLDGpjjIzc0VcXFxIi4uTgAQ//vf/0RcXJxISkoSQpRNceDu7i5++ukncfToUTFgwIAKpzjo1KmTiI6OFnv27BHNmze3uP0/Oztb+Pj4iBEjRoj4+HixZs0a4eTkdMPt/yqVSsyfP1+cOHFCzJw5s9qmOLhVH3Nzc8Wrr74qoqKiRGJiovjjjz9E586dRfPmzUVhYaFN9HHixIlCp9OJnTt3Wtwifu3aNXOb2vq5rInf49v178yZM2LWrFni4MGDIjExUfz000+iadOmonfv3jbRvzfffFPs2rVLJCYmiqNHj4o333xTSJIkfv/9dyGEbb93lemjrb9/N/PvOw5t7X1kiLJBn376qWjcuLFQq9WiW7duYv/+/XKXdIOhQ4cKPz8/oVarRcOGDcXQoUPFmTNnzPsLCgrE888/Lzw8PISTk5N4/PHHxeXLly2e4/z58+Lhhx8Wjo6OwsvLS7zyyiuipKTEos2OHTtEx44dhVqtFk2bNhXLli2rkf7s2LFDALjhMWrUKCFE2TQHb7/9tvDx8REajUb06dNHnDp1yuI5MjMzxfDhw4WLi4twc3MTo0ePFrm5uRZtjhw5Inr27Ck0Go1o2LChmDt37g21rFu3TrRo0UKo1WrRtm1b8euvv9Z4H69duyYeeugh0aBBA+Hg4CACAwPFuHHjbvgHpy73saK+AbD4manNn8vq/j2+Xf+Sk5NF7969haenp9BoNKJZs2bitddes5hnqC7379lnnxWBgYFCrVaLBg0aiD59+pgDlBC2/d5Vpo+2/v7dzL9DlK29j5IQQlh37oqIiIiIOCaKiIiIqAoYooiIiIiqgCGKiIiIqAoYooiIiIiqgCGKiIiIqAoYooiIiIiqgCGKiIiIqAoYooiIiIiqgCGKiAhAkyZN8PHHH8tdBhHZEIYoIrIpkiTd8vHOO+9U6XkPHDiA8ePH31FtiYmJeOqpp+Dv7w+tVotGjRphwIABOHnyJADg/PnzkCTJvHAsEdk2ldwFEBFZ4/Lly+a/r127FjNmzMCpU6fM21xcXMx/F0LAaDRCpbr9P3UNGjS4o7pKSkrw4IMPomXLltiwYQP8/Pxw4cIFbNmyBdnZ2Xf03ERUN/FMFBHZFF9fX/NDp9NBkiTz1ydPnoSrqyu2bNmCkJAQaDQa7NmzB2fPnsWAAQPg4+MDFxcXdO3aFX/88YfF8/77cp4kSViyZAkef/xxODk5oXnz5vj5559vWtfx48dx9uxZfP755+jevTsCAwNxzz33YPbs2ejevTsAICgoCADQqVMnSJKE++67z3z8kiVL0Lp1a2i1WrRq1Qqff/65eV/5Gaw1a9agR48e0Gq1aNeuHXbt2lUN31EiqiqGKCKyO2+++Sbmzp2LEydOoH379sjLy8MjjzyCyMhIxMXFoW/fvujfvz+Sk5Nv+TzvvvsuhgwZgqNHj+KRRx5BREQEsrKyKmzboEEDKBQKfP/99zAajRW2iYmJAQD88ccfuHz5MjZs2AAAWLlyJWbMmIH33nsPJ06cwH//+1+8/fbb+OabbyyOf+211/DKK68gLi4OoaGh6N+/PzIzM6399hBRdRFERDZq2bJlQqfTmb/esWOHACA2btx422Pbtm0rPv30U/PXgYGB4qOPPjJ/DUC89dZb5q/z8vIEALFly5abPudnn30mnJychKurq7j//vvFrFmzxNmzZ837ExMTBQARFxdncdzdd98tVq1aZbHtP//5jwgNDbU4bu7cueb9JSUlolGjRuL999+/bV+JqGbwTBQR2Z0uXbpYfJ2Xl4dXX30VrVu3hru7O1xcXHDixInbnolq3769+e/Ozs5wc3NDenr6TdtPmjQJqampWLlyJUJDQ7F+/Xq0bdsW27Ztu+kx+fn5OHv2LMaMGQMXFxfzY/bs2Th79qxF29DQUPPfVSoVunTpghMnTtyyD0RUcziwnIjsjrOzs8XXr776KrZt24b58+ejWbNmcHR0xBNPPIHi4uJbPo+Dg4PF15IkwWQy3fIYV1dX9O/fH/3798fs2bMRHh6O2bNn48EHH6ywfV5eHgDgq6++gl6vt9inVCpv+VpEJC+eiSIiu7d3714888wzePzxxxEcHAxfX1+cP3++xl9XkiS0atUK+fn5AAC1Wg0AFmOmfHx84O/vj3PnzqFZs2YWj/KB6OX2799v/ntpaSliY2PRunXrGu8HEVWMZ6KIyO41b94cGzZsQP/+/SFJEt5+++3bnlGy1uHDhzFz5kyMGDECbdq0gVqtxq5du/D111/jjTfeAAB4e3vD0dERW7duRaNGjaDVaqHT6fDuu+/ixRdfhE6nQ9++fVFUVISDBw/i6tWrmDp1qvk1Fi5ciObNm6N169b46KOPcPXqVTz77LPV2g8iqjyGKCKye//73//w7LPPokePHvDy8sIbb7yBnJycan2NRo0aoUmTJnj33XfNUxKUf/3yyy8DKBvHtGDBAsyaNQszZsxAr169sHPnTowdOxZOTk744IMP8Nprr8HZ2RnBwcF46aWXLF5j7ty5mDt3Lg4fPoxmzZrh559/hpeXV7X2g4gqTxJCCLmLICKimzt//jyCgoIQFxeHjh07yl0OEf2NY6KIiIiIqoAhioiIiKgKeDmPiIiIqAp4JoqIiIioChiiiIiIiKqAIYqIiIioChiiiIiIiKqAIYqIiIioChiiiIiIiKqAIYqIiIioChiiiIiIiKrg/wHnqtlOKAzxRAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class WarmupCosineDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, max_lr, warmup_steps, total_steps):\n",
    "        super().__init__()\n",
    "        self.max_lr = max_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        \n",
    "        # Linear Warm-up\n",
    "        warmup_lr = self.max_lr * (step / self.warmup_steps)\n",
    "        \n",
    "        # Cosine Decay (excluding the warm-up phase)\n",
    "        decay_steps = tf.maximum(step - self.warmup_steps, 0)\n",
    "        remaining_steps = self.total_steps - self.warmup_steps\n",
    "        cosine_decay = 0.5 * (1 + tf.math.cos(np.pi * decay_steps / remaining_steps))\n",
    "        decayed_lr = self.max_lr * cosine_decay\n",
    "        \n",
    "        return tf.where(step < self.warmup_steps, warmup_lr, decayed_lr)\n",
    "\n",
    "# Example usage\n",
    "max_lr = 0.001\n",
    "warmup_steps = 4000\n",
    "total_steps = steps_per_epoch * epochs\n",
    "seq2seq_schedule = WarmupCosineDecaySchedule(max_lr, warmup_steps, total_steps)\n",
    "\n",
    "plt.plot(seq2seq_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Train Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7769ff4-9b7c-4cdc-a2d1-58feb5186532",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "69a6c8d9-e961-436d-9d62-8743ea5c80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(seq2seq_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e1b4b-3765-42c6-874e-d491832e76d5",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6e05ffa5-8fa5-4177-b727-d5a50a2964a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != pad_token_idx\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != pad_token_idx\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ec20b8c5-a458-43a6-a62f-b49febbf6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "908d1e3c-7a66-45f9-834b-aeab590c323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f'{model.name}_weights_checkpoint.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "48d23bc3-d78b-4a24-839a-478e88686817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2.16953, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1603s - loss: 2.6940 - masked_accuracy: 0.2387 - val_loss: 2.1695 - val_masked_accuracy: 0.3466 - 1603s/epoch - 1s/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.16953 to 1.85601, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1515s - loss: 2.0235 - masked_accuracy: 0.3937 - val_loss: 1.8560 - val_masked_accuracy: 0.4448 - 1515s/epoch - 952ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 1.85601 to 1.66784, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1562s - loss: 1.7887 - masked_accuracy: 0.4668 - val_loss: 1.6678 - val_masked_accuracy: 0.5017 - 1562s/epoch - 982ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 1.66784 to 1.49657, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1558s - loss: 1.6054 - masked_accuracy: 0.5228 - val_loss: 1.4966 - val_masked_accuracy: 0.5528 - 1558s/epoch - 979ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.49657 to 1.40392, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1590s - loss: 1.4815 - masked_accuracy: 0.5591 - val_loss: 1.4039 - val_masked_accuracy: 0.5806 - 1590s/epoch - 1000ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.40392 to 1.32722, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1558s - loss: 1.3931 - masked_accuracy: 0.5848 - val_loss: 1.3272 - val_masked_accuracy: 0.6026 - 1558s/epoch - 979ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.32722 to 1.26387, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1562s - loss: 1.3256 - masked_accuracy: 0.6041 - val_loss: 1.2639 - val_masked_accuracy: 0.6209 - 1562s/epoch - 981ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.26387 to 1.20931, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1557s - loss: 1.2711 - masked_accuracy: 0.6202 - val_loss: 1.2093 - val_masked_accuracy: 0.6365 - 1557s/epoch - 978ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.20931 to 1.17807, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1564s - loss: 1.2231 - masked_accuracy: 0.6344 - val_loss: 1.1781 - val_masked_accuracy: 0.6454 - 1564s/epoch - 983ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.17807 to 1.13540, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1591s - loss: 1.1822 - masked_accuracy: 0.6456 - val_loss: 1.1354 - val_masked_accuracy: 0.6585 - 1591s/epoch - 1000ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.13540 to 1.10339, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1569s - loss: 1.1434 - masked_accuracy: 0.6570 - val_loss: 1.1034 - val_masked_accuracy: 0.6663 - 1569s/epoch - 986ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 1.10339 to 1.06816, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1566s - loss: 1.1074 - masked_accuracy: 0.6673 - val_loss: 1.0682 - val_masked_accuracy: 0.6769 - 1566s/epoch - 984ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 1.06816 to 1.03788, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1583s - loss: 1.0736 - masked_accuracy: 0.6771 - val_loss: 1.0379 - val_masked_accuracy: 0.6849 - 1583s/epoch - 995ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss improved from 1.03788 to 1.00276, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1593s - loss: 1.0386 - masked_accuracy: 0.6868 - val_loss: 1.0028 - val_masked_accuracy: 0.6957 - 1593s/epoch - 1s/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 1.00276 to 0.98173, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1597s - loss: 1.0033 - masked_accuracy: 0.6969 - val_loss: 0.9817 - val_masked_accuracy: 0.7017 - 1597s/epoch - 1s/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.98173 to 0.94147, saving model to seq2_seq_recurrent_3_weights_checkpoint.h5\n",
      "1591/1591 - 1586s - loss: 0.9689 - masked_accuracy: 0.7072 - val_loss: 0.9415 - val_masked_accuracy: 0.7135 - 1586s/epoch - 997ms/step\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_ds,\n",
    "                validation_data=valid_ds,\n",
    "                epochs=epochs,\n",
    "                callbacks=[model_checkpoint_callback],\n",
    "                verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b6bb1a77-a4ca-4780-8494-1589a4c199e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual inference on test input\n",
    "test_inp = np.zeros((1, FRAME_LEN, lm_shape))\n",
    "test_ctx = np.array([[start_token_idx]])\n",
    "logits = model((test_inp, test_ctx))\n",
    "pred_idx = np.argmax(tf.nn.softmax(logits)[0][0])\n",
    "num_to_char[pred_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae568f75-c578-4e32-a203-9bd9bc31089d",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "9830e3a4-b7c0-490a-847c-bd188364efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_chars = tf.constant(list(char_to_num.keys()), dtype=tf.string)\n",
    "tf_nums = tf.constant(list(char_to_num.values()), dtype=tf.int64)\n",
    "\n",
    "class EncoderDecoderModelWrapper(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(EncoderDecoderModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.save_name = self.model.name\n",
    "        \n",
    "        self.tf_char_to_num = tf.lookup.StaticHashTable(\n",
    "            initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=tf_chars,\n",
    "                values=tf_nums,\n",
    "            ),\n",
    "            default_value=tf.constant(-1, dtype=tf.int64),\n",
    "            name=\"tf_char_to_num_lut\"\n",
    "        )\n",
    "        \n",
    "        self.tf_num_to_char = tf.lookup.StaticHashTable(\n",
    "            initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "                keys=tf_nums,\n",
    "                values=tf_chars,\n",
    "            ),\n",
    "            default_value=tf.constant(\"unknown\", dtype=tf.string),\n",
    "            name=\"tf_num_to_char_lut\"\n",
    "        )\n",
    "\n",
    "        self.ctx_len = tf.constant(self.model.max_future_input_size, dtype=tf.int32)\n",
    "\n",
    "    @tf.function()\n",
    "    def info(self):\n",
    "        return tf.constant(FEATURE_COLUMNS)\n",
    "\n",
    "    # @tf.function(input_signature=[\n",
    "    #     tf.TensorSpec(shape=[tf.constant(FRAME_LEN), tf.constant(lm_shape)], dtype=tf.float32),\n",
    "    #     tf.TensorSpec(shape=[], dtype=tf.string), # The input string is trimmed to max size set when initializing model\n",
    "    # ])\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None, tf.constant(len(FEATURE_COLUMNS))], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.string), # The input string is trimmed to max size set when initializing model\n",
    "    ])\n",
    "    def predict(self, landmarks, ctx):\n",
    "        # Prepare input\n",
    "        # Trim context to not exceed max size\n",
    "        ctx_length = tf.strings.length(ctx)\n",
    "        starting_pos = tf.maximum(ctx_length - self.ctx_len, 0)\n",
    "        ctx = tf.strings.substr(ctx, starting_pos, self.ctx_len)\n",
    "\n",
    "        ctx_chars = tf.strings.unicode_split(ctx, input_encoding='UTF-8')\n",
    "        ctx_tokens = self.tf_char_to_num.lookup(ctx_chars)\n",
    "        batched_ctx_tokens = tf.expand_dims(ctx_tokens, axis=0) # Adds first (\"batch\") dimension to the tensor\n",
    "\n",
    "        landmarks = preprocess_landmark_a(landmarks)\n",
    "        batched_landmarks = tf.expand_dims(landmarks, axis=0)\n",
    "        \n",
    "        # Inference\n",
    "        logits = self.model((batched_landmarks, batched_ctx_tokens), training=False)\n",
    "        logits = logits[:, -1:, :][0][0] # Select the last element in the middle dimension (the first is the batch dim, the last is the num_of_classes dim)\n",
    "        \n",
    "        # Parse result\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        pred_prob = tf.reduce_max(probabilities)\n",
    "        pred_idx = tf.argmax(probabilities)\n",
    "        pred_char = self.tf_num_to_char.lookup(pred_idx)\n",
    "        \n",
    "        return {'result' : pred_char, 'confidence': pred_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8d270af3-e942-4bce-9103-63ea511aa3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = EncoderDecoderModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "429e7979-2e1c-4da6-9785-c6bccc32dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.45 s\n",
      "Wall time: 1.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'result': <tf.Tensor: shape=(), dtype=string, numpy=b'h'>,\n",
       " 'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.55950594>}"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "wrapped_model.predict(np.zeros((FRAME_LEN, len(FEATURE_COLUMNS))), \"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46554d-b85a-427b-b8fc-6d8dfe97c4b0",
   "metadata": {},
   "source": [
    "Note: first time execution is usually slower than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "6d675d6f-b90d-4c1e-9390-0ff87bf4bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: seq2_seq_recurrent_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: seq2_seq_recurrent_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A model with the name (seq2_seq_recurrent_3) was just saved!\n"
     ]
    }
   ],
   "source": [
    "save_model_name = wrapped_model.save_name\n",
    "if os.path.isdir(save_model_name):\n",
    "    print(f\"A model with the same name ({save_model_name}) has already been saved!\")\n",
    "else:\n",
    "    tf.saved_model.save(wrapped_model, export_dir=save_model_name)\n",
    "    print(f\"A model with the name ({save_model_name}) was just saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5134bee-834e-4c51-9c0e-6171544801e7",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19523338-3c1c-42bc-aebf-a2ccdb6f7587",
   "metadata": {},
   "source": [
    "## From saved weights\n",
    "\n",
    "1. Create model object with same parameters\n",
    "2. Build the model (run inference on it, to initialize the parameters)\n",
    "3. Load the weights\n",
    "4. Convert it to tf.Module or run manual inference on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "734e396a-eb2b-40ae-9499-366f0238fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dd880874-8945-43e7-9784-b8a614af2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 108)\n",
      "(32, 33)\n",
      "(32, 33, 62)\n",
      "Model: \"seq2_seq_recurrent_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_3 (Land  multiple                  1746688   \n",
      " markEmbedding)                                                  \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " positional_token_embedding  multiple                  15872     \n",
      " _27 (PositionalTokenEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " gru_26 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_27 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_28 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " gru_29 (GRU)                multiple                  394752    \n",
      "                                                                 \n",
      " additive_attention_13 (Add  multiple                  0 (unused)\n",
      " itiveAttention)                                                 \n",
      "                                                                 \n",
      " dense_27 (Dense)            multiple                  15934     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3357502 (12.81 MB)\n",
      "Trainable params: 3357502 (12.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example inference to build the model\n",
    "(lm, ctx), _label = next(iter(train_ds))\n",
    "output = new_model((lm, ctx))\n",
    "\n",
    "print(lm.shape)\n",
    "print(ctx.shape)\n",
    "print(output.shape)\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a41aa238-925b-48c6-bc42-b4a213ed79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights(\"model_weights_checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fdfde474-15c5-4d1b-949c-a432aeb15c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual inference on test input\n",
    "test_inp = np.zeros((1, FRAME_LEN, lm_shape))\n",
    "test_ctx = np.array([[start_token_idx]])\n",
    "logits = new_model((test_inp, test_ctx), training=False)\n",
    "logits = logits[:, -1:, :][0][0]\n",
    "pred_idx = np.argmax(tf.nn.softmax(logits))\n",
    "num_to_char[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "56c067e6-32fd-437a-b659-fa69ff208da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = FingerSpellingTransformer(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d174c620-c2dc-493a-9e2b-66fd05cc4b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': <tf.Tensor: shape=(), dtype=string, numpy=b'm'>,\n",
       " 'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.09073451>}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(np.zeros((FRAME_LEN, lm_shape)), \"<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd57bf-2295-4854-a91f-dcdf00747d5f",
   "metadata": {},
   "source": [
    "## From saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9eca4e3f-2ee8-44a1-bcd7-9551ab17caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.saved_model.load(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b6b88b3d-b223-41ff-adac-70083196180b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': <tf.Tensor: shape=(), dtype=string, numpy=b'_'>,\n",
       " 'confidence': <tf.Tensor: shape=(), dtype=float32, numpy=0.02006547>}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(np.zeros((FRAME_LEN, lm_shape)), \"<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc4af1",
   "metadata": {},
   "source": [
    "## On Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bde7f632-d38d-43d4-821e-8e4a5c9ff9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(fs_model, inp, max_len):\n",
    "    ctx = str(num_to_char[start_token_idx])\n",
    "    for i in range(max_len):\n",
    "        res = fs_model.predict(inp, ctx)\n",
    "        res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        ctx += res_char\n",
    "\n",
    "        if res_char == num_to_char[end_token_idx]:\n",
    "            break\n",
    "    return ctx\n",
    "\n",
    "def generate_teacher_forcing(fs_model, inp, expected):\n",
    "    pred = str(num_to_char[start_token_idx])\n",
    "    ctx = str(num_to_char[start_token_idx])\n",
    "    for e in expected:\n",
    "        if e == 'P':\n",
    "            break\n",
    "        res = fs_model.predict(inp, ctx)\n",
    "        res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        pred += res_char\n",
    "        ctx += e\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "61f60471-b3c1-43f9-9378-e76071361a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: golfcourseweddings>PPPPPPPPPPPPP\n",
      "Gen on own: <+61-05-206-18>\n",
      "Gen teacher forcing: <+obf-rtro wodey>g..\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 9396 yuchi lane>PPPPPPPPPPPPPPPP\n",
      "Gen on own: <93396 yuchi lane>\n",
      "Gen teacher forcing: <9336 yuchi lane>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 506-993-4525>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <506-993-4525>\n",
      "Gen teacher forcing: <506-993-4525>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 428823 winter house>PPPPPPPPPPPP\n",
      "Gen on own: <428823 winter horse>\n",
      "Gen teacher forcing: <428823 winter horse>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: america_2>PPPPPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <anmrica.12>\n",
      "Gen teacher forcing: <anrrica.1>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 628-137-7392>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <628-137-7392>\n",
      "Gen teacher forcing: <628-137-7392>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 370 ivy spring>PPPPPPPPPPPPPPPPP\n",
      "Gen on own: <370 vys pring>\n",
      "Gen teacher forcing: <370 vvy spring>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 889-022-9065>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <889-022-9065>\n",
      "Gen teacher forcing: <889-022-9065>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: ronnie simon>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <randie orraie>\n",
      "Gen teacher forcing: <rarnee>oteent\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: oscar lowery>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <scarlowerry>\n",
      "Gen teacher forcing: <sscarllowerr>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 98 delanna felder>PPPPPPPPPPPPPP\n",
      "Gen on own: <98 deland felde r>\n",
      "Gen teacher forcing: <98 delandaffelde >\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 555 sam hill mine ct>PPPPPPPPPPP\n",
      "Gen on own: <55 samhill mine ct>\n",
      "Gen teacher forcing: <55  samhhill mine ct>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 6559 circle west>PPPPPPPPPPPPPPP\n",
      "Gen on own: <6599 circlwest>\n",
      "Gen teacher forcing: <65999circlwwwest>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: yarzarmg.blogspot.com/ecobonus>P\n",
      "Gen on own: <rzartalana.blogspot.com.ar>\n",
      "Gen teacher forcing: <rrrzanaabblogspot.com.ecobaue>>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 991508 alcoda>PPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <91508 aloda>\n",
      "Gen teacher forcing: <911508 alooda>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: mrjamesmay/>PPPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <mumri.mesmy/>\n",
      "Gen teacher forcing: <mumamesmyy/>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 769-110-5964>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <699-710-5964>\n",
      "Gen teacher forcing: <689-500-5964>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 864 maries valley lane>PPPPPPPPP\n",
      "Gen on own: <7645 maires dey ave>\n",
      "Gen teacher forcing: <7645maiies diraay aane>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: www.heimaslod.is>PPPPPPPPPPPPPPP\n",
      "Gen on own: <www.heimasleod.is>\n",
      "Gen teacher forcing: <www.heimasled.is>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 411-498-1536>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <410-498-1536>\n",
      "Gen teacher forcing: <410-498-1536>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 241-900-8965>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <+241-00-89-65>\n",
      "Gen teacher forcing: <+41-000-8965>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 816 shadow royal>PPPPPPPPPPPPPPP\n",
      "Gen on own: <816 roya>\n",
      "Gen teacher forcing: <816 rwolowsroy>>>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: +66-1473-055>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <+66-573-051473-05>\n",
      "Gen teacher forcing: <+66-5473-051>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: https://www.lwqshop.com>PPPPPPPP\n",
      "Gen on own: <https://www.lw.shop.com>\n",
      "Gen teacher forcing: <https://www.lw..hop.com>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 65 cbr lane>PPPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <65 br drive>\n",
      "Gen teacher forcing: <65 b r dene>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 630 daffodil ridge>PPPPPPPPPPPPP\n",
      "Gen on own: <6301 foodi prings>\n",
      "Gen teacher forcing: <6301difoo i irivge>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 3142 glasva school>PPPPPPPPPPPPP\n",
      "Gen on own: <3142 glas school>\n",
      "Gen teacher forcing: <3142 glas a school>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: www.nbkoyo.com/allenexcelldude>P\n",
      "Gen on own: <www.nexceldudenexxcem/alenexx>\n",
      "Gen teacher forcing: <www.nek/.u.com/aleenexxelloede>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 829 englewood lake>PPPPPPPPPPPPP\n",
      "Gen on own: <299 ennatl lane>\n",
      "Gen teacher forcing: <2229ennlanaod lane>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: dexter estes>PPPPPPPPPPPPPPPPPPP\n",
      "Gen on own: <eter ester>\n",
      "Gen teacher forcing: <eetter ester>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 3843 dunleavy place>PPPPPPPPPPPP\n",
      "Gen on own: <3843 dunle avillace>\n",
      "Gen teacher forcing: <3843 dunle vy place>\n",
      "\n",
      "~~~\n",
      "\n",
      "Expected: 83 private road 5334>PPPPPPPPPPP\n",
      "Gen on own: <83 prippro 2534>\n",
      "Gen teacher forcing: <83 pripatr road 5344>\n",
      "\n",
      "~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(inp_batch, _ctx), expected_batch = random.choice([batch for batch in valid_ds])\n",
    "for seq, expected in zip(inp_batch, expected_batch):\n",
    "    expected = \"\".join([num_to_char[num.numpy()] for num in expected])\n",
    "\n",
    "    print(\"Expected: \" + expected)\n",
    "    print(\"Gen on own: \" + generate(loaded_model, seq, MAX_PHRASE_LEN))\n",
    "    print(\"Gen teacher forcing: \" + generate_teacher_forcing(loaded_model, seq, expected))\n",
    "    print('\\n~~~\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c854fb4",
   "metadata": {},
   "source": [
    "## Real life testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c4cd-a892-42c2-945f-f56247be98b8",
   "metadata": {},
   "source": [
    "### Util for handling video feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e8053562-23ae-4813-878a-7d3be6bc57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_landmarks_on_image(image, results):\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        mp_holistic.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.pose_landmarks,\n",
    "        mp_holistic.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles\n",
    "        .get_default_pose_landmarks_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.left_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.right_hand_landmarks,\n",
    "        mp_holistic.HAND_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style()\n",
    "    )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f9a3dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_result(res):\n",
    "    # Extract specific pose landmarks if available\n",
    "    px = []\n",
    "    py = []\n",
    "    pz = []\n",
    "    if res.pose_landmarks:\n",
    "        for i in POSE:\n",
    "            lm = res.pose_landmarks.landmark[i]\n",
    "            px.append(lm.x)\n",
    "            py.append(lm.y)\n",
    "            pz.append(lm.z)\n",
    "    else:\n",
    "        px = [0.0]*len(POSE)\n",
    "        py = [0.0]*len(POSE)\n",
    "        pz = [0.0]*len(POSE)\n",
    "\n",
    "    # Extract left hand landmarks if available\n",
    "    lx = []\n",
    "    ly = []\n",
    "    lz = []\n",
    "    if res.left_hand_landmarks:\n",
    "        for lm in res.left_hand_landmarks.landmark:\n",
    "            lx.append(lm.x)\n",
    "            ly.append(lm.y)\n",
    "            lz.append(lm.z)\n",
    "    else:\n",
    "        lx = [0.0]*21\n",
    "        ly = [0.0]*21\n",
    "        lz = [0.0]*21\n",
    "\n",
    "    # Extract right hand landmarks if available\n",
    "    rx = []\n",
    "    ry = []\n",
    "    rz = []\n",
    "    if res.right_hand_landmarks:\n",
    "        for lm in res.right_hand_landmarks.landmark:\n",
    "            rx.append(lm.x)\n",
    "            ry.append(lm.y)\n",
    "            rz.append(lm.z)\n",
    "    else:\n",
    "        rx = [0.0]*21\n",
    "        ry = [0.0]*21\n",
    "        rz = [0.0]*21\n",
    "\n",
    "    return list(chain(rx, lx, px, ry, ly, py, rz, lz, pz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8a638eb3-c919-4c9c-9131-fdf6e4b1eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_loop(source, process_data_func):\n",
    "    video = cv2.VideoCapture(source)\n",
    "    display_handle=display(None, display_id=True)\n",
    "    try:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "            while True:\n",
    "                _, frame = video.read()\n",
    "    \n",
    "                if frame is None:\n",
    "                    break\n",
    "    \n",
    "                #image = cv2.resize(frame, (360, 240))\n",
    "                image=frame\n",
    "    \n",
    "                # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "                image.flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                data = extract_from_result(results)\n",
    "\n",
    "                process_data_func(data)\n",
    "    \n",
    "                # Draw landmark annotation on the image.\n",
    "                image = draw_landmarks_on_image(image, results)\n",
    "    \n",
    "                image = cv2.flip(image, 1)\n",
    "                _, image = cv2.imencode('.jpeg', image)\n",
    "                display_handle.update(Image(data=image.tobytes()))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        video.release()\n",
    "        display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ea833-fa1d-4703-9711-6ab969e68ad4",
   "metadata": {},
   "source": [
    "### Util for handling models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52fc59-c54b-4051-a3eb-b4bc0ac10305",
   "metadata": {},
   "source": [
    "#### Signing detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7efb8a27-d26f-4097-8398-b2ce313aeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load once\n",
    "signing_detection_model = tf.saved_model.load(\"signing_detection_model\")\n",
    "\n",
    "class SigningDetectionModel:\n",
    "    def __init__(self):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "\n",
    "    def is_signing(self, inp_lm):\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        return signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy() == 1\n",
    "\n",
    "class BufferedSigningDetectionModel:\n",
    "    def __init__(self, buffer_len=5, confidence_number=3):\n",
    "        self.signing_detection_model_input = list(np.zeros((15, 156)))\n",
    "        self.signing_detector_buffer = deque(maxlen=buffer_len)\n",
    "        self.confidence_number = confidence_number \n",
    "\n",
    "    def is_signing(self, inp_lm):\n",
    "        self.signing_detection_model_input.pop(0)\n",
    "        self.signing_detection_model_input.append(inp_lm)\n",
    "        pred = signing_detection_model.predict(self.signing_detection_model_input)[\"result\"].numpy()\n",
    "        self.signing_detector_buffer.append(pred)\n",
    "        buffered_pred, count = Counter(self.signing_detector_buffer).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            return buffered_pred == 1\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284ff73-1c6c-472d-bdda-3e0b7917fa74",
   "metadata": {},
   "source": [
    "#### Fingerspelling recognition models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c69bcc5e-1560-4d3e-be01-d385b99f1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only load once\n",
    "loaded_transformer_model = loaded_model\n",
    "\n",
    "class ContinuousRecognitionModel:\n",
    "    def __init__(self, confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7):\n",
    "        self.input = []\n",
    "        self.trust_confidence = confidence_threshold\n",
    "        self.inp_len = inp_buf_len\n",
    "        self.inner_fifo = deque(maxlen=out_buf_len)\n",
    "        self.confidence_number = out_majority_threshold\n",
    "        self.context = str(num_to_char[start_token_idx])\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        if len(self.input) >= self.inp_len:\n",
    "            self.input.pop(0)\n",
    "        self.input.append(frame)\n",
    "    \n",
    "        inp = pre_process(self.input)\n",
    "        res = loaded_transformer_model.predict(inp, self.context)\n",
    "        pred = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "        prob = res[\"confidence\"].numpy()\n",
    "\n",
    "        if prob < self.trust_confidence:\n",
    "            return\n",
    "\n",
    "        self.inner_fifo.append(pred)\n",
    "        pred_char, count = Counter(self.inner_fifo).most_common(1)[0]\n",
    "        if count >= self.confidence_number:\n",
    "            if self.context[-1] != pred_char:\n",
    "                self.context += pred_char\n",
    "                print(pred_char, end=\"\")\n",
    "                \n",
    "                # Predicted the end\n",
    "                if pred_char == '>':\n",
    "                    # restart the detection\n",
    "                    self.context = str(num_to_char[start_token_idx])\n",
    "                    self.inner_fifo.clear()\n",
    "                    self.input.clear()\n",
    "\n",
    "class NonContinuousRecognitionModel:\n",
    "    def __init__(self, max_out_length=MAX_PHRASE_LEN, max_input_length=FRAME_LEN, confidence_threshold=0.2):\n",
    "        self.max_out_length = max_out_length\n",
    "        self.max_input_length = max_input_length\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.input = []\n",
    "\n",
    "    def reset_buffer(self):\n",
    "        self.input.clear()\n",
    "\n",
    "    def translate_buffered_content(self, reset_buffer=False):\n",
    "        if len(self.input) > 0:\n",
    "            inp = pre_process(self.input)\n",
    "            print(self._generate_with_confidence(inp))\n",
    "        if reset_buffer:\n",
    "            self.reset_buffer()\n",
    "\n",
    "    def process_frame(self, inp_lm):\n",
    "        self.input.append(inp_lm)\n",
    "        \n",
    "        if len(self.input) >= FRAME_LEN:\n",
    "            self.translate_buffered_content()\n",
    "            self.reset_buffer()\n",
    "\n",
    "    def _generate_with_confidence(self, inp):\n",
    "        ctx = str(num_to_char[start_token_idx])\n",
    "        for i in range(self.max_out_length):\n",
    "            res = loaded_model.predict(inp, ctx)\n",
    "            res_char = res[\"result\"].numpy().decode(\"utf-8\")\n",
    "            prob = res[\"confidence\"].numpy()\n",
    "            if prob > self.confidence_threshold:\n",
    "                ctx += res_char\n",
    "                if res_char == num_to_char[end_token_idx]:\n",
    "                    break\n",
    "        return ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba5da1-cc4b-42b4-8fae-1d5705f0a7e8",
   "metadata": {},
   "source": [
    "### Running diffrent configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb7332-f214-4946-a80e-77f262439c3a",
   "metadata": {},
   "source": [
    "#### Continuous model without signing detection\n",
    "\n",
    "Works well for isolated sequences. Can't handle sudden pauses, and stops.\n",
    "Extremely sensitive to window size. Also, the training data was from professional signers. For beginners who sign slower the same window size isn't suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b6b6fc2b-4255-4945-9c2e-f2b4243f2087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beark"
     ]
    }
   ],
   "source": [
    "fs_model = ContinuousRecognitionModel(confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7)\n",
    "video_loop(\"test_videos/bear.mp4\", lambda data: fs_model.process_frame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df92a3-b760-46c5-8a9d-e648ea4cda71",
   "metadata": {},
   "source": [
    "#### Continuous model with signing detection\n",
    "\n",
    "Handles breaks at the start and end, but doesn't account for breaks mid-signing, or multiple words, as the model's buffer is only filled when signing is detected. This can lead to jumps in the buffer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "80ca41cf-da3e-4fb4-9b3d-e418056fe553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------*******************************--------*********a*******l*******************************i*******g*******a*******t*******o*******r*******>--------------*****"
     ]
    }
   ],
   "source": [
    "sign_detector = SigningDetectionModel()\n",
    "#sign_detector = BufferedSigningDetectionModel()\n",
    "fs_model = ContinuousRecognitionModel(confidence_threshold=0.2, inp_buf_len=30, out_buf_len=10, out_majority_threshold=7)\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        print(\"*\", end=\"\")\n",
    "        fs_model.process_frame(data)\n",
    "    else:\n",
    "        print(\"-\", end=\"\")\n",
    "\n",
    "video_loop(\"test_videos/alligator.mp4\", process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1368de0-d61c-41d7-89c2-a607b4599cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO model that takes input frames all the time but only predicts when signing is detected!!!!!\n",
    "# Continue the train of thought for the rest!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4fe0e6-c163-461a-9b79-076a9f8fec7f",
   "metadata": {},
   "source": [
    "#### Translate in long chunks\n",
    "\n",
    "This model performs well on single words that fit into the buffer. But for longer text it fails to translate well, presumable because the signs are cut off at the wrong positions. For longer text pause detection/signing detection is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "95ace07c-a3fc-49fb-85f9-8397fc8df6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<beark>\n"
     ]
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(max_out_length=MAX_PHRASE_LEN, max_input_length=FRAME_LEN, confidence_threshold=0.2)\n",
    "video_loop(\"test_videos/bear.mp4\", lambda data: fs_model.process_frame(data))\n",
    "fs_model.translate_buffered_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5602c-a076-4d9e-92c2-2559310564b1",
   "metadata": {},
   "source": [
    "#### Translate in long chunks  with signing detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "55175fc3-7a78-44f6-be9c-8da04278431e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<monkey >\n"
     ]
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(max_out_length=MAX_PHRASE_LEN, max_input_length=FRAME_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "#sign_detector = BufferedSigningDetectionModel()\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "\n",
    "video_loop(\"test_videos/monkey.mp4\", process_data)\n",
    "fs_model.translate_buffered_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb94e63-4e15-4e2c-9fd1-41e536e9d823",
   "metadata": {},
   "source": [
    "#### Translate longer sequences with stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "4aa9fd48-102b-4bb4-8d0b-4d20f81be733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<earkn>\n",
      "<\n",
      "<tigerr>\n",
      "<\n",
      "<ebraa>\n",
      "<\n",
      "<yena>\n",
      "<\n",
      "<angaroo hangaroo>\n",
      "<monkey >\n",
      "<\n",
      "<lyonn parks>\n",
      "<s\n",
      "<alligator>\n",
      "<horse horse>\n",
      "<\n"
     ]
    }
   ],
   "source": [
    "fs_model = NonContinuousRecognitionModel(max_out_length=MAX_PHRASE_LEN, max_input_length=FRAME_LEN, confidence_threshold=0.2)\n",
    "sign_detector = SigningDetectionModel()\n",
    "#sign_detector = BufferedSigningDetectionModel()\n",
    "\n",
    "def process_data(data):\n",
    "    if sign_detector.is_signing(data):\n",
    "        fs_model.process_frame(data)\n",
    "    else:\n",
    "        fs_model.translate_buffered_content(reset_buffer=True)\n",
    "\n",
    "video_loop(\"test_videos/fingerspelling_animals.mp4\", process_data)\n",
    "fs_model.translate_buffered_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29899b-a167-496c-94dd-2e1dde5d6626",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9976f-3618-4351-a12f-935d767dd921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ab6f2a40-51a1-4248-92a1-ad6e1795b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5abe44cf-87bc-4759-907a-76ed62da70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "35962a56-944c-415f-aaa7-54cac9182a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Please set a valid api key!\n"
     ]
    }
   ],
   "source": [
    "key = os.environ.get('OPEN_AI_API_KEY')\n",
    "if key is not None:\n",
    "    openai.api_key = key\n",
    "else:\n",
    "    print(\"Error: Please set a valid api key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4d1aa220-b829-4e0d-a05b-6929d0877a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_output(pred):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\", \n",
    "       messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a machine that tries to correct the output of a fingerspelling recognition model. Some letters might be missing, but it's also possible that the given text has extra characters. Only reply the corrected text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"angaro angaro\"},\n",
    "        {\"role\": \"system\", \"content\": \"kangaroo\"},\n",
    "        {\"role\": \"user\", \"content\": \"beark\"},\n",
    "        {\"role\": \"system\", \"content\": \"bear\"},\n",
    "        {\"role\": \"user\", \"content\": \"6 halee hale\"},\n",
    "        {\"role\": \"system\", \"content\": \"whale\"},\n",
    "        {\"role\": \"user\", \"content\": pred},\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a5becc88-a2f8-4cbf-9926-868725581db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earth/tiger/angary ligator alligator horse gro'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_output(\"earkh/tiger/tiger angar key ligator alligator h horse gro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a5f3b-7033-434f-8197-098396375c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493c35f-5609-4088-8039-49bcb7438ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b883a33-fae7-4e1a-901d-9402ca93db7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
