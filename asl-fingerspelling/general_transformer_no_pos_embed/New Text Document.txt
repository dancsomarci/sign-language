class BaseAttention(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__()
        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
        self.layernorm = tf.keras.layers.LayerNormalization()
        self.add = tf.keras.layers.Add()

class CrossAttention(BaseAttention):
    def call(self, x, context):
        attn_output, attn_scores = self.mha(
            query=x,
            key=context,
            value=context,
            return_attention_scores=True)
       
        # Cache the attention scores for plotting later.
        self.last_attn_scores = attn_scores
    
        x = self.add([x, attn_output])
        x = self.layernorm(x)
    
        return x
    
class GlobalSelfAttention(BaseAttention):
    def call(self, x):
        attn_output = self.mha(
            query=x,
            value=x,
            key=x)
        x = self.add([x, attn_output])
        x = self.layernorm(x)
        return x
    
class CausalSelfAttention(BaseAttention):
    def call(self, x):
        attn_output = self.mha(
            query=x,
            value=x,
            key=x,
            use_causal_mask = True)
        x = self.add([x, attn_output])
        x = self.layernorm(x)
        return x
    
class FeedForward(tf.keras.layers.Layer):
    def __init__(self, d_model, dff, dropout_rate=0.1):
        super().__init__()
        self.seq = tf.keras.Sequential([
          tf.keras.layers.Dense(dff, activation='relu'),
          tf.keras.layers.Dense(d_model),
          tf.keras.layers.Dropout(dropout_rate)
        ])
        self.add = tf.keras.layers.Add()
        self.layer_norm = tf.keras.layers.LayerNormalization()

    def call(self, x):
        x = self.add([x, self.seq(x)])
        x = self.layer_norm(x) 
        return x
    
class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):
        super().__init__()
    
        self.self_attention = GlobalSelfAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate)
    
        self.ffn = FeedForward(d_model, dff)
    
    def call(self, x):
        x = self.self_attention(x)
        x = self.ffn(x)
        return x
    
class Encoder(tf.keras.layers.Layer):
    def __init__(self, *, len_of_seq, num_layers, d_model, num_heads,
               dff, num_conv_layers, filter_size, dropout_rate=0.1):
        super().__init__()
    
        self.d_model = d_model
        self.num_layers = num_layers
    
        self.pos_embedding = LandmarkEmbedding(
            len_of_seq, d_model, num_conv_layers, filter_size)
    
        self.enc_layers = [
            EncoderLayer(d_model=d_model,
                         num_heads=num_heads,
                         dff=dff,
                         dropout_rate=dropout_rate)
            for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(dropout_rate)

    def call(self, x):
        # `x` is landmark sequences with shape: (batch, seq_len, features)
        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.
        
        x = self.dropout(x)
    
        for i in range(self.num_layers):
            x = self.enc_layers[i](x)
    
        return x  # Shape `(batch_size, seq_len, d_model)`.
    
class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):
        super(DecoderLayer, self).__init__()
    
        self.causal_self_attention = CausalSelfAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate)
        
        self.cross_attention = CrossAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate)
    
        self.ffn = FeedForward(d_model, dff)

    def call(self, x, context):
        x = self.causal_self_attention(x=x)
        x = self.cross_attention(x=x, context=context)
    
        # Cache the last attention scores for plotting later
        self.last_attn_scores = self.cross_attention.last_attn_scores
    
        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.
        return x
    
class Decoder(tf.keras.layers.Layer):
    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, max_future_input_size, dropout_rate=0.1):
        super(Decoder, self).__init__()
    
        self.d_model = d_model
        self.num_layers = num_layers
    
        self.pos_embedding = PositionalTokenEmbedding(vocab_size=vocab_size, d_model=d_model, max_future_input_size=max_future_input_size)
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.dec_layers = [
            DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)
            for _ in range(num_layers)]
    
        self.last_attn_scores = None

    def call(self, x, context):
        # `x` is token-IDs shape (batch, target_seq_len)
        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)
    
        x = self.dropout(x)
    
        for i in range(self.num_layers):
            x  = self.dec_layers[i](x, context)
    
        self.last_attn_scores = self.dec_layers[-1].last_attn_scores
    
        # The shape of x is (batch_size, target_seq_len, d_model).
        return x
    
class GeneralTransformer(tf.keras.Model):
    def __init__(self, *, len_lm_seq, num_enc_layers, num_conv_layers, filter_size, num_dec_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, max_future_input_size, dropout_rate=0.1):
        super().__init__()
        self.max_future_input_size = max_future_input_size
        
        self.encoder = Encoder(len_of_seq=len_lm_seq,
                               num_layers=num_enc_layers, d_model=d_model,
                               num_heads=num_heads, dff=dff,
                               num_conv_layers=num_conv_layers,
                               filter_size=filter_size,
                               dropout_rate=dropout_rate)
        
        self.decoder = Decoder(num_layers=num_dec_layers, d_model=d_model,
                               num_heads=num_heads, dff=dff,
                               vocab_size=target_vocab_size,
                               max_future_input_size=max_future_input_size,
                               dropout_rate=dropout_rate)
    
        self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inputs):
        # To use a Keras model with `.fit` you must pass all your inputs in the
        # first argument.
        landmark_seq, prev_gen_context  = inputs
    
        encoded_lm_seq = self.encoder(landmark_seq)  # (batch_size, landmark_seq_len, d_model)
        x = self.decoder(prev_gen_context, encoded_lm_seq)  # (batch_size, target_len, d_model)
        logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

        try:
            # Drop the keras mask, so it doesn't scale the losses/metrics.
            # b/250038731
            del logits._keras_mask
        except AttributeError:
            pass
    
        return logits

# This function is important in case the model needs to be loaded only from the saved weights.
# The exact model needs to be recreated, with all parameters matching!
def get_model(d_model):
    max_future_input_size = MAX_PHRASE_LEN  # In the future the input can be longer than what it's trained on
    
    general_transformer = GeneralTransformer(
        len_lm_seq=FRAME_LEN,
        num_enc_layers=2,
        num_conv_layers=3,
        filter_size=11,
        num_dec_layers=4,
        d_model=d_model,
        num_heads=2,
        dff=256, 
        input_vocab_size=len(char_to_num),
        target_vocab_size=len(char_to_num),
        max_future_input_size=max_future_input_size,
        dropout_rate=0.1)

    return general_transformer

D_MODEL = 256 # embedding size

model = get_model(D_MODEL)

Epoch 1/30

Epoch 1: val_loss improved from inf to 1.52279, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 204s - loss: 2.3852 - masked_accuracy: 0.3164 - val_loss: 1.5228 - val_masked_accuracy: 0.5520 - 204s/epoch - 128ms/step
Epoch 2/30

Epoch 2: val_loss improved from 1.52279 to 1.16539, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 174s - loss: 1.3511 - masked_accuracy: 0.6069 - val_loss: 1.1654 - val_masked_accuracy: 0.6625 - 174s/epoch - 109ms/step
Epoch 3/30

Epoch 3: val_loss improved from 1.16539 to 1.03985, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 174s - loss: 1.1406 - masked_accuracy: 0.6715 - val_loss: 1.0399 - val_masked_accuracy: 0.6992 - 174s/epoch - 110ms/step
Epoch 4/30

Epoch 4: val_loss improved from 1.03985 to 0.81002, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 174s - loss: 0.9669 - masked_accuracy: 0.7202 - val_loss: 0.8100 - val_masked_accuracy: 0.7643 - 174s/epoch - 109ms/step
Epoch 5/30

Epoch 5: val_loss improved from 0.81002 to 0.73262, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 174s - loss: 0.8632 - masked_accuracy: 0.7492 - val_loss: 0.7326 - val_masked_accuracy: 0.7853 - 174s/epoch - 109ms/step
Epoch 6/30

Epoch 6: val_loss improved from 0.73262 to 0.67852, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 174s - loss: 0.7948 - masked_accuracy: 0.7679 - val_loss: 0.6785 - val_masked_accuracy: 0.8002 - 174s/epoch - 109ms/step
Epoch 7/30

Epoch 7: val_loss did not improve from 0.67852
1591/1591 - 174s - loss: 0.7313 - masked_accuracy: 0.7862 - val_loss: 0.6955 - val_masked_accuracy: 0.7945 - 174s/epoch - 109ms/step
Epoch 8/30

Epoch 8: val_loss improved from 0.67852 to 0.58491, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 174s - loss: 0.6794 - masked_accuracy: 0.8007 - val_loss: 0.5849 - val_masked_accuracy: 0.8271 - 174s/epoch - 109ms/step
Epoch 9/30

Epoch 9: val_loss improved from 0.58491 to 0.54978, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 173s - loss: 0.6335 - masked_accuracy: 0.8136 - val_loss: 0.5498 - val_masked_accuracy: 0.8370 - 173s/epoch - 109ms/step
Epoch 10/30

Epoch 10: val_loss improved from 0.54978 to 0.52182, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.5979 - masked_accuracy: 0.8234 - val_loss: 0.5218 - val_masked_accuracy: 0.8451 - 171s/epoch - 108ms/step
Epoch 11/30

Epoch 11: val_loss improved from 0.52182 to 0.49620, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.5674 - masked_accuracy: 0.8322 - val_loss: 0.4962 - val_masked_accuracy: 0.8526 - 171s/epoch - 108ms/step
Epoch 12/30

Epoch 12: val_loss improved from 0.49620 to 0.47402, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.5427 - masked_accuracy: 0.8389 - val_loss: 0.4740 - val_masked_accuracy: 0.8588 - 171s/epoch - 108ms/step
Epoch 13/30

Epoch 13: val_loss improved from 0.47402 to 0.44841, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.5178 - masked_accuracy: 0.8458 - val_loss: 0.4484 - val_masked_accuracy: 0.8658 - 172s/epoch - 108ms/step
Epoch 14/30

Epoch 14: val_loss improved from 0.44841 to 0.42872, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.4962 - masked_accuracy: 0.8519 - val_loss: 0.4287 - val_masked_accuracy: 0.8715 - 171s/epoch - 108ms/step
Epoch 15/30

Epoch 15: val_loss improved from 0.42872 to 0.41758, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.4764 - masked_accuracy: 0.8573 - val_loss: 0.4176 - val_masked_accuracy: 0.8740 - 171s/epoch - 108ms/step
Epoch 16/30

Epoch 16: val_loss improved from 0.41758 to 0.39940, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.4589 - masked_accuracy: 0.8621 - val_loss: 0.3994 - val_masked_accuracy: 0.8792 - 171s/epoch - 108ms/step
Epoch 17/30

Epoch 17: val_loss improved from 0.39940 to 0.37418, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.4484 - masked_accuracy: 0.8648 - val_loss: 0.3742 - val_masked_accuracy: 0.8863 - 171s/epoch - 108ms/step
Epoch 18/30

Epoch 18: val_loss improved from 0.37418 to 0.36695, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.4329 - masked_accuracy: 0.8690 - val_loss: 0.3670 - val_masked_accuracy: 0.8884 - 171s/epoch - 108ms/step
Epoch 19/30

Epoch 19: val_loss improved from 0.36695 to 0.35192, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.4138 - masked_accuracy: 0.8744 - val_loss: 0.3519 - val_masked_accuracy: 0.8926 - 172s/epoch - 108ms/step
Epoch 20/30

Epoch 20: val_loss improved from 0.35192 to 0.34514, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.3989 - masked_accuracy: 0.8789 - val_loss: 0.3451 - val_masked_accuracy: 0.8945 - 172s/epoch - 108ms/step
Epoch 21/30

Epoch 21: val_loss improved from 0.34514 to 0.32056, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.3860 - masked_accuracy: 0.8821 - val_loss: 0.3206 - val_masked_accuracy: 0.9018 - 172s/epoch - 108ms/step
Epoch 22/30

Epoch 22: val_loss improved from 0.32056 to 0.31187, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.3721 - masked_accuracy: 0.8861 - val_loss: 0.3119 - val_masked_accuracy: 0.9042 - 171s/epoch - 108ms/step
Epoch 23/30

Epoch 23: val_loss improved from 0.31187 to 0.30484, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.3607 - masked_accuracy: 0.8895 - val_loss: 0.3048 - val_masked_accuracy: 0.9061 - 172s/epoch - 108ms/step
Epoch 24/30

Epoch 24: val_loss improved from 0.30484 to 0.29000, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.3489 - masked_accuracy: 0.8922 - val_loss: 0.2900 - val_masked_accuracy: 0.9102 - 172s/epoch - 108ms/step
Epoch 25/30

Epoch 25: val_loss improved from 0.29000 to 0.27778, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.3374 - masked_accuracy: 0.8960 - val_loss: 0.2778 - val_masked_accuracy: 0.9137 - 172s/epoch - 108ms/step
Epoch 26/30

Epoch 26: val_loss did not improve from 0.27778
1591/1591 - 171s - loss: 0.3289 - masked_accuracy: 0.8982 - val_loss: 0.2786 - val_masked_accuracy: 0.9128 - 171s/epoch - 108ms/step
Epoch 27/30

Epoch 27: val_loss improved from 0.27778 to 0.25780, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.3255 - masked_accuracy: 0.8990 - val_loss: 0.2578 - val_masked_accuracy: 0.9196 - 172s/epoch - 108ms/step
Epoch 28/30

Epoch 28: val_loss improved from 0.25780 to 0.25553, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.3174 - masked_accuracy: 0.9013 - val_loss: 0.2555 - val_masked_accuracy: 0.9198 - 171s/epoch - 108ms/step
Epoch 29/30

Epoch 29: val_loss improved from 0.25553 to 0.24335, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.3033 - masked_accuracy: 0.9054 - val_loss: 0.2433 - val_masked_accuracy: 0.9237 - 172s/epoch - 108ms/step
Epoch 30/30

Epoch 30: val_loss improved from 0.24335 to 0.23670, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.2958 - masked_accuracy: 0.9072 - val_loss: 0.2367 - val_masked_accuracy: 0.9257 - 172s/epoch - 108ms/step
CPU times: user 1h 9min 38s, sys: 4min 20s, total: 1h 13min 59s
Wall time: 1h 26min 35s
<keras.callbacks.History at 0x783c8c28f0a0>

Epoch 1/30

Epoch 1: val_loss improved from 0.23670 to 0.22560, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.2876 - masked_accuracy: 0.9098 - val_loss: 0.2256 - val_masked_accuracy: 0.9287 - 172s/epoch - 108ms/step
Epoch 2/30

Epoch 2: val_loss improved from 0.22560 to 0.22127, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2779 - masked_accuracy: 0.9124 - val_loss: 0.2213 - val_masked_accuracy: 0.9301 - 171s/epoch - 108ms/step
Epoch 3/30

Epoch 3: val_loss improved from 0.22127 to 0.21367, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2702 - masked_accuracy: 0.9148 - val_loss: 0.2137 - val_masked_accuracy: 0.9322 - 171s/epoch - 108ms/step
Epoch 4/30

Epoch 4: val_loss improved from 0.21367 to 0.21239, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2634 - masked_accuracy: 0.9170 - val_loss: 0.2124 - val_masked_accuracy: 0.9321 - 171s/epoch - 108ms/step
Epoch 5/30

Epoch 5: val_loss improved from 0.21239 to 0.20041, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2565 - masked_accuracy: 0.9188 - val_loss: 0.2004 - val_masked_accuracy: 0.9363 - 171s/epoch - 108ms/step
Epoch 6/30

Epoch 6: val_loss improved from 0.20041 to 0.19180, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2496 - masked_accuracy: 0.9207 - val_loss: 0.1918 - val_masked_accuracy: 0.9387 - 171s/epoch - 108ms/step
Epoch 7/30

Epoch 7: val_loss improved from 0.19180 to 0.18606, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2423 - masked_accuracy: 0.9228 - val_loss: 0.1861 - val_masked_accuracy: 0.9404 - 171s/epoch - 108ms/step
Epoch 8/30

Epoch 8: val_loss improved from 0.18606 to 0.18046, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2405 - masked_accuracy: 0.9235 - val_loss: 0.1805 - val_masked_accuracy: 0.9421 - 171s/epoch - 108ms/step
Epoch 9/30

Epoch 9: val_loss improved from 0.18046 to 0.17520, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2347 - masked_accuracy: 0.9251 - val_loss: 0.1752 - val_masked_accuracy: 0.9438 - 171s/epoch - 108ms/step
Epoch 10/30

Epoch 10: val_loss improved from 0.17520 to 0.17062, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2274 - masked_accuracy: 0.9270 - val_loss: 0.1706 - val_masked_accuracy: 0.9452 - 171s/epoch - 108ms/step
Epoch 11/30

Epoch 11: val_loss improved from 0.17062 to 0.16552, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2212 - masked_accuracy: 0.9290 - val_loss: 0.1655 - val_masked_accuracy: 0.9467 - 171s/epoch - 108ms/step
Epoch 12/30

Epoch 12: val_loss improved from 0.16552 to 0.16181, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.2166 - masked_accuracy: 0.9303 - val_loss: 0.1618 - val_masked_accuracy: 0.9479 - 172s/epoch - 108ms/step
Epoch 13/30

Epoch 13: val_loss improved from 0.16181 to 0.15741, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2105 - masked_accuracy: 0.9319 - val_loss: 0.1574 - val_masked_accuracy: 0.9491 - 171s/epoch - 108ms/step
Epoch 14/30

Epoch 14: val_loss improved from 0.15741 to 0.14871, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2052 - masked_accuracy: 0.9337 - val_loss: 0.1487 - val_masked_accuracy: 0.9520 - 171s/epoch - 108ms/step
Epoch 15/30

Epoch 15: val_loss improved from 0.14871 to 0.14574, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.2019 - masked_accuracy: 0.9346 - val_loss: 0.1457 - val_masked_accuracy: 0.9529 - 171s/epoch - 108ms/step
Epoch 16/30

Epoch 16: val_loss did not improve from 0.14574
1591/1591 - 171s - loss: 0.2002 - masked_accuracy: 0.9354 - val_loss: 0.1469 - val_masked_accuracy: 0.9522 - 171s/epoch - 108ms/step
Epoch 17/30

Epoch 17: val_loss improved from 0.14574 to 0.13256, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.1905 - masked_accuracy: 0.9380 - val_loss: 0.1326 - val_masked_accuracy: 0.9572 - 172s/epoch - 108ms/step
Epoch 18/30

Epoch 18: val_loss improved from 0.13256 to 0.13173, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 207s - loss: 0.1883 - masked_accuracy: 0.9386 - val_loss: 0.1317 - val_masked_accuracy: 0.9571 - 207s/epoch - 130ms/step
Epoch 19/30

Epoch 19: val_loss improved from 0.13173 to 0.12021, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.1846 - masked_accuracy: 0.9401 - val_loss: 0.1202 - val_masked_accuracy: 0.9611 - 171s/epoch - 108ms/step
Epoch 20/30

Epoch 20: val_loss did not improve from 0.12021
1591/1591 - 171s - loss: 0.1804 - masked_accuracy: 0.9409 - val_loss: 0.1245 - val_masked_accuracy: 0.9599 - 171s/epoch - 107ms/step
Epoch 21/30

Epoch 21: val_loss improved from 0.12021 to 0.11924, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.1779 - masked_accuracy: 0.9421 - val_loss: 0.1192 - val_masked_accuracy: 0.9612 - 171s/epoch - 108ms/step
Epoch 22/30

Epoch 22: val_loss improved from 0.11924 to 0.11635, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 172s - loss: 0.1747 - masked_accuracy: 0.9427 - val_loss: 0.1163 - val_masked_accuracy: 0.9621 - 172s/epoch - 108ms/step
Epoch 23/30

Epoch 23: val_loss improved from 0.11635 to 0.11072, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 173s - loss: 0.1694 - masked_accuracy: 0.9446 - val_loss: 0.1107 - val_masked_accuracy: 0.9637 - 173s/epoch - 109ms/step
Epoch 24/30

Epoch 24: val_loss improved from 0.11072 to 0.10285, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.1649 - masked_accuracy: 0.9459 - val_loss: 0.1028 - val_masked_accuracy: 0.9665 - 171s/epoch - 108ms/step
Epoch 25/30

Epoch 25: val_loss did not improve from 0.10285
1591/1591 - 171s - loss: 0.1613 - masked_accuracy: 0.9470 - val_loss: 0.1035 - val_masked_accuracy: 0.9662 - 171s/epoch - 108ms/step
Epoch 26/30

Epoch 26: val_loss improved from 0.10285 to 0.09492, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.1567 - masked_accuracy: 0.9486 - val_loss: 0.0949 - val_masked_accuracy: 0.9691 - 171s/epoch - 108ms/step
Epoch 27/30

Epoch 27: val_loss improved from 0.09492 to 0.09188, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.1538 - masked_accuracy: 0.9496 - val_loss: 0.0919 - val_masked_accuracy: 0.9698 - 171s/epoch - 108ms/step
Epoch 28/30

Epoch 28: val_loss did not improve from 0.09188
1591/1591 - 171s - loss: 0.1492 - masked_accuracy: 0.9506 - val_loss: 0.0939 - val_masked_accuracy: 0.9691 - 171s/epoch - 108ms/step
Epoch 29/30

Epoch 29: val_loss improved from 0.09188 to 0.09059, saving model to general_transformer_3_weights_checkpoint.h5
1591/1591 - 171s - loss: 0.1462 - masked_accuracy: 0.9516 - val_loss: 0.0906 - val_masked_accuracy: 0.9704 - 171s/epoch - 108ms/step
Epoch 30/30

Epoch 30: val_loss did not improve from 0.09059
1591/1591 - 171s - loss: 0.1437 - masked_accuracy: 0.9528 - val_loss: 0.0923 - val_masked_accuracy: 0.9695 - 171s/epoch - 108ms/step
CPU times: user 1h 8min 31s, sys: 4min 20s, total: 1h 12min 51s
Wall time: 1h 27min 17s
<keras.callbacks.History at 0x783c8c28e890>